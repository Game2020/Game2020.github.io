<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  

  
  <title>Machine Learning Concepts | Game 2020</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Process">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Concepts">
<meta property="og:url" content="https://2020.iosdevlog.com/2020/02/22/ds/index.html">
<meta property="og:site_name" content="Game 2020">
<meta property="og:description" content="Process">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/Process.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/708342AF-41CC-4702-B41B-08DE83166234.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/B19BDBEE-22D5-4A3E-8861-4790CDDE01E0.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/FC3EF1F8-AC76-4D03-9E45-036D76C4E216.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/AA360355-4A67-41ED-8475-81391BD62DB3.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/8EAF745C-496E-45EA-B94F-FEC16E431FAD.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/1F3239B7-A891-4326-831C-0F01A7ACFA00.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/7B2111F9-6BF0-43B2-B130-C24CAAC39365.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/77C64150-2B24-41B5-8F70-AD154A20EC66.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/33911326-3EBD-4DF9-9875-B75F287E30D0.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/06091448-C605-4DEA-9650-D71A77C710C8.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/6D6D7323-715D-42F8-9B71-39BE839F2C4B.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/AAAF1D3A-8184-4843-A1E3-AC53E43315FC.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/A37BBE5F-E77F-411E-BA91-5A2D475063D0.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/396DE3E5-85C2-487D-9324-FB83FCC7F8FD.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/690903C2-BC9F-400F-877E-7B625F2A20BD.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/B8AA9D14-192E-4C93-A166-A429A293990C.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/2EE71F38-3B82-48E7-A8B8-44FC8B2A5805.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/B7F5DF41-7571-412D-81AD-4B683CC1812B.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/9E271C6A-6FBB-41FC-960B-4AA394E6EEA5.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/0397888B-C44B-4178-B1D5-F7FB2ED1F3EC.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/8942D559-DDAA-4EE2-AD81-3D6DE9C1540C.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/C2D73C61-1BAC-4146-B59C-2C6D75CAFCF6.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/4597CE60-7571-403E-98C5-B3087805A418.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/B96B2BB2-CF51-4989-8BE7-1A825082D2D6.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/329F57B1-378D-47B7-B536-0AD0F626A708.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/A9DC5C23-CACF-429F-83B2-F432DA3B3F1A.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/FC311DB8-C4BD-4D96-A9B0-786CF4091DE7.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/19746786-7A83-4A0A-B8C2-38F7E7189DFE.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/55037B97-44EC-4173-9D7E-2755F6648E9C.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/2956137E-E540-471C-A34B-66BBE0507483.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/54388388-46D3-4E97-A228-23669D4E1E88.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/FE866A04-B684-4395-92EB-E73593004643.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/E07D7B09-0747-48C9-ADC6-87AA26B8D3BF.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/870B8130-69C4-4DBD-9BCF-D8D96D3C7D77.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/E90A57EE-85B2-4601-BF52-1F71E557A13F.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/4D1F3F76-3341-47A5-96C1-9097E5C87A38.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/860CED1C-EA17-44D3-A99E-B696A87A92BF.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/7E9DE18D-616C-45BB-A0C2-02C7E469F034.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/5F52537E-8AF1-42FA-8CB4-2B2549F3FDA7.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/4739C552-28D0-4FAB-80B1-10DC7BBE1014.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/5EFA0B04-EB55-4414-8DFF-69C42072527A.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/6BAD212C-E812-4D94-887E-B8FC28594153.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/B3457C07-7876-49F4-9FDA-5516DECF2E65.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/CFF452C6-1F11-465E-BDE8-EA3B3D55251D.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/8F02DF2E-0723-4EDA-83E1-04DEBB0A222F.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/F2C3B775-C53A-4AA9-A398-120FBDFF6EF3.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/FF9BC15A-3117-4FA4-B59D-9EBD6E46F3A6.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/D6AD4AEC-161F-4A81-A996-50C936E3752B.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/E0EC20FC-0906-4B7C-A75F-27338A35CB48.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/BCFC979D-F514-45F2-8543-8AFC4539CD08.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/250ECB8A-F48D-4814-B542-2C3FC45B2127.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/97F11D13-2F43-499D-9E53-5CE038479F74.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/B0FFBB3D-70E6-4C13-AFEC-4E1E4F133926.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/F669FC47-9F7C-45BB-9311-148E23624F68.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/7886A650-E942-414E-B9C1-FF657BFA8738.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/C8581E8F-5C6B-424E-839D-9A01C3BEAE53.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/7839524D-1CE8-4634-A5D3-8B493ADBF0CE.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/DF7A3A69-3945-4222-86DA-EF91B8FF6740.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/5D348DC1-B376-4FCA-9FA5-F2C2E4697D84.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/AED450B9-7864-4B51-8554-0B9A8CBB92BE.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/4427EABD-9457-44F5-AE53-47D654CA898C.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/B26E5AA5-905F-49BB-A492-F0D34378ACC2.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/187B983B-A9BE-4B1C-8ACF-ACB40B0E5725.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/19DB13EA-7024-46CD-9404-64E8EA2850A4.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/C416473F-2CEC-4F12-8E9D-B0D7D1C686A6.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/15CD2CF7-B982-4C9C-B69F-33D20657E7A2.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/0C8FD4CE-AC05-4975-ABDD-B54BD11F8312.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/65EE6640-3536-4A5D-8935-020FDEA6FCDB.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/031EBE1C-F064-4DB0-9DE7-CABBA8D17668.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/0DDFCAD3-A0C9-4978-A0F1-B47872AC82B2.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/45D4572F-11EE-4280-A978-384D0E1D872A.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/877F41BA-7063-48C3-AA8C-26173DDA8DE4.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/A416F2D4-5106-41F7-8E5B-40B5E73EA434.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/D2FA6B8A-FA31-4015-87B8-5FDCC70A63F4.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/4E2AA87B-6A9A-42C7-8C3E-C2B25E198D30.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/695F6632-0CA9-4AE7-9058-6F7E6DC780B4.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/1F2AE40B-6E5E-4501-AEB1-FFCAF145B311.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/7E0FE9AC-605D-4FB4-8790-DE3B05336F1F.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/87F7C9A2-8634-4E57-85E2-9B3C4B9D33F8.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/0C8FD4CE-AC05-4975-ABDD-B54BD11F8312.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/53B97C2C-8826-477D-BC26-41CABEC4A42E.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/397D4FCF-04A7-4D68-927F-D6414FA747FE.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/8F75FD12-E83A-4DE5-94ED-CDDD231E3E14.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/044BC9F5-9D15-4A64-9529-63403036B623.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/C228CF35-C057-4DF4-BEC2-6A7CB7D1C92F.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/1452F430-91D5-4E1A-8021-3BA21B90AD83.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/091E0B05-1B93-4959-B8C7-7E135A336EF7.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/F45BC780-34A4-4478-8204-361A57CAC7A4.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/F3BE7BDA-CC8D-44E9-9586-3C4EA3B875B2.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/385CB279-89CD-4AD6-82E7-D80BAA9F2826.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/B3C1808C-9C79-4E45-9AC6-A5685A5D8407.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/C5A62BA8-B2BB-4325-A5CC-ADBF498E685A.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/72E0D7A0-9E26-49E9-A540-18A69873BE47.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/3B5287CE-BF34-4688-844F-57875F627F8A.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/249C3E38-0B9A-4493-982C-CC26B7614B12.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/D75A0917-3A7D-441E-B18F-A51B087761D3.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/A1AD6FA2-19D3-4C06-A092-420D007E8E3E.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/1810007C-AE05-41BF-A70B-6FDA309289BC.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/5697CF3A-4832-4849-98B9-3132500C6576.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/EB87C752-40AC-40E4-89BF-E85F3AA6AE97.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/0FCF647C-94C1-42AA-B26D-BB5F5DB14248.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/5100966F-881F-45C0-9D48-807D86657D02.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/76B7C54A-E48B-4D1B-A1CF-602F165D7C09.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/DB988698-5B05-4D61-8514-5DE7B085C286.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/3DAD67C3-1FCB-475E-96B7-E2D2793A9FB6.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/F6753EA8-E4DB-4409-978F-AB2BCC323032.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/4C67AB0A-2FB0-4B03-B9FA-E36203B3E37C.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/F340BAA3-345D-4D1E-9A6A-761D6BCF4E1F.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/B6CD5EBF-451C-4E1F-AA90-DFB9FD2165FE.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/D54BBB93-EE9A-433D-99ED-7D4C82B94AD2.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/C2E4A20B-CE21-414D-82F6-D179E30C7572.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/FCECCABF-A68B-421E-8498-EDF2D313371B.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/C4BDC911-1EDC-44D1-AEFA-FDC64360BA6D.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/D42822B8-CE14-4B16-A24D-8151614F601D.png">
<meta property="og:image" content="https://2020.iosdevlog.com/2020/02/22/ds/9E4B97B9-A6DC-4F0D-8D8A-7030819525B1.png">
<meta property="article:published_time" content="2020-02-22T11:10:39.000Z">
<meta property="article:modified_time" content="2020-02-22T11:19:03.548Z">
<meta property="article:author" content="iOSDevLog">
<meta property="article:tag" content="ML">
<meta property="article:tag" content="DS">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://2020.iosdevlog.com/2020/02/22/ds/Process.png">
  
    <link rel="alternate" href="/atom.xml" title="Game 2020" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Game 2020</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">https://2020.iosdevlog.com</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/2020">2020 Calendar</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://2020.iosdevlog.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-ds" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/22/ds/" class="article-date">
  <time datetime="2020-02-22T11:10:39.000Z" itemprop="datePublished">2020-02-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/AI/">AI</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Machine Learning Concepts
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <figure>
<img src="https://2020.iosdevlog.com/2020/02/22/ds/Process.png" alt="" /><figcaption>Process</figcaption>
</figure>
<a id="more"></a>
<h2 id="types">Types</h2>
<h3 id="regression">Regression</h3>
<ul>
<li>A supervised problem, the outputs are continuous rather than discrete.</li>
</ul>
<h3 id="classification">Classification</h3>
<ul>
<li>Inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one or more (multi-label classification) of these classes. This is typically tackled in a supervised way.</li>
</ul>
<h3 id="clustering">Clustering</h3>
<ul>
<li>A set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.</li>
</ul>
<h3 id="density-estimation">Density Estimation</h3>
<ul>
<li>Finds the distribution of inputs in some space.</li>
</ul>
<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>
<ul>
<li>Simplifies inputs by mapping them into a lower-dimensional space.</li>
</ul>
<h2 id="kind">Kind</h2>
<h3 id="parametric">Parametric</h3>
<ul>
<li><p>Step 1: Making an assumption about the functional form or shape of our function (f), i.e.: f is linear, thus we will select a linear model.</p></li>
<li><p>Step 2: Selecting a procedure to fit or train our model. This means estimating the Beta parameters in the linear function. A common approach is the (ordinary) least squares, amongst others.</p></li>
</ul>
<h3 id="non-parametric">Non-Parametric</h3>
<ul>
<li>When we do not make assumptions about the form of our function (f). However, since these methods do not reduce the problem of estimating f to a small number of parameters, a large number of observations is required in order to obtain an accurate estimate for f. An example would be the thin-plate spline model.</li>
</ul>
<h2 id="categories">Categories</h2>
<h3 id="supervised">Supervised</h3>
<ul>
<li>The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs.</li>
</ul>
<h3 id="unsupervised">Unsupervised</h3>
<ul>
<li>No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).</li>
</ul>
<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<ul>
<li>A computer program interacts with a dynamic environment in which it must perform a certain goal (such as <a href="https://en.wikipedia.org/wiki/Autonomous_car" target="_blank" rel="noopener">driving a vehicle</a> or playing a game against an opponent). The program is provided feedback in terms of rewards and punishments as it navigates its problem space.</li>
</ul>
<h2 id="approaches">Approaches</h2>
<h3 id="decision-tree-learning">Decision tree learning</h3>
<h3 id="association-rule-learning">Association rule learning</h3>
<h3 id="artificial-neural-networks">Artificial neural networks</h3>
<h3 id="deep-learning">Deep learning</h3>
<h3 id="inductive-logic-programming">Inductive logic programming</h3>
<h3 id="support-vector-machines">Support vector machines</h3>
<h3 id="clustering-1">Clustering</h3>
<h3 id="bayesian-networks">Bayesian networks</h3>
<h3 id="reinforcement-learning-1">Reinforcement learning</h3>
<h3 id="representation-learning">Representation learning</h3>
<h3 id="similarity-and-metric-learning">Similarity and metric learning</h3>
<h3 id="sparse-dictionary-learning">Sparse dictionary learning</h3>
<h3 id="genetic-algorithms">Genetic algorithms</h3>
<h3 id="rule-based-machine-learning">Rule-based machine learning</h3>
<h3 id="learning-classifier-systems">Learning classifier systems</h3>
<h2 id="taxonomy">Taxonomy</h2>
<h3 id="generative-methods">Generative Methods</h3>
<ul>
<li><p>Popular models</p>
<ul>
<li><p>Mixtures of Gaussians, Mixtures of experts, Hidden Markov Models (HMM)</p></li>
<li><p>Gaussians, Naïve Bayes, Mixtures of multinomials</p></li>
<li><p>Sigmoidal belief networks, Bayesian networks, Markov random fields</p></li>
</ul></li>
<li><p>Model class-conditional pdfs and prior probabilities. “Generative” since sampling can generate synthetic data points.</p></li>
</ul>
<h3 id="discriminative-methods">Discriminative Methods</h3>
<ul>
<li><p>Directly estimate posterior probabilities. No attempt to model underlying probability distributions. Focus computational resources on given task– better performance</p></li>
<li><p>Popular Models</p>
<ul>
<li><p>Logistic regression, SVMs</p></li>
<li><p>Traditional neural networks, Nearest neighbor</p></li>
<li><p>Conditional Random Fields (CRF)</p></li>
</ul></li>
</ul>
<h2 id="selection-criteria">Selection Criteria</h2>
<h3 id="prediction-accuracy-vs-model-interpretability">Prediction Accuracy vs Model Interpretability</h3>
<ul>
<li>There is an inherent tradeoff between Prediction Accuracy and Model Interpretability, that is to say that as the model get more flexible in the way the function (f) is selected, they get obscured, and are hard to interpret. Flexible methods are better for inference, and inflexible methods are preferable for prediction.</li>
</ul>
<h2 id="libraries">Libraries</h2>
<h3 id="python">Python</h3>
<ul>
<li><p>Numpy</p>
<ul>
<li>Adds support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays</li>
</ul></li>
<li><p>Pandas</p>
<ul>
<li>Offers data structures and operations for manipulating numerical tables and time series</li>
</ul></li>
<li><p>Scikit-Learn</p>
<ul>
<li>It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.</li>
</ul></li>
<li><p>Tensorflow</p>
<ul>
<li><p>Components<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/708342AF-41CC-4702-B41B-08DE83166234.png" /></p>
<ul>
<li><p>Does lazy evaluation. Need to build the graph, and then run it in a session.<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/B19BDBEE-22D5-4A3E-8861-4790CDDE01E0.png" /></p>
<ul>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/FC3EF1F8-AC76-4D03-9E45-036D76C4E216.png" /></p></li>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/AA360355-4A67-41ED-8475-81391BD62DB3.png" /></p></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>MXNet</p>
<ul>
<li>Is an modern open-source deep learning framework used to train, and deploy deep neural networks. MXNet library is portable and can scale to multiple GPUs and multiple machines. MXNet is supported by major Public Cloud providers including AWS and Azure. Amazon has chosen MXNet as its deep learning framework of choice at AWS.</li>
</ul></li>
<li><p>Keras</p>
<ul>
<li>Is an open source neural network library written in Python. It is capable of running on top of MXNet, Deeplearning4j, Tensorflow, CNTK or Theano. Designed to enable fast experimentation with deep neural networks, it focuses on being minimal, modular and extensible.</li>
</ul></li>
<li><p>Torch</p>
<ul>
<li>Torch is an open source machine learning library, a scientific computing framework, and a script language based on the Lua programming language. It provides a wide range of algorithms for deep machine learning, and uses the scripting language LuaJIT, and an underlying C implementation.</li>
</ul></li>
<li><p>Microsoft Cognitive Toolkit</p>
<ul>
<li>Previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.</li>
</ul></li>
</ul>
<h2 id="tuning">Tuning</h2>
<h3 id="cross-validation">Cross-validation</h3>
<ul>
<li><p>One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/8EAF745C-496E-45EA-B94F-FEC16E431FAD.png" /></li>
</ul></li>
<li><p>Methods</p>
<ul>
<li><p>Leave-p-out cross-validation</p></li>
<li><p>Leave-one-out cross-validation</p></li>
<li><p>k-fold cross-validation</p></li>
<li><p>Holdout method</p></li>
<li><p>Repeated random sub-sampling validation</p></li>
</ul></li>
</ul>
<h3 id="hyperparameters">Hyperparameters</h3>
<ul>
<li><p>Grid Search</p>
<ul>
<li>The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.</li>
</ul></li>
<li><p>Random Search</p>
<ul>
<li>Since grid searching is an exhaustive and therefore potentially expensive method, several alternatives have been proposed. In particular, a randomized search that simply samples parameter settings a fixed number of times has been found to be more effective in high-dimensional spaces than exhaustive search.</li>
</ul></li>
<li><p>Gradient-based optimization</p>
<ul>
<li>For specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks. Since then, these methods have been extended to other models such as support vector machines or logistic regression.</li>
</ul></li>
</ul>
<h3 id="early-stopping-regularization">Early Stopping (Regularization)</h3>
<ul>
<li>Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit, and stop the algorithm then.</li>
</ul>
<h3 id="overfitting">Overfitting</h3>
<ul>
<li>When a given method yields a small training MSE (or cost), but a large test MSE (or cost), we are said to be overfitting the data. This happens because our statistical learning procedure is trying too hard to find pattens in the data, that might be due to random chance, rather than a property of our function. In other words, the algorithms may be learning the training data too well. If model overfits, try removing some features, decreasing degrees of freedom, or adding more data.</li>
</ul>
<h3 id="underfitting">Underfitting</h3>
<ul>
<li>Opposite of Overfitting. Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. It occurs when the model or algorithm does not fit the data enough. Underfitting occurs if the model or algorithm shows low variance but high bias (to contrast the opposite, overfitting from high variance and low bias). It is often a result of an excessively simple model.</li>
</ul>
<h3 id="bootstrap">Bootstrap</h3>
<ul>
<li>Test that applies Random Sampling with Replacement of the available data, and assigns measures of accuracy (bias, variance, etc.) to sample estimates.</li>
</ul>
<h3 id="bagging">Bagging</h3>
<ul>
<li>An approach to ensemble learning that is based on bootstrapping. Shortly, given a training set, we produce multiple different training sets (called bootstrap samples), by sampling with replacement from the original dataset. Then, for each bootstrap sample, we build a model. The results in an ensemble of models, where each model votes with the equal weight. Typically, the goal of this procedure is to reduce the variance of the model of interest (e.g. decision trees).</li>
</ul>
<h2 id="performance-analysis">Performance Analysis</h2>
<h3 id="confusion-matrix">Confusion Matrix</h3>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/1F3239B7-A891-4326-831C-0F01A7ACFA00.png" /></li>
</ul>
<h3 id="accuracy">Accuracy</h3>
<ul>
<li>Fraction of correct predictions, not reliable as skewed when the data set is unbalanced (that is, when the number of samples in different classes vary greatly)</li>
</ul>
<h3 id="f1-score">f1 score</h3>
<ul>
<li><p>Precision</p>
<ul>
<li>Out of all the examples the classifier labeled as positive, what fraction were correct?<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/7B2111F9-6BF0-43B2-B130-C24CAAC39365.png" /></li>
</ul></li>
<li><p>Recall</p>
<ul>
<li>Out of all the positive examples there were, what fraction did the classifier pick up?<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/77C64150-2B24-41B5-8F70-AD154A20EC66.png" /></li>
</ul></li>
<li><p>Harmonic Mean of Precision and Recall: (2 * p * r / (p + r))</p></li>
</ul>
<h3 id="roc-curve---receiver-operating-characteristics">ROC Curve - Receiver Operating Characteristics</h3>
<ul>
<li>True Positive Rate (Recall / Sensitivity) vs False Positive Rate (1-Specificity)<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/33911326-3EBD-4DF9-9875-B75F287E30D0.png" /></li>
</ul>
<h3 id="bias-variance-tradeoff">Bias-Variance Tradeoff</h3>
<ul>
<li><p>Bias refers to the amount of error that is introduced by approximating a real-life problem, which may be extremely complicated, by a simple model. If Bias is high, and/or if the algorithm performs poorly even on your training data, try adding more features, or a more flexible model.</p></li>
<li><p>Variance is the amount our model’s prediction would change when using a different training data set. High: Remove features, or obtain more data.</p></li>
</ul>
<h3 id="goodness-of-fit-r2">Goodness of Fit = R^2</h3>
<ul>
<li>1.0 - sum_of_squared_errors / total_sum_of_squares(y)</li>
</ul>
<h3 id="mean-squared-error-mse">Mean Squared Error (MSE)</h3>
<ul>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/06091448-C605-4DEA-9650-D71A77C710C8.png" /></p>
<ul>
<li>The mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors or deviations—that is, the difference between the estimator and what is estimated.</li>
</ul></li>
</ul>
<h3 id="error-rate">Error Rate</h3>
<ul>
<li><p>The proportion of mistakes made if we apply out estimate model function the the training observations in a classification setting.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/6D6D7323-715D-42F8-9B71-39BE839F2C4B.png" /></li>
</ul></li>
</ul>
<h2 id="motivation">Motivation</h2>
<h3 id="prediction">Prediction</h3>
<ul>
<li>When we are interested mainly in the predicted variable as a result of the inputs, but not on the each way of the inputs affect the prediction. In a real estate example, Prediction would answer the question of: Is my house over or under valued? Non-linear models are very good at these sort of predictions, but not great for inference because the models are much less interpretable.</li>
</ul>
<h3 id="inference">Inference</h3>
<ul>
<li>When we are interested in the way each one of the inputs affect the prediction. In a real estate example, Inference would answer the question of: How much would my house cost if it had a view of the sea? Linear models are more suited for inference because the models themselves are easier to understand than their non-linear counterparts.</li>
</ul>
<h1 id="machine-learning-process">Machine Learning Process</h1>
<h2 id="data">Data</h2>
<h3 id="find">Find</h3>
<h3 id="collect">Collect</h3>
<h3 id="explore">Explore</h3>
<h3 id="clean-features">Clean Features</h3>
<h3 id="impute-features">Impute Features</h3>
<h3 id="engineer-features">Engineer Features</h3>
<h3 id="select-features">Select Features</h3>
<h3 id="encode-features">Encode Features</h3>
<h3 id="build-datasets">Build Datasets</h3>
<ul>
<li>Machine Learning is math. In specific, performing Linear Algebra on Matrices. Our data values must be numeric.</li>
</ul>
<h2 id="model">Model</h2>
<h3 id="select-algorithm-based-on-question-and-data-available">Select Algorithm based on question and data available</h3>
<h2 id="cost-function">Cost Function</h2>
<h3 id="the-cost-function-will-provide-a-measure-of-how-far-my-algorithm-and-its-parameters-are-from-accurately-representing-my-training-data.">The cost function will provide a measure of how far my algorithm and its parameters are from accurately representing my training data.</h3>
<h3 id="sometimes-referred-to-as-cost-or-loss-function-when-the-goal-is-to-minimise-it-or-objective-function-when-the-goal-is-to-maximise-it.">Sometimes referred to as Cost or Loss function when the goal is to minimise it, or Objective function when the goal is to maximise it.</h3>
<h2 id="optimization">Optimization</h2>
<h3 id="having-selected-a-cost-function-we-need-a-method-to-minimise-the-cost-function-or-maximise-the-objective-function.-typically-this-is-done-by-gradient-descent-or-stochastic-gradient-descent.">Having selected a cost function, we need a method to minimise the Cost function, or maximise the Objective function. Typically this is done by Gradient Descent or Stochastic Gradient Descent.</h3>
<h2 id="tuning-1">Tuning</h2>
<h3 id="different-algorithms-have-different-hyperparameters-which-will-affect-the-algorithms-performance.-there-are-multiple-methods-for-hyperparameter-tuning-such-as-grid-and-random-search.">Different Algorithms have different Hyperparameters, which will affect the algorithms performance. There are multiple methods for Hyperparameter Tuning, such as Grid and Random search.</h3>
<h2 id="results-and-benchmarking">Results and Benchmarking</h2>
<h3 id="analyse-the-performance-of-each-algorithms-and-discuss-results.">Analyse the performance of each algorithms and discuss results.</h3>
<h3 id="are-the-results-good-enough-for-production">Are the results good enough for production?</h3>
<h3 id="is-the-ml-algorithm-training-and-inference-completing-in-a-reasonable-timeframe">Is the ML algorithm training and inference completing in a reasonable timeframe?</h3>
<h2 id="scaling">Scaling</h2>
<h3 id="how-does-my-algorithm-scale-for-both-training-and-inference">How does my algorithm scale for both training and inference?</h3>
<h2 id="deployment-and-operationalisation">Deployment and Operationalisation</h2>
<h3 id="how-can-feature-manipulation-be-done-for-training-and-inference-in-real-time">How can feature manipulation be done for training and inference in real-time?</h3>
<h3 id="how-to-make-sure-that-the-algorithm-is-retrained-periodically-and-deployed-into-production">How to make sure that the algorithm is retrained periodically and deployed into production?</h3>
<h3 id="how-will-the-ml-algorithms-be-integrated-with-other-systems">How will the ML algorithms be integrated with other systems?</h3>
<h2 id="infrastructure">Infrastructure</h2>
<h3 id="can-the-infrastructure-running-the-machine-learning-process-scale">Can the infrastructure running the machine learning process scale?</h3>
<h3 id="how-is-access-to-the-ml-algorithm-provided-rest-api-sdk">How is access to the ML algorithm provided? REST API? SDK?</h3>
<h3 id="is-the-infrastructure-appropriate-for-the-algorithm-we-are-running-cpus-or-gpus">Is the infrastructure appropriate for the algorithm we are running? CPU's or GPU's?</h3>
<h2 id="direction">Direction</h2>
<h3 id="saas---pre-built-machine-learning-models">SaaS - Pre-built Machine Learning models</h3>
<ul>
<li><p>Google Cloud</p>
<ul>
<li><p>Vision API</p></li>
<li><p>Speech API</p></li>
<li><p>Jobs API</p></li>
<li><p>Video Intelligence API</p></li>
<li><p>Language API</p></li>
<li><p>Translation API</p></li>
</ul></li>
<li><p>AWS</p>
<ul>
<li><p>Rekognition</p></li>
<li><p>Lex</p></li>
<li><p>Polly</p></li>
</ul></li>
<li><p>… many others</p></li>
</ul>
<h3 id="data-science-and-applied-machine-learning">Data Science and Applied Machine Learning</h3>
<ul>
<li><p>Google Cloud</p>
<ul>
<li>ML Engine</li>
</ul></li>
<li><p>AWS</p>
<ul>
<li>Amazon Machine Learning</li>
</ul></li>
<li><p>Tools: Jupiter / Datalab / Zeppelin</p></li>
<li><p>… many others</p></li>
</ul>
<h3 id="machine-learning-research">Machine Learning Research</h3>
<ul>
<li><p>Tensorflow</p></li>
<li><p>MXNet</p></li>
<li><p>Torch</p></li>
<li><p>… many others</p></li>
</ul>
<h2 id="question">Question</h2>
<h3 id="is-this-a-or-b">Is this A or B?</h3>
<ul>
<li>Classification</li>
</ul>
<h3 id="how-much-or-how-many-of-these">How much, or how many of these?</h3>
<ul>
<li>Regression</li>
</ul>
<h3 id="is-this-anomalous">Is this anomalous?</h3>
<ul>
<li>Anomaly Detection</li>
</ul>
<h3 id="how-can-these-elements-be-grouped">How can these elements be grouped?</h3>
<ul>
<li>Clustering</li>
</ul>
<h3 id="what-should-i-do-now">What should I do now?</h3>
<ul>
<li>Reinforcement Learning</li>
</ul>
<h1 id="machine-learning-mathematics">Machine Learning Mathematics</h1>
<h2 id="costlossmin-objectivemax-functions">Cost/Loss(Min) Objective(Max) Functions</h2>
<h3 id="intuition">Intuition</h3>
<ul>
<li><p>The cost function will tell us how right the predictions of our model and weight matrix are, and the choice of cost function will drive how much we care about how wrong each prediction is. For instance, a hinge loss will assume the difference between each incorrect prediction value is linear. Were we to square the hinge loss and use that as our cost function, we would be telling the system that being very wrong gets exponentially worse as we get away from the right prediction. Cross Entropy would offer a probabilistic approach.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/AAAF1D3A-8184-4843-A1E3-AC53E43315FC.png" /></li>
</ul></li>
</ul>
<h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3>
<ul>
<li><p>Many cost functions are the result of applying Maximum Likelihood. For instance, the Least Squares cost function can be obtained via Maximum Likelihood. Cross-Entropy is another example.</p></li>
<li><p>The likelihood of a parameter value (or vector of parameter values), θ, given outcomes x, is equal to the probability (density) assumed for those observed outcomes given those parameter values, that is</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/A37BBE5F-E77F-411E-BA91-5A2D475063D0.png" /></li>
</ul></li>
<li><p>The natural logarithm of the likelihood function, called the log-likelihood, is more convenient to work with. Because the logarithm is a monotonically increasing function, the logarithm of a function achieves its maximum value at the same points as the function itself, and hence the log-likelihood can be used in place of the likelihood in maximum likelihood estimation and related techniques.</p></li>
<li><p>In general, for a fixed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the <a href="https://en.wikipedia.org/wiki/Likelihood_function" target="_blank" rel="noopener">likelihood function</a>. Intuitively, this maximizes the "agreement" of the selected model with the observed data, and for discrete random variables it indeed maximizes the probability of the observed data under the resulting distribution. Maximum-likelihood estimation gives a unified approach to estimation, which is <a href="https://en.wikipedia.org/wiki/Well_defined" target="_blank" rel="noopener">well-defined</a> in the case of the <a href="https://en.wikipedia.org/wiki/Normal_distribution" target="_blank" rel="noopener">normal distribution</a> and many other problems.</p>
<ul>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/396DE3E5-85C2-487D-9324-FB83FCC7F8FD.png" /></p></li>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/690903C2-BC9F-400F-877E-7B625F2A20BD.png" /></p></li>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/B8AA9D14-192E-4C93-A166-A429A293990C.png" /></p></li>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/2EE71F38-3B82-48E7-A8B8-44FC8B2A5805.png" /></p></li>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/B7F5DF41-7571-412D-81AD-4B683CC1812B.png" /></p></li>
</ul></li>
</ul>
<h3 id="cross-entropy">Cross-Entropy</h3>
<ul>
<li><p>Cross entropy can be used to define the loss function in machine learning and optimization. The true probability pi is the true label, and the given distribution qi is the predicted value of the current model.</p>
<ul>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/9E271C6A-6FBB-41FC-960B-4AA394E6EEA5.png" /></p></li>
<li><p>Cross-entropy error function and logistic regression<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/0397888B-C44B-4178-B1D5-F7FB2ED1F3EC.png" /></p></li>
</ul></li>
</ul>
<h3 id="logistic">Logistic</h3>
<ul>
<li><p>The logistic loss function is defined as:</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/8942D559-DDAA-4EE2-AD81-3D6DE9C1540C.png" /></li>
</ul></li>
</ul>
<h3 id="quadratic">Quadratic</h3>
<ul>
<li><p>The use of a quadratic loss function is common, for example when using least squares techniques. It is often more mathematically tractable than other loss functions because of the properties of variances, as well as being symmetric: an error above the target causes the same loss as the same magnitude of error below the target. If the target is t, then a quadratic loss function is:</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C2D73C61-1BAC-4146-B59C-2C6D75CAFCF6.png" /></li>
</ul></li>
</ul>
<h3 id="loss">0-1 Loss</h3>
<ul>
<li><ul>
<li><p>In <a href="https://en.wikipedia.org/wiki/Statistics" target="_blank" rel="noopener">statistics</a> and <a href="https://en.wikipedia.org/wiki/Decision_theory" target="_blank" rel="noopener">decision theory</a>, a frequently used loss function is the 0-1 loss function</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/4597CE60-7571-403E-98C5-B3087805A418.png" /></li>
</ul></li>
</ul></li>
</ul>
<h3 id="hinge-loss">Hinge Loss</h3>
<ul>
<li><p>The hinge loss is a loss function used for training classifiers. For an intended output t = ±1 and a classifier score y, the hinge loss of the prediction y is defined as:</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B96B2BB2-CF51-4989-8BE7-1A825082D2D6.png" /></li>
</ul></li>
</ul>
<h3 id="exponential">Exponential</h3>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/329F57B1-378D-47B7-B536-0AD0F626A708.png" /></li>
</ul>
<h3 id="hellinger-distance">Hellinger Distance</h3>
<ul>
<li><p>It is used to quantify the similarity between two probability distributions. It is a type of f-divergence.</p>
<ul>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/A9DC5C23-CACF-429F-83B2-F432DA3B3F1A.png" /></p></li>
<li><p>To define the Hellinger distance in terms of <a href="https://en.wikipedia.org/wiki/Measure_theory" target="_blank" rel="noopener">measure theory</a>, let P and Q denote two <a href="https://en.wikipedia.org/wiki/Probability_measure" target="_blank" rel="noopener">probability measures</a> that are <a href="https://en.wikipedia.org/wiki/Absolute_continuity" target="_blank" rel="noopener">absolutely continuous</a> with respect to a third probability measure λ. The square of the Hellinger distance between P and Q is defined as the quantity</p></li>
</ul></li>
</ul>
<h3 id="kullback-leibler-divengence">Kullback-Leibler Divengence</h3>
<ul>
<li><p>Is a measure of how one probability distribution diverges from a second expected probability distribution. Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference.</p>
<ul>
<li><p>Discrete<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/FC311DB8-C4BD-4D96-A9B0-786CF4091DE7.png" /></p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/19746786-7A83-4A0A-B8C2-38F7E7189DFE.png" /></li>
</ul></li>
</ul></li>
</ul>
<h3 id="itakurasaito-distance">Itakura–Saito distance</h3>
<ul>
<li><p>is a measure of the difference between an original spectrum P(ω) and an approximation<br />
P^(ω) of that spectrum. Although it is not a perceptual measure, it is intended to reflect perceptual (dis)similarity.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/55037B97-44EC-4173-9D7E-2755F6648E9C.png" /></li>
</ul></li>
</ul>
<h3 id="httpsstats.stackexchange.comquestions154879a-list-of-cost-functions-used-in-neural-networks-alongside-applications">https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications</h3>
<h3 id="httpsen.wikipedia.orgwikiloss_functions_for_classification">https://en.wikipedia.org/wiki/Loss_functions_for_classification</h3>
<h2 id="probability">Probability</h2>
<h3 id="concepts">Concepts</h3>
<ul>
<li><p>Frequentist vs Bayesian Probability</p>
<ul>
<li><p>Frequentist</p>
<ul>
<li>Basic notion of probability: # Results / # Attempts</li>
</ul></li>
<li><p>Bayesian</p>
<ul>
<li>The probability is not a number, but a distribution itself.</li>
</ul></li>
<li><p>http://www.behind-the-enemy-lines.com/2008/01/are-you-bayesian-or-frequentist-or.html</p></li>
</ul></li>
<li><p>Random Variable</p>
<ul>
<li><p>In <a href="https://en.wikipedia.org/wiki/Probability_and_statistics" target="_blank" rel="noopener">probability and statistics</a>, a random variable, random quantity, aleatory variable or stochastic variable is a <a href="https://en.wikipedia.org/wiki/Variable_(mathematics)" target="_blank" rel="noopener">variable</a> whose value is subject to variations due to chance (i.e. <a href="https://en.wikipedia.org/wiki/Randomness" target="_blank" rel="noopener">randomness</a>, in a mathematical sense). A random variable can take on a set of possible different values (similarly to other mathematical variables), each with an associated probability, in contrast to other mathematical variables.</p>
<ul>
<li><p>Expectation (Expected Value) of a Random Variable<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/2956137E-E540-471C-A34B-66BBE0507483.png" /></p>
<ul>
<li>Same, for continuous variables<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/54388388-46D3-4E97-A228-23669D4E1E88.png" /></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Independence</p>
<ul>
<li><p>Two <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)" target="_blank" rel="noopener">events</a> are independent, statistically independent, or stochastically independent if the occurrence of one does not affect the probability of the other.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/FE866A04-B684-4395-92EB-E73593004643.png" /></li>
</ul></li>
</ul></li>
<li><p>Conditionality</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/E07D7B09-0747-48C9-ADC6-87AA26B8D3BF.png" /></li>
</ul></li>
<li><p>Bayes Theorem (rule, law)</p>
<ul>
<li><p>Simple Form<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/870B8130-69C4-4DBD-9BCF-D8D96D3C7D77.png" /></p>
<ul>
<li>With Law of Total probability<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/E90A57EE-85B2-4601-BF52-1F71E557A13F.png" /></li>
</ul></li>
</ul></li>
<li><p>Marginalisation</p>
<ul>
<li><p>The marginal distribution of a <a href="https://en.wikipedia.org/wiki/Subset" target="_blank" rel="noopener">subset</a> of a collection of <a href="https://en.wikipedia.org/wiki/Random_variable" target="_blank" rel="noopener">random variables</a> is the <a href="https://en.wikipedia.org/wiki/Probability_distribution" target="_blank" rel="noopener">probability distribution</a> of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables.</p>
<ul>
<li><p>Continuous<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/4D1F3F76-3341-47A5-96C1-9097E5C87A38.png" /></p>
<ul>
<li><p>Discrete<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/860CED1C-EA17-44D3-A99E-B696A87A92BF.png" /></p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/7E9DE18D-616C-45BB-A0C2-02C7E469F034.png" /></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Law of Total Probability</p>
<ul>
<li><p>Is a fundamental rule relating <a href="https://en.wikipedia.org/wiki/Marginal_probability" target="_blank" rel="noopener">marginal probabilities</a> to <a href="https://en.wikipedia.org/wiki/Conditional_probabilities" target="_blank" rel="noopener">conditional probabilities</a>. It expresses the total probability of an outcome which can be realized via several distinct events - hence the name.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5F52537E-8AF1-42FA-8CB4-2B2549F3FDA7.png" /></li>
</ul></li>
</ul></li>
<li><p>Chain Rule</p>
<ul>
<li>Permits the calculation of any member of the <a href="https://en.wikipedia.org/wiki/Joint_distribution" target="_blank" rel="noopener">joint distribution</a> of a set of <a href="https://en.wikipedia.org/wiki/Random_variables" target="_blank" rel="noopener">random variables</a> using only <a href="https://en.wikipedia.org/wiki/Conditional_probabilities" target="_blank" rel="noopener">conditional probabilities</a>.<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/4739C552-28D0-4FAB-80B1-10DC7BBE1014.png" /></li>
</ul></li>
<li><p>Bayesian Inference</p>
<ul>
<li>Bayesian inference derives the <a href="https://en.m.wikipedia.org/wiki/Posterior_probability" target="_blank" rel="noopener">posterior probability</a> as a <a href="https://en.m.wikipedia.org/wiki/Consequence_relation" target="_blank" rel="noopener">consequence</a> of two <a href="https://en.m.wikipedia.org/wiki/Antecedent_(logic)" target="_blank" rel="noopener">antecedents</a>, a <a href="https://en.m.wikipedia.org/wiki/Prior_probability" target="_blank" rel="noopener">prior probability</a> and a "<a href="https://en.m.wikipedia.org/wiki/Likelihood_function" target="_blank" rel="noopener">likelihood function</a>" derived from a <a href="https://en.m.wikipedia.org/wiki/Statistical_model" target="_blank" rel="noopener">statistical model</a> for the observed data. Bayesian inference computes the posterior probability according to <a href="https://en.m.wikipedia.org/wiki/Bayes%27_theorem" target="_blank" rel="noopener">Bayes' theorem</a>. It can be applied iteratively so to update the confidence on out hypothesis.<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/5EFA0B04-EB55-4414-8DFF-69C42072527A.png" /></li>
</ul></li>
</ul>
<h2 id="distributions">Distributions</h2>
<h3 id="definition">Definition</h3>
<ul>
<li>Is a table or an equation that links each outcome of a statistical experiment with the probability of occurence. When Continuous, is is described by the Probability Density Function</li>
</ul>
<h3 id="types-density-function">Types (Density Function)</h3>
<ul>
<li><p>Normal (Gaussian)<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/6BAD212C-E812-4D94-887E-B8FC28594153.png" /></p>
<ul>
<li><p>Poisson<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/B3457C07-7876-49F4-9FDA-5516DECF2E65.png" /></p>
<ul>
<li>Uniform<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/CFF452C6-1F11-465E-BDE8-EA3B3D55251D.png" /></li>
</ul></li>
</ul></li>
<li><p>Bernoulli<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/8F02DF2E-0723-4EDA-83E1-04DEBB0A222F.png" /></p>
<ul>
<li><p>Gamma<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/F2C3B775-C53A-4AA9-A398-120FBDFF6EF3.png" /></p>
<ul>
<li>Binomial<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/FF9BC15A-3117-4FA4-B59D-9EBD6E46F3A6.png" /></li>
</ul></li>
</ul></li>
</ul>
<h3 id="cumulative-distribution-function-cdf">Cumulative Distribution Function (CDF)</h3>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D6AD4AEC-161F-4A81-A996-50C936E3752B.png" /></li>
</ul>
<h2 id="information-theory">Information Theory</h2>
<h3 id="entropy">Entropy</h3>
<p><img src="https://2020.iosdevlog.com/2020/02/22/ds/E0EC20FC-0906-4B7C-A75F-27338A35CB48.png" /></p>
<ul>
<li><p>Entropy is a measure of unpredictability of information content.</p>
<ul>
<li>To evaluate a language model, we should measure how much surprise it gives us for real sequences in that language. For each real word encountered, the language model will give a probability p. And we use -log(p) to quantify the surprise. And we average the total surprise over a long enough sequence. So, in case of a 1000-letter sequence with 500 A and 500 B, the surprise given by the 1/3-2/3 model will be:<br />
[-500<em>log(1/3) - 500</em>log(2/3)]/1000 = 1/2 * Log(9/2)<br />
While the correct 1/2-1/2 model will give:<br />
[-500<em>log(1/2) - 500</em>log(1/2)]/1000 = 1/2 * Log(8/2)<br />
So, we can see, the 1/3, 2/3 model gives more surprise, which indicates it is worse than the correct model.<br />
Only when the sequence is long enough, the average effect will mimic the expectation over the 1/2-1/2 distribution. If the sequence is short, it won't give a convincing result.</li>
</ul></li>
</ul>
<h3 id="cross-entropy-1">Cross Entropy</h3>
<p><img src="https://2020.iosdevlog.com/2020/02/22/ds/BCFC979D-F514-45F2-8543-8AFC4539CD08.png" /></p>
<ul>
<li>Cross entropy between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an "unnatural" probability distribution q, rather than the "true" distribution p.</li>
</ul>
<h3 id="joint-entropy">Joint Entropy</h3>
<p><img src="https://2020.iosdevlog.com/2020/02/22/ds/250ECB8A-F48D-4814-B542-2C3FC45B2127.png" /></p>
<h3 id="conditional-entropy">Conditional Entropy</h3>
<p><img src="https://2020.iosdevlog.com/2020/02/22/ds/97F11D13-2F43-499D-9E53-5CE038479F74.png" /></p>
<h3 id="mutual-information">Mutual Information</h3>
<p><img src="https://2020.iosdevlog.com/2020/02/22/ds/B0FFBB3D-70E6-4C13-AFEC-4E1E4F133926.png" /></p>
<h3 id="kullback-leibler-divergence">Kullback-Leibler Divergence</h3>
<p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F669FC47-9F7C-45BB-9311-148E23624F68.png" /></p>
<h2 id="density-estimation-1">Density Estimation</h2>
<h3 id="mostly-non-parametric.-parametric-makes-assumptions-on-my-datarandom-variables-for-instance-that-they-are-normally-distributed.-non-parametric-does-not.">Mostly Non-Parametric. Parametric makes assumptions on my data/random-variables, for instance, that they are normally distributed. Non-parametric does not.</h3>
<h3 id="the-methods-are-generally-intended-for-description-rather-than-formal-inference">The methods are generally intended for description rather than formal inference</h3>
<h3 id="methods">Methods</h3>
<ul>
<li><p>Kernel Density Estimation<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/7886A650-E942-414E-B9C1-FF657BFA8738.png" /></p>
<ul>
<li><p>non-negative</p></li>
<li><p>it’s a type of PDF that it is symmetric</p></li>
<li><p>real-valued</p></li>
<li><p>symmetric</p></li>
<li><p>integral over function is equal to 1</p></li>
<li><p>non-parametric</p></li>
<li><p>calculates kernel distributions for every sample point, and then adds all the distributions</p></li>
<li><p>Uniform, Triangle, Quartic, Triweight, Gaussian, Cosine, others...</p></li>
</ul></li>
<li><p>Cubic Spline</p>
<ul>
<li>A cubic spline is a function created from cubic polynomials on each between-knot interval by pasting them together twice continuously differentiable at the knots.</li>
</ul></li>
</ul>
<h2 id="regularization">Regularization</h2>
<h3 id="l1-norm">L1 norm</h3>
<ul>
<li><p>Manhattan Distance</p>
<ul>
<li><p>L1-norm is also known as least absolute deviations (LAD), least absolute errors (LAE). It is basically minimizing the sum of the absolute differences (S) between the target value and the estimated values.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C8581E8F-5C6B-424E-839D-9A01C3BEAE53.png" /></li>
</ul></li>
<li><p>Intuitively, the L1 norm prefers a weight matrix which contains the larger number of zeros.</p></li>
</ul></li>
</ul>
<h3 id="l2-norm">L2 norm</h3>
<ul>
<li><p>Euclidean Distance</p>
<ul>
<li><p>L2-norm is also known as least squares. It is basically minimizing the sum of the square of the differences (S) between the target value and the estimated values:</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/7839524D-1CE8-4634-A5D3-8B493ADBF0CE.png" /></li>
</ul></li>
<li><p>Intuitively, the L2 norm prefers a weight matrix where the norm is distributed across all weight matrix entries.</p></li>
</ul></li>
</ul>
<h3 id="early-stopping">Early Stopping</h3>
<ul>
<li>Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit, and stop the algorithm then.</li>
</ul>
<h3 id="dropout">Dropout</h3>
<ul>
<li>Is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term "dropout" refers to dropping out units (both hidden and visible) in a neural network</li>
</ul>
<h3 id="sparse-regularizer-on-columns">Sparse regularizer on columns</h3>
<ul>
<li><p>This regularizer defines an L2 norm on each column and an L1 norm over all columns. It can be solved by proximal methods.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/DF7A3A69-3945-4222-86DA-EF91B8FF6740.png" /></li>
</ul></li>
</ul>
<h3 id="nuclear-norm-regularization">Nuclear norm regularization</h3>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5D348DC1-B376-4FCA-9FA5-F2C2E4697D84.png" /></li>
</ul>
<h3 id="mean-constrained-regularization">Mean-constrained regularization</h3>
<ul>
<li><p>This regularizer constrains the functions learned for each task to be similar to the overall average of the functions across all tasks. This is useful for expressing prior information that each task is expected to share similarities with each other task. An example is predicting blood iron levels measured at different times of the day, where each task represents a different person.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/AED450B9-7864-4B51-8554-0B9A8CBB92BE.png" /></li>
</ul></li>
</ul>
<h3 id="clustered-mean-constrained-regularization">Clustered mean-constrained regularization</h3>
<ul>
<li><p>This regularizer is similar to the mean-constrained regularizer, but instead enforces similarity between tasks within the same cluster. This can capture more complex prior information. This technique has been used to predict Netflix recommendations.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/4427EABD-9457-44F5-AE53-47D654CA898C.png" /></li>
</ul></li>
</ul>
<h3 id="graph-based-similarity">Graph-based similarity</h3>
<ul>
<li><p>More general than above, similarity between tasks can be defined by a function. The regularizer encourages the model to learn similar functions for similar tasks.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B26E5AA5-905F-49BB-A492-F0D34378ACC2.png" /></li>
</ul></li>
</ul>
<h2 id="optimization-1">Optimization</h2>
<h3 id="gradient-descent">Gradient Descent</h3>
<ul>
<li>Is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.</li>
</ul>
<h3 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h3>
<ul>
<li><p>Gradient descent uses total gradient over all examples per update, SGD updates after only 1 or few examples:</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/187B983B-A9BE-4B1C-8ACF-ACB40B0E5725.png" /></li>
</ul></li>
</ul>
<h3 id="mini-batch-stochastic-gradient-descent-sgd">Mini-batch Stochastic Gradient Descent (SGD)</h3>
<ul>
<li>Gradient descent uses total gradient over all examples per update, SGD updates after only 1 example</li>
</ul>
<h3 id="momentum">Momentum</h3>
<ul>
<li>Idea: Add a fraction v of previous update to current one. When the gradient keeps pointing in the same direction, this will<br />
increase the size of the steps taken towards the minimum.</li>
</ul>
<h3 id="adagrad">Adagrad</h3>
<ul>
<li>Adaptive learning rates for each parameter</li>
</ul>
<h2 id="statistics">Statistics</h2>
<h3 id="measures-of-central-tendency">Measures of Central Tendency</h3>
<ul>
<li><p>Mean</p></li>
<li><p>Median</p>
<ul>
<li>Value in the middle or an ordered list, or average of two in middle.</li>
</ul></li>
<li><p>Mode</p>
<ul>
<li>Most Frequent Value</li>
</ul></li>
<li><p>Quantile</p>
<ul>
<li>Division of probability distributions based on contiguous intervals with equal probabilities. In short: Dividing observations numbers in a sample list equally.</li>
</ul></li>
</ul>
<h3 id="dispersion">Dispersion</h3>
<ul>
<li><p>Range</p></li>
<li><p>Medium Absolute Deviation (MAD)</p>
<ul>
<li>The average of the absolute value of the deviation of each value from the mean</li>
</ul></li>
<li><p>Inter-quartile Range (IQR)</p>
<ul>
<li>Three quartiles divide the data in approximately four equally divided parts</li>
</ul></li>
<li><p>Variance</p>
<ul>
<li><p>Definition</p>
<ul>
<li>The average of the squared differences from the Mean. Formally, is the expectation of the squared deviation of a random variable from its mean, and it informally measures how far a set of (random) numbers are spread out from their mean.</li>
</ul></li>
<li><p>Types</p>
<ul>
<li><p>Continuous<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/19DB13EA-7024-46CD-9404-64E8EA2850A4.png" /></p>
<ul>
<li>Discrete<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/C416473F-2CEC-4F12-8E9D-B0D7D1C686A6.png" /></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Standard Deviation</p>
<ul>
<li><p>sqrt(variance)<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/15CD2CF7-B982-4C9C-B69F-33D20657E7A2.png" /></p>
<ul>
<li><p>z-score/value/factor</p>
<ul>
<li>The signed number of <a href="https://en.wikipedia.org/wiki/Standard_deviation" target="_blank" rel="noopener">standard deviations</a> an observation or <a href="https://en.wikipedia.org/wiki/Data" target="_blank" rel="noopener">datum</a> is above the <a href="https://en.wikipedia.org/wiki/Mean" target="_blank" rel="noopener">mean</a>.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 id="relationship">Relationship</h3>
<ul>
<li><p>Covariance</p>
<ul>
<li><p>dot(de_mean(x), de_mean(y)) / (n - 1)<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/0C8FD4CE-AC05-4975-ABDD-B54BD11F8312.png" /></p>
<ul>
<li>A measure of how much two random variables change together. http://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean</li>
</ul></li>
</ul></li>
<li><p>Correlation</p>
<ul>
<li><p>Pearson</p>
<ul>
<li>Benchmarks linear relationship, most appropriate for measurements taken from an interval scale, is a measure of the linear dependence between two variables<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/65EE6640-3536-4A5D-8935-020FDEA6FCDB.png" /></li>
</ul></li>
<li><p>Spearman</p>
<ul>
<li>Benchmarks monotonic relationship (whether linear or not), Spearman's coefficient is appropriate for both continuous and discrete variables, including ordinal variables.</li>
</ul></li>
<li><p>Kendall</p>
<ul>
<li><p>Is a <a href="https://en.wikipedia.org/wiki/Statistic" target="_blank" rel="noopener">statistic</a> used to measure the <a href="https://en.wikipedia.org/wiki/Ordinal_association" target="_blank" rel="noopener">ordinal association</a> between two measured quantities.</p></li>
<li><p>Contrary to the <a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" target="_blank" rel="noopener">Spearman correlation</a>, the Kendall correlation is not affected by how far from each other ranks are but only by whether the ranks between observations are equal or not, and is thus only appropriate for <a href="https://en.wikipedia.org/wiki/Discrete_variable" target="_blank" rel="noopener">discrete variables</a> but not defined for <a href="https://en.wikipedia.org/wiki/Continuous_variable" target="_blank" rel="noopener">continuous variables</a>.</p></li>
</ul></li>
<li><p>Summary: Pearson’s r for two normally distributed variables // Spearman’s rho for ratio data, ordinal data, etc (rank-order correlation) // Kendall’s tau for ordinal variables</p></li>
</ul></li>
<li><p>Co-occurrence</p>
<ul>
<li>The results are presented in a matrix format, where the cross tabulation of two fields is a cell value. The cell value represents the percentage of times that the two fields exist in the same events.</li>
</ul></li>
</ul>
<h3 id="techniques">Techniques</h3>
<ul>
<li><p>Null Hypothesis</p>
<ul>
<li>Is a general statement or default position that there is no relationship between two measured phenomena, or no association among groups. The null hypothesis is generally assumed to be true until evidence indicates otherwise.</li>
</ul></li>
<li><p>p-value</p>
<ul>
<li><p>Five heads in a row Example</p>
<ul>
<li><p>This demonstrates that specifying a direction (on a symmetric test statistic) halves the p-value (increases the significance) and can mean the difference between data being considered significant or not.</p></li>
<li><p>Suppose a researcher flips a coin five times in a row and assumes a null hypothesis that the coin is fair. The test statistic of "total number of heads" can be one-tailed or two-tailed: a one-tailed test corresponds to seeing if the coin is biased towards heads, but a two-tailed test corresponds to seeing if the coin is biased either way. The researcher flips the coin five times and observes heads each time (HHHHH), yielding a test statistic of 5. In a one-tailed test, this is the upper extreme of all possible outcomes, and yields a p-value of (1/2)5 = 1/32 ≈ 0.03. If the researcher assumed a significance level of 0.05, this result would be deemed significant and the hypothesis that the coin is fair would be rejected. In a two-tailed test, a test statistic of zero heads (TTTTT) is just as extreme and thus the data of HHHHH would yield a p-value of 2×(1/2)5 = 1/16 ≈ 0.06, which is not significant at the 0.05 level.</p></li>
</ul></li>
<li><p>In this method, as part of experimental design, before performing the experiment, one first chooses a model (the null hypothesis) and a threshold value for p, called the significance level of the test, traditionally 5% or 1% and denoted as α. If the p-value is less than the chosen significance level (α), that suggests that the observed data is sufficiently inconsistent with the null hypothesis that the null hypothesis may be rejected. However, that does not prove that the tested hypothesis is true. For typical analysis, using the standard α = 0.05 cutoff, the null hypothesis is rejected when p &lt; .05 and not rejected when p &gt; .05. The p-value does not, in itself, support reasoning about the probabilities of hypotheses but is only a tool for deciding whether to reject the null hypothesis.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/031EBE1C-F064-4DB0-9DE7-CABBA8D17668.png" /></li>
</ul></li>
</ul></li>
<li><p>p-hacking</p>
<ul>
<li>The process of data mining involves automatically testing huge numbers of hypotheses about a single <a href="https://en.wikipedia.org/wiki/Data_set" target="_blank" rel="noopener">data set</a> by exhaustively searching for combinations of variables that might show a correlation. Conventional tests of <a href="https://en.wikipedia.org/wiki/Statistical_significance" target="_blank" rel="noopener">statistical significance</a> are based on the probability that an observation arose by chance, and necessarily accept some risk of mistaken test results, called the <a href="https://en.wikipedia.org/wiki/Statistical_significance" target="_blank" rel="noopener">significance</a>.</li>
</ul></li>
</ul>
<h3 id="central-limit-theorem">Central Limit Theorem</h3>
<ul>
<li><p>States that a random variable defined as the average of a large number of independent and identically distributed random variables is itself approximately normally distributed.</p>
<ul>
<li>http://blog.vctr.me/posts/central-limit-theorem.html</li>
</ul></li>
</ul>
<h3 id="experiments-and-tests">Experiments and Tests</h3>
<ul>
<li><p>Flow Chart of Commonly Used Stat Tests<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/0DDFCAD3-A0C9-4978-A0F1-B47872AC82B2.png" /></p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/45D4572F-11EE-4280-A978-384D0E1D872A.png" /></li>
</ul></li>
<li><p>Research Question</p>
<ul>
<li><p>Research question (Q):</p>
<ul>
<li>Asks whether the independent variable has an effect: “If there is a change in the independent variable, will there also be a change in the dependent variable?”</li>
</ul></li>
<li><p>Null hypothesis (Ho):</p>
<ul>
<li>The assumption that there is no effect: “There is no change in the dependent variable when the independent variable changes.”</li>
</ul></li>
</ul></li>
<li><p>Types of variables</p>
<ul>
<li><p>Dependent variable is the measure of interest</p></li>
<li><p>Independent variable is manipulated to observe the effect on dependent variable</p></li>
<li><p>Controlled variables are materials, measurements and methods that don’t change</p></li>
</ul></li>
<li><p>Experiment design</p>
<ul>
<li><p>Between subjects: Each subject sees one and only one condition</p></li>
<li><p>Within subjects: Subjects see more than one or all conditions</p></li>
</ul></li>
<li><p>Testing reliability with p-values</p>
<ul>
<li><p>Most tests calculate a p-value measuring observation extremity</p></li>
<li><p>Compare to significance level threshold α</p></li>
<li><p>α is the probability of rejecting H0 given that it is true</p></li>
<li><p>Commonly use α of 5% or 1%</p></li>
</ul></li>
</ul>
<h2 id="linear-algebra">Linear Algebra</h2>
<h3 id="matrices">Matrices</h3>
<ul>
<li><p>Almost all Machine Learning algorithms use Matrix algebra in one way or another. This is a broad subject, too large to be included here in it’s full length. Here’s a start: https://en.wikipedia.org/wiki/Matrix_(mathematics)</p>
<ul>
<li><p>Basic Operations: Addition, Multiplication, Transposition</p></li>
<li><p>Transformations</p></li>
<li><p>Trace, Rank, Determinante, Inverse</p></li>
</ul></li>
</ul>
<h3 id="eigenvectors-and-eigenvalues">Eigenvectors and Eigenvalues</h3>
<ul>
<li><p>In <a href="https://en.wikipedia.org/wiki/Linear_algebra" target="_blank" rel="noopener">linear algebra</a>, an eigenvector or characteristic vector of a <a href="https://en.wikipedia.org/wiki/Linear_map" target="_blank" rel="noopener">linear transformation</a> T from a <a href="https://en.wikipedia.org/wiki/Vector_space" target="_blank" rel="noopener">vector space</a> V over a <a href="https://en.wikipedia.org/wiki/Field_(mathematics)" target="_blank" rel="noopener">field</a> F into itself is a non-zero <a href="https://en.wikipedia.org/wiki/Vector_space" target="_blank" rel="noopener">vector</a> that does not change its direction when that linear transformation is applied to it.</p>
<ul>
<li>http://setosa.io/ev/eigenvectors-and-eigenvalues/<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/877F41BA-7063-48C3-AA8C-26173DDA8DE4.png" /></li>
</ul></li>
</ul>
<h3 id="derivatives-chain-rule">Derivatives Chain Rule</h3>
<ul>
<li><p>Rule<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/A416F2D4-5106-41F7-8E5B-40B5E73EA434.png" /></p>
<ul>
<li>Leibniz Notation<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/D2FA6B8A-FA31-4015-87B8-5FDCC70A63F4.png" /></li>
</ul></li>
</ul>
<h3 id="jacobian-matrix">Jacobian Matrix</h3>
<ul>
<li><p>The <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)" target="_blank" rel="noopener">matrix</a> of all first-order <a href="https://en.wikipedia.org/wiki/Partial_derivative" target="_blank" rel="noopener">partial derivatives</a> of a <a href="https://en.wikipedia.org/wiki/Vector-valued_function" target="_blank" rel="noopener">vector-valued function</a>. When the matrix is a <a href="https://en.wikipedia.org/wiki/Square_matrix" target="_blank" rel="noopener">square matrix</a>, both the matrix and its <a href="https://en.wikipedia.org/wiki/Determinant" target="_blank" rel="noopener">determinant</a> are referred to as the Jacobian in literature</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/4E2AA87B-6A9A-42C7-8C3E-C2B25E198D30.png" /></li>
</ul></li>
</ul>
<h3 id="gradient">Gradient</h3>
<ul>
<li><p>The gradient is a multi-variable generalization of the derivative. The gradient is a vector-valued function, as opposed to a derivative, which is scalar-valued.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/695F6632-0CA9-4AE7-9058-6F7E6DC780B4.png" /></li>
</ul></li>
</ul>
<h3 id="tensors">Tensors</h3>
<ul>
<li><p>For Machine Learning purposes, a Tensor can be described as a Multidimentional Matrix Matrix. Depending on the dimensions, the Tensor can be a Scalar, a Vector, a Matrix, or a Multidimentional Matrix.</p>
<ul>
<li><p>When measuring the forces applied to an infinitesimal cube, one can store the force values in a multidimensional matrix.<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/1F2AE40B-6E5E-4501-AEB1-FFCAF145B311.png" /></p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/7E0FE9AC-605D-4FB4-8790-DE3B05336F1F.png" /></li>
</ul></li>
</ul></li>
</ul>
<h3 id="curse-of-dimensionality">Curse of Dimensionality</h3>
<ul>
<li>When the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality.</li>
</ul>
<h1 id="machine-learning-data-processing">Machine Learning Data Processing</h1>
<h2 id="feature-selection">Feature Selection</h2>
<h3 id="correlation">Correlation</h3>
<ul>
<li><p>Features should be uncorrelated with each other and highly correlated to the feature we’re trying to predict.<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/87F7C9A2-8634-4E57-85E2-9B3C4B9D33F8.png" /></p>
<ul>
<li><p>Covariance</p>
<ul>
<li>A measure of how much two random variables change together. Math: dot(de_mean(x), de_mean(y)) / (n - 1)<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/0C8FD4CE-AC05-4975-ABDD-B54BD11F8312.png" /></li>
</ul></li>
</ul></li>
</ul>
<h3 id="dimensionality-reduction-1">Dimensionality Reduction</h3>
<ul>
<li><p>Principal Component Analysis (PCA)</p>
<ul>
<li><p>Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.</p>
<ul>
<li>Plot the variance per feature and select the features with the largest variance.</li>
</ul></li>
</ul></li>
<li><p>Singular Value Decomposition (SVD)</p>
<ul>
<li><p>SVD is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix (for example, a symmetric matrix with positive eigenvalues) to any m×n matrix via an extension of the polar decomposition. It has many useful applications in signal processing and statistics.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/53B97C2C-8826-477D-BC26-41CABEC4A42E.png" /></li>
</ul></li>
</ul></li>
</ul>
<h3 id="importance">Importance</h3>
<ul>
<li><p>Filter Methods</p>
<ul>
<li><p>Filter type methods select features based only on general metrics like the correlation with the variable to predict. Filter methods suppress the least interesting variables. The other variables will be part of a classification or a regression model used to classify or to predict data. These methods are particularly effective in computation time and robust to overfitting.</p>
<ul>
<li><p>Correlation</p></li>
<li><p>Linear Discriminant Analysis</p></li>
<li><p>ANOVA: Analysis of Variance</p></li>
<li><p>Chi-Square</p></li>
</ul></li>
</ul></li>
<li><p>Wrapper Methods</p>
<ul>
<li><p>Wrapper methods evaluate subsets of variables which allows, unlike filter approaches, to detect the possible interactions between variables. The two main disadvantages of these methods are : The increasing overfitting risk when the number of observations is insufficient. AND. The significant computation time when the number of variables is large.</p>
<ul>
<li><p>Forward Selection</p></li>
<li><p>Backward Elimination</p></li>
<li><p>Recursive Feature Ellimination</p></li>
<li><p>Genetic Algorithms</p></li>
</ul></li>
</ul></li>
<li><p>Embedded Methods</p>
<ul>
<li><p>Embedded methods try to combine the advantages of both previous methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously.</p>
<ul>
<li><p>Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.</p></li>
<li><p>Ridge regression performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients.</p></li>
</ul></li>
</ul></li>
</ul>
<h2 id="feature-encoding">Feature Encoding</h2>
<h3 id="machine-learning-algorithms-perform-linear-algebra-on-matrices-which-means-all-features-must-be-numeric.-encoding-helps-us-do-this.">Machine Learning algorithms perform Linear Algebra on Matrices, which means all features must be numeric. Encoding helps us do this.</h3>
<h3 id="label-encoding">Label Encoding</h3>
<p><img src="https://2020.iosdevlog.com/2020/02/22/ds/397D4FCF-04A7-4D68-927F-D6414FA747FE.png" /></p>
<ul>
<li><p>One Hot Encoding<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/8F75FD12-E83A-4DE5-94ED-CDDD231E3E14.png" /></p>
<ul>
<li>In One Hot Encoding, make sure the encodings are done in a way that all features are linearly independent.</li>
</ul></li>
</ul>
<h2 id="feature-normalisation-or-scaling">Feature Normalisation or Scaling</h2>
<h3 id="section"></h3>
<ul>
<li><p>Since the range of values of raw data varies widely, in some <a href="https://en.wikipedia.org/wiki/Machine_learning" target="_blank" rel="noopener">machine learning</a> algorithms, objective functions will not work properly without <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)" target="_blank" rel="noopener">normalization</a>. Another reason why feature scaling is applied is that <a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener">gradient descent</a> converges much faster with feature scaling than without it.</p></li>
<li><p>Methods</p>
<ul>
<li><p>Rescaling</p>
<ul>
<li><p>The simplest method is rescaling the range of features to scale the range in [0, 1] or [−1, 1].</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/044BC9F5-9D15-4A64-9529-63403036B623.png" /></li>
</ul></li>
</ul></li>
<li><p>Standardization</p>
<ul>
<li><p>Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C228CF35-C057-4DF4-BEC2-6A7CB7D1C92F.png" /></li>
</ul></li>
</ul></li>
<li><p>Scaling to unit length</p>
<ul>
<li><p>To scale the components of a feature vector such that the complete vector has length one.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/1452F430-91D5-4E1A-8021-3BA21B90AD83.png" /></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h2 id="dataset-construction">Dataset Construction</h2>
<h3 id="training-dataset">Training Dataset</h3>
<ul>
<li><p>A set of examples used for learning</p>
<ul>
<li><ul>
<li>To fit the parameters of the classifier in the Multilayer Perceptron, for instance, we would use the training set to find the “optimal” weights when using back-progapation.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="test-dataset">Test Dataset</h3>
<ul>
<li><p>A set of examples used only to assess the performance of a fully-trained classifier</p>
<ul>
<li>In the Multilayer Perceptron case, we would use the test to estimate the error rate after we have chosen the final model (MLP size and actual weights) After assessing the final model on the test set, YOU MUST NOT tune the model any further.</li>
</ul></li>
</ul>
<h3 id="validation-dataset">Validation Dataset</h3>
<ul>
<li><p>A set of examples used to tune the parameters of a classifier</p>
<ul>
<li>In the Multilayer Perceptron case, we would use the validation set to find the “optimal” number of hidden units or determine a stopping point for the back-propagation algorithm</li>
</ul></li>
</ul>
<h3 id="cross-validation-1">Cross Validation</h3>
<ul>
<li>One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds.</li>
</ul>
<h2 id="feature-engineering">Feature Engineering</h2>
<h3 id="decompose">Decompose</h3>
<ul>
<li>Converting 2014-09-20T20:45:40Z into categorical attributes like hour_of_the_day, part_of_day, etc.</li>
</ul>
<h3 id="discretization">Discretization</h3>
<ul>
<li><p>Continuous Features</p>
<ul>
<li>Typically data is discretized into partitions of K equal lengths/width (equal intervals) or K% of the total data (equal frequencies).</li>
</ul></li>
<li><p>Categorical Features</p>
<ul>
<li>Values for categorical features may be combined, particularly when there’s few samples for some categories.</li>
</ul></li>
</ul>
<h3 id="reframe-numerical-quantities">Reframe Numerical Quantities</h3>
<ul>
<li>Changing from grams to kg, and losing detail might be both wanted and efficient for calculation</li>
</ul>
<h3 id="crossing">Crossing</h3>
<ul>
<li>Creating new features as a combination of existing features. Could be multiplying numerical features, or combining categorical variables. This is a great way to add domain expertise knowledge to the dataset.</li>
</ul>
<h2 id="feature-imputation">Feature Imputation</h2>
<h3 id="hot-deck">Hot-Deck</h3>
<ul>
<li>The technique then finds the first missing value and uses the cell value immediately prior to the data that are missing to impute the missing value.</li>
</ul>
<h3 id="cold-deck">Cold-Deck</h3>
<ul>
<li>Selects donors from another dataset to complete missing data.</li>
</ul>
<h3 id="mean-substitution">Mean-substitution</h3>
<ul>
<li>Another imputation technique involves replacing any missing value with the mean of that variable for all other cases, which has the benefit of not changing the sample mean for that variable.</li>
</ul>
<h3 id="regression-1">Regression</h3>
<ul>
<li>A regression model is estimated to predict observed values of a variable based on other variables, and that model is then used to impute values in cases where that variable is missing</li>
</ul>
<h3 id="some-libraries...">Some Libraries...</h3>
<p><img src="https://2020.iosdevlog.com/2020/02/22/ds/091E0B05-1B93-4959-B8C7-7E135A336EF7.png" /></p>
<h2 id="feature-cleaning">Feature Cleaning</h2>
<h3 id="missing-values">Missing values</h3>
<ul>
<li>One may choose to either omit elements from a dataset that contain missing values or to impute a value</li>
</ul>
<h3 id="special-values">Special values</h3>
<ul>
<li>Numeric variables are endowed with several formalized special values including ±Inf, NA and NaN. Calculations involving special values often result in special values, and need to be handled/cleaned</li>
</ul>
<h3 id="outliers">Outliers</h3>
<ul>
<li>They should be detected, but not necessarily removed. Their inclusion in the analysis is a statistical decision.</li>
</ul>
<h3 id="obvious-inconsistencies">Obvious inconsistencies</h3>
<ul>
<li>A person's age cannot be negative, a man cannot be pregnant and an under-aged person cannot possess a drivers license.</li>
</ul>
<h2 id="data-exploration">Data Exploration</h2>
<h3 id="variable-identification">Variable Identification</h3>
<ul>
<li>Identify Predictor (Input) and Target (output) variables. Next, identify the data type and category of the variables.</li>
</ul>
<h3 id="univariate-analysis">Univariate Analysis</h3>
<ul>
<li><p>Continuous Features</p>
<ul>
<li>Mean, Median, Mode, Min, Max, Range, Quartile, IQR, Variance, Standard Deviation, Skewness, Histogram, Box Plot</li>
</ul></li>
<li><p>Categorical Features</p>
<ul>
<li>Frequency, Histogram</li>
</ul></li>
</ul>
<h3 id="bi-variate-analysis">Bi-variate Analysis</h3>
<ul>
<li><p>Finds out the relationship between two variables.</p></li>
<li><p>Scatter Plot</p></li>
<li><p>Correlation Plot - Heatmap</p></li>
<li><ul>
<li><p>Two-way table</p>
<ul>
<li>We can start analyzing the relationship by creating a two-way table of count and count%.</li>
</ul></li>
<li><p>Stacked Column Chart</p></li>
<li><p>Chi-Square Test</p>
<ul>
<li>This test is used to derive the statistical significance of relationship between the variables.</li>
</ul></li>
<li><p>Z-Test/ T-Test</p></li>
<li><p>ANOVA</p></li>
</ul></li>
</ul>
<h2 id="data-types">Data Types</h2>
<h3 id="nominal---is-for-mutual-exclusive-but-not-ordered-categories.">Nominal - is for mutual exclusive, but not ordered, categories.</h3>
<h3 id="ordinal---is-one-where-the-order-matters-but-not-the-difference-between-values.">Ordinal - is one where the order matters but not the difference between values.</h3>
<h3 id="ratio---has-all-the-properties-of-an-interval-variable-and-also-has-a-clear-definition-of-0.0.">Ratio - has all the properties of an interval variable, and also has a clear definition of 0.0.</h3>
<h3 id="interval---is-a-measurement-where-the-difference-between-two-values-is-meaningful.">Interval - is a measurement where the difference between two values is meaningful.</h3>
<h3 id="section-1"></h3>
<p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F45BC780-34A4-4478-8204-361A57CAC7A4.png" /></p>
<ul>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F3BE7BDA-CC8D-44E9-9586-3C4EA3B875B2.png" /></p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/385CB279-89CD-4AD6-82E7-D80BAA9F2826.png" /></li>
</ul></li>
</ul>
<h1 id="machine-learning-models">Machine Learning Models</h1>
<h2 id="regression-2">Regression</h2>
<h3 id="linear-regression">Linear Regression</h3>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B3C1808C-9C79-4E45-9AC6-A5685A5D8407.png" /></li>
</ul>
<h3 id="generalised-linear-models-glms">Generalised Linear Models (GLMs)</h3>
<ul>
<li><p>Is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.</p></li>
<li><p>Link Function</p>
<ul>
<li><p>Identity</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C5A62BA8-B2BB-4325-A5CC-ADBF498E685A.png" /></li>
</ul></li>
<li><p>Inverse</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/72E0D7A0-9E26-49E9-A540-18A69873BE47.png" /></li>
</ul></li>
<li><p>Logit</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/3B5287CE-BF34-4688-844F-57875F627F8A.png" /></li>
</ul></li>
</ul></li>
<li><p>Cost Function is found via Maximum Likelihood Estimation</p></li>
</ul>
<h3 id="locally-estimated-scatterplot-smoothing-loess">Locally Estimated Scatterplot Smoothing (LOESS)</h3>
<h3 id="ridge-regression">Ridge Regression</h3>
<h3 id="least-absolute-shrinkage-and-selection-operator-lasso">Least Absolute Shrinkage and Selection Operator (LASSO)</h3>
<h3 id="logistic-regression">Logistic Regression</h3>
<ul>
<li><p>Logistic Function<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/249C3E38-0B9A-4493-982C-CC26B7614B12.png" /></p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D75A0917-3A7D-441E-B18F-A51B087761D3.png" /></li>
</ul></li>
</ul>
<h2 id="bayesian">Bayesian</h2>
<h3 id="naive-bayes">Naive Bayes</h3>
<p><img src="https://2020.iosdevlog.com/2020/02/22/ds/A1AD6FA2-19D3-4C06-A092-420D007E8E3E.png" /></p>
<ul>
<li>Naive Bayes Classifier. We neglect the denominator as we calculate for every class and pick the max of the numerator<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/1810007C-AE05-41BF-A70B-6FDA309289BC.png" /></li>
</ul>
<h3 id="multinomial-naive-bayes">Multinomial Naive Bayes</h3>
<h3 id="bayesian-belief-network-bbn">Bayesian Belief Network (BBN)</h3>
<h2 id="dimensionality-reduction-2">Dimensionality Reduction</h2>
<h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3>
<h3 id="partial-least-squares-regression-plsr">Partial Least Squares Regression (PLSR)</h3>
<h3 id="principal-component-regression-pcr">Principal Component Regression (PCR)</h3>
<h3 id="partial-least-squares-discriminant-analysis">Partial Least Squares Discriminant Analysis</h3>
<h3 id="quadratic-discriminant-analysis-qda">Quadratic Discriminant Analysis (QDA)</h3>
<h3 id="linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)</h3>
<h2 id="instance-based">Instance Based</h2>
<h3 id="k-nearest-neighbour-knn">k-nearest Neighbour (kNN)</h3>
<h3 id="learning-vector-quantization-lvq">Learning Vector Quantization (LVQ)</h3>
<h3 id="self-organising-map-som">Self-Organising Map (SOM)</h3>
<h3 id="locally-weighted-learning-lwl">Locally Weighted Learning (LWL)</h3>
<h2 id="decision-tree">Decision Tree</h2>
<h3 id="random-forest">Random Forest</h3>
<h3 id="classification-and-regression-tree-cart">Classification and Regression Tree (CART)</h3>
<h3 id="gradient-boosting-machines-gbm">Gradient Boosting Machines (GBM)</h3>
<h3 id="conditional-decision-trees">Conditional Decision Trees</h3>
<h3 id="gradient-boosted-regression-trees-gbrt">Gradient Boosted Regression Trees (GBRT)</h3>
<h2 id="clustering-2">Clustering</h2>
<h3 id="algorithms">Algorithms</h3>
<ul>
<li><p>Hierarchical Clustering</p>
<ul>
<li><p>Linkage</p>
<ul>
<li><p>complete</p></li>
<li><p>single</p></li>
<li><p>average</p></li>
<li><p>centroid</p></li>
</ul></li>
<li><p>Dissimilarity Measure</p>
<ul>
<li><p>Euclidean</p>
<ul>
<li>Euclidean distance or Euclidean metric is the "ordinary" straight-line distance between two points in Euclidean space.</li>
</ul></li>
<li><p>Manhattan</p>
<ul>
<li>The distance between two points measured along axes at right angles.</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>k-Means</p>
<ul>
<li>How many clusters do we select?</li>
</ul></li>
<li><p>k-Medians</p></li>
<li><p>Fuzzy C-Means</p></li>
<li><p>Self-Organising Maps (SOM)</p></li>
<li><p>Expectation Maximization</p></li>
<li><p>DBSCAN</p></li>
</ul>
<h3 id="validation">Validation</h3>
<ul>
<li><p>Data Structure Metrics</p>
<ul>
<li><p>Dunn Index</p></li>
<li><p>Connectivity</p></li>
<li><p>Silhouette Width</p></li>
</ul></li>
<li><p>Stability Metrics</p>
<ul>
<li><p>Non-overlap APN</p></li>
<li><p>Average Distance AD</p></li>
<li><p>Figure of Merit FOM</p></li>
<li><p>Average Distance Between Means ADM</p></li>
</ul></li>
</ul>
<h2 id="neural-networks">Neural Networks</h2>
<h3 id="unit-neurons">Unit (Neurons)</h3>
<ul>
<li><p>A unit often refers to the activation function in a layer by which the inputs are transformed via a nonlinear activation function (for example by the logistic sigmoid function). Usually, a unit has several incoming connections and several outgoing connections.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5697CF3A-4832-4849-98B9-3132500C6576.png" /></li>
</ul></li>
</ul>
<h3 id="input-layer">Input Layer</h3>
<ul>
<li>Comprised of multiple Real-Valued inputs. Each input must be linearly independent from each other.</li>
</ul>
<h3 id="hidden-layers">Hidden Layers</h3>
<ul>
<li><p>Layers other than the input and output layers. A layer is the highest-level building block in deep learning. A layer is a container that usually receives weighted input, transforms it with a set of mostly non-linear functions and then passes these values as output to the next layer.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/EB87C752-40AC-40E4-89BF-E85F3AA6AE97.png" /></li>
</ul></li>
</ul>
<h3 id="batch-normalization">Batch Normalization</h3>
<ul>
<li><p>Using mini-batches of examples, as opposed to one example at a time, is helpful in several ways. First, the gradient of the loss over a mini-batch is an estimate of the gradient over the training set, whose quality improves as the batch size increases. Second, computation over a batch can be much more efficient than m computations for individual examples, due to the parallelism afforded by the modern computing platforms.</p>
<ul>
<li>With SGD, the training proceeds in steps, and at each step we consider a mini- batch x1...m of size m. The mini-batch is used to approx- imate the gradient of the loss function with respect to the parameters.</li>
</ul></li>
</ul>
<h3 id="learning-rate">Learning Rate</h3>
<ul>
<li><p>Neural networks are often trained by gradient descent on the weights. This means at each iteration we use backpropagation to calculate the derivative of the loss function with respect to each weight and subtract it from that weight.</p>
<ul>
<li>However, if you actually try that, the weights will change far too much each iteration, which will make them “overcorrect” and the loss will actually increase/diverge. So in practice, people usually multiply each derivative by a small value called the “learning rate” before they subtract it from its corresponding weight.</li>
</ul></li>
<li><p>Tricks</p>
<ul>
<li><p>Simplest recipe: keep it fixed and use the same for all parameters.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/0FCF647C-94C1-42AA-B26D-BB5F5DB14248.png" /></li>
</ul></li>
<li><p>Better results by allowing learning rates to decrease Options:</p>
<ul>
<li><p>Reduce by 0.5 when validation error stops improving</p></li>
<li><p>Reduction by O(1/t) because of theoretical convergence guarantees, with hyper-parameters ε0 and τ and t is iteration numbers.</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5100966F-881F-45C0-9D48-807D86657D02.png" /></li>
</ul></li>
<li><p>Better yet: No hand-set learning of rates by using AdaGrad</p></li>
</ul></li>
</ul></li>
</ul>
<h3 id="weight-initialization">Weight Initialization</h3>
<ul>
<li><p>All Zero Initialization</p>
<ul>
<li><p>In the ideal situation, with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative. A reasonable-sounding idea then might be to set all the initial weights to zero, which you expect to be the “best guess” in expectation.</p>
<ul>
<li>But, this turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during back-propagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.</li>
</ul></li>
</ul></li>
<li><p>Initialization with Small Random Numbers</p>
<ul>
<li><p>Thus, you still want the weights to be very close to zero, but not identically zero. In this way, you can random these neurons to small numbers which are very close to zero, and it is treated as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network.</p>
<ul>
<li>The implementation for weights might simply drawing values from a normal distribution with zero mean, and unit standard deviation. It is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.</li>
</ul></li>
</ul></li>
<li><p>Calibrating the Variances</p>
<ul>
<li><p>One problem with the above suggestion is that the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs. It turns out that you can normalize the variance of each neuron's output to 1 by scaling its weight vector by the square root of its fan-in (i.e., its number of inputs)</p>
<ul>
<li>This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence. The detailed derivations can be found from Page. 18 to 23 of the slides. Please note that, in the derivations, it does not consider the influence of ReLU neurons.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="backpropagation">Backpropagation</h3>
<ul>
<li><p>Is a method used in artificial neural networks to calculate the error contribution of each neuron after a batch of data. It calculates the gradient of the loss function. It is commonly used in the gradient descent optimization algorithm. It is also called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers.</p>
<ul>
<li><p>Neural Network taking 4 dimension vector representation of words.<br />
<img src="https://2020.iosdevlog.com/2020/02/22/ds/76B7C54A-E48B-4D1B-A1CF-602F165D7C09.png" /></p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/DB988698-5B05-4D61-8514-5DE7B085C286.png" /></li>
</ul></li>
</ul></li>
<li><p>In this method, we reuse partial derivatives computed for higher layers in lower layers, for efficiency.</p>
<ul>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/3DAD67C3-1FCB-475E-96B7-E2D2793A9FB6.png" /></p>
<ul>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F6753EA8-E4DB-4409-978F-AB2BCC323032.png" /></p>
<ul>
<li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/4C67AB0A-2FB0-4B03-B9FA-E36203B3E37C.png" /></p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/F340BAA3-345D-4D1E-9A6A-761D6BCF4E1F.png" /></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 id="activation-functions">Activation Functions</h3>
<ul>
<li><p>Defines the output of that node given an input or set of inputs.</p></li>
<li><p>Types</p>
<ul>
<li><p>ReLU</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B6CD5EBF-451C-4E1F-AA90-DFB9FD2165FE.png" /></li>
</ul></li>
<li><p>Sigmoid / Logistic</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D54BBB93-EE9A-433D-99ED-7D4C82B94AD2.png" /></li>
</ul></li>
<li><p>Binary</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C2E4A20B-CE21-414D-82F6-D179E30C7572.png" /></li>
</ul></li>
<li><p>Tanh</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/FCECCABF-A68B-421E-8498-EDF2D313371B.png" /></li>
</ul></li>
<li><p>Softplus</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C4BDC911-1EDC-44D1-AEFA-FDC64360BA6D.png" /></li>
</ul></li>
<li><p>Softmax</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D42822B8-CE14-4B16-A24D-8151614F601D.png" /></li>
</ul></li>
<li><p>Maxout</p>
<ul>
<li><img src="https://2020.iosdevlog.com/2020/02/22/ds/9E4B97B9-A6DC-4F0D-8D8A-7030819525B1.png" /></li>
</ul></li>
<li><p>Leaky ReLU, PReLU, RReLU, ELU, SELU, and others.</p></li>
</ul></li>
</ul>
<p>参考：<a href="https://github.com/dformoso/machine-learning-mindmap" target="_blank" rel="noopener" class="uri">https://github.com/dformoso/machine-learning-mindmap</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://2020.iosdevlog.com/2020/02/22/ds/" data-id="ck71yfit4006ut42wd7ktf3qt" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DS/" rel="tag">DS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>

      
        <p><span>作  者:</span><span><a href="https://iosdevlog.com" target="_blank" rel="noopener">iOSDevLog</a></span></p>
        <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DS/" rel="tag">DS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/02/22/9787115500809/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          《极简算法史：从数学到机器的故事》读书笔记
        
      </div>
    </a>
  
  
    <a href="/2020/02/21/9787111642602/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">《机器学习算法的数学解析与Python实现》读书笔记</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/1001/">1001</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AI/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DL/">DL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/editor/">editor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/game/">game</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/party/">party</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/plan/">plan</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%87%86%E5%A4%87/">准备</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B0%8F%E6%AD%A6/">小武</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/">编译原理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%82%BA%E7%82%8E/">肺炎</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%87%AA%E6%88%91%E6%8F%90%E5%8D%87/">自我提升</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E5%88%92/">计划</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6/">读书</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ANN/" rel="tag">ANN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AR/" rel="tag">AR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algebra/" rel="tag">Algebra</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Android/" rel="tag">Android</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/App/" rel="tag">App</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Client/" rel="tag">Client</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Compiler/" rel="tag">Compiler</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Construction/" rel="tag">Construction</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Coursera/" rel="tag">Coursera</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DL/" rel="tag">DL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DS/" rel="tag">DS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deployment/" rel="tag">Deployment</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/English/" rel="tag">English</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Godot/" rel="tag">Godot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kindle/" rel="tag">Kindle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Language/" rel="tag">Language</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lua/" rel="tag">Lua</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/" rel="tag">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MediaWiki/" rel="tag">MediaWiki</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NN/" rel="tag">NN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyTorch/" rel="tag">PyTorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scene/" rel="tag">Scene</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TensorFlow/" rel="tag">TensorFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VR/" rel="tag">VR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/beginner/" rel="tag">beginner</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/download/" rel="tag">download</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/geektime/" rel="tag">geektime</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/" rel="tag">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/https/" rel="tag">https</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/iOS/" rel="tag">iOS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/js/" rel="tag">js</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kindle/" rel="tag">kindle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mobile/" rel="tag">mobile</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/" rel="tag">nlp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/paper/" rel="tag">paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/plan/" rel="tag">plan</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/plugin/" rel="tag">plugin</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/svm/" rel="tag">svm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/todo/" rel="tag">todo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vscode/" rel="tag">vscode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wiki/" rel="tag">wiki</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/year/" rel="tag">year</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%B8%9C%E9%87%8E%E5%9C%AD%E5%90%BE/" rel="tag">东野圭吾</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%B8%AD%E5%9B%BD/" rel="tag">中国</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%BA%E7%89%A9/" rel="tag">人物</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BC%BD%E5%88%A9%E7%95%A5%E7%B3%BB%E5%88%97/" rel="tag">伽利略系列</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BD%93%E8%82%B2/" rel="tag">体育</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%81%87%E9%9D%A2%E7%B3%BB%E5%88%97/" rel="tag">假面系列</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%85%8D%E8%B4%B9/" rel="tag">免费</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A0%E8%B4%BA%E6%81%AD%E4%B8%80%E9%83%8E%E7%B3%BB%E5%88%97/" rel="tag">加贺恭一郎系列</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/" rel="tag">可视化</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9C%B0%E7%90%83%E7%BC%96%E5%B9%B4%E5%8F%B2/" rel="tag">地球编年史</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%A9%E4%B8%8B%E4%B8%80%E5%A4%A7%E4%BA%94%E9%83%8E%E7%B3%BB%E5%88%97/" rel="tag">天下一大五郎系列</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/" rel="tag">极客时间</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%83%AD%E7%82%B9/" rel="tag">热点</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%9B%B4%E6%92%AD/" rel="tag">直播</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%BB%E5%8A%A8/" rel="tag">移动</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%98%E6%9C%AC/" rel="tag">绘本</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A5%BF%E7%90%B4/" rel="tag">西琴</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1/" rel="tag">设计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AF%BB%E4%B9%A6/" rel="tag">读书</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%94%99%E8%AF%AF/" rel="tag">错误</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ANN/" style="font-size: 11.43px;">ANN</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/Algebra/" style="font-size: 10px;">Algebra</a> <a href="/tags/Android/" style="font-size: 11.43px;">Android</a> <a href="/tags/App/" style="font-size: 10px;">App</a> <a href="/tags/Client/" style="font-size: 10px;">Client</a> <a href="/tags/Compiler/" style="font-size: 11.43px;">Compiler</a> <a href="/tags/Construction/" style="font-size: 11.43px;">Construction</a> <a href="/tags/Coursera/" style="font-size: 10px;">Coursera</a> <a href="/tags/DL/" style="font-size: 15.71px;">DL</a> <a href="/tags/DS/" style="font-size: 10px;">DS</a> <a href="/tags/Deployment/" style="font-size: 10px;">Deployment</a> <a href="/tags/English/" style="font-size: 10px;">English</a> <a href="/tags/Godot/" style="font-size: 17.14px;">Godot</a> <a href="/tags/Kindle/" style="font-size: 10px;">Kindle</a> <a href="/tags/Language/" style="font-size: 10px;">Language</a> <a href="/tags/Lua/" style="font-size: 10px;">Lua</a> <a href="/tags/ML/" style="font-size: 18.57px;">ML</a> <a href="/tags/Math/" style="font-size: 14.29px;">Math</a> <a href="/tags/MediaWiki/" style="font-size: 11.43px;">MediaWiki</a> <a href="/tags/NN/" style="font-size: 10px;">NN</a> <a href="/tags/PyTorch/" style="font-size: 12.86px;">PyTorch</a> <a href="/tags/Python/" style="font-size: 11.43px;">Python</a> <a href="/tags/Scene/" style="font-size: 10px;">Scene</a> <a href="/tags/TensorFlow/" style="font-size: 11.43px;">TensorFlow</a> <a href="/tags/VR/" style="font-size: 10px;">VR</a> <a href="/tags/beginner/" style="font-size: 11.43px;">beginner</a> <a href="/tags/download/" style="font-size: 10px;">download</a> <a href="/tags/geektime/" style="font-size: 10px;">geektime</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/https/" style="font-size: 10px;">https</a> <a href="/tags/iOS/" style="font-size: 11.43px;">iOS</a> <a href="/tags/js/" style="font-size: 10px;">js</a> <a href="/tags/kindle/" style="font-size: 10px;">kindle</a> <a href="/tags/mobile/" style="font-size: 10px;">mobile</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a> <a href="/tags/paper/" style="font-size: 12.86px;">paper</a> <a href="/tags/plan/" style="font-size: 10px;">plan</a> <a href="/tags/plugin/" style="font-size: 11.43px;">plugin</a> <a href="/tags/svm/" style="font-size: 10px;">svm</a> <a href="/tags/todo/" style="font-size: 10px;">todo</a> <a href="/tags/vscode/" style="font-size: 11.43px;">vscode</a> <a href="/tags/wiki/" style="font-size: 11.43px;">wiki</a> <a href="/tags/year/" style="font-size: 10px;">year</a> <a href="/tags/%E4%B8%9C%E9%87%8E%E5%9C%AD%E5%90%BE/" style="font-size: 20px;">东野圭吾</a> <a href="/tags/%E4%B8%AD%E5%9B%BD/" style="font-size: 10px;">中国</a> <a href="/tags/%E4%BA%BA%E7%89%A9/" style="font-size: 10px;">人物</a> <a href="/tags/%E4%BC%BD%E5%88%A9%E7%95%A5%E7%B3%BB%E5%88%97/" style="font-size: 10px;">伽利略系列</a> <a href="/tags/%E4%BD%93%E8%82%B2/" style="font-size: 10px;">体育</a> <a href="/tags/%E5%81%87%E9%9D%A2%E7%B3%BB%E5%88%97/" style="font-size: 10px;">假面系列</a> <a href="/tags/%E5%85%8D%E8%B4%B9/" style="font-size: 10px;">免费</a> <a href="/tags/%E5%8A%A0%E8%B4%BA%E6%81%AD%E4%B8%80%E9%83%8E%E7%B3%BB%E5%88%97/" style="font-size: 14.29px;">加贺恭一郎系列</a> <a href="/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/" style="font-size: 10px;">可视化</a> <a href="/tags/%E5%9C%B0%E7%90%83%E7%BC%96%E5%B9%B4%E5%8F%B2/" style="font-size: 10px;">地球编年史</a> <a href="/tags/%E5%A4%A9%E4%B8%8B%E4%B8%80%E5%A4%A7%E4%BA%94%E9%83%8E%E7%B3%BB%E5%88%97/" style="font-size: 10px;">天下一大五郎系列</a> <a href="/tags/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/" style="font-size: 10px;">极客时间</a> <a href="/tags/%E7%83%AD%E7%82%B9/" style="font-size: 10px;">热点</a> <a href="/tags/%E7%9B%B4%E6%92%AD/" style="font-size: 10px;">直播</a> <a href="/tags/%E7%A7%BB%E5%8A%A8/" style="font-size: 10px;">移动</a> <a href="/tags/%E7%BB%98%E6%9C%AC/" style="font-size: 10px;">绘本</a> <a href="/tags/%E8%A5%BF%E7%90%B4/" style="font-size: 10px;">西琴</a> <a href="/tags/%E8%AE%BE%E8%AE%A1/" style="font-size: 10px;">设计</a> <a href="/tags/%E8%AF%BB%E4%B9%A6/" style="font-size: 11.43px;">读书</a> <a href="/tags/%E9%94%99%E8%AF%AF/" style="font-size: 10px;">错误</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/02/25/nlp/">AI 及 NLP 基础</a>
          </li>
        
          <li>
            <a href="/2020/02/24/coursera/">Coursera 课程免费旁听与下载</a>
          </li>
        
          <li>
            <a href="/2020/02/23/TensorFlow-Data-and-Deployment/">TensorFlow-Data-and-Deployment</a>
          </li>
        
          <li>
            <a href="/2020/02/22/9787115500809/">《极简算法史：从数学到机器的故事》读书笔记</a>
          </li>
        
          <li>
            <a href="/2020/02/22/ds/">Machine Learning Concepts</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 iOSDevLog<br>
      Powered by © 2020 - <a href="https://www.iosdevlog.com" target="_blank">贾献华 2020 博客</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/2020" class="mobile-nav-link">2020 Calendar</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>