<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Game 2020</title>
  
  <subtitle>https://2020.iosdevlog.com</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://2020.iosdevlog.com/"/>
  <updated>2020-03-10T14:04:46.091Z</updated>
  <id>https://2020.iosdevlog.com/</id>
  
  <author>
    <name>iOSDevLog</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《机器学习：Python实践》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/03/10/9787121331107/"/>
    <id>https://2020.iosdevlog.com/2020/03/10/9787121331107/</id>
    <published>2020-03-10T13:53:06.000Z</published>
    <updated>2020-03-10T14:04:46.091Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/1.jpg" alt="" /><figcaption>《机器学习：Python实践》</figcaption></figure><p>书名：机器学习：Python实践<br />作者：魏贞原出版社：电子工业出版社<br />出版时间：2018-01<br />ISBN：9787121331107</p><a id="more"></a><h2 id="内容简介">内容简介</h2><p>本书系统地讲解了机器学习的基本知识，以及在实际项目中使用机器学习的基本步骤和方法；详细地介绍了在进行数据处理、分析时怎样选择合适的算法，以及建立模型并优化等方法，通过不同的例子展示了机器学习在具体项目中的应用和实践经验，是一本非常好的机器学习入门和实践的书籍。</p><p>不同于很多讲解机器学习的书籍，本书以实践为导向，使用scikit-learn作为编程框架，强调简单、快速地建立模型，解决实际项目问题。读者通过对本书的学习，可以迅速上手实践机器学习，并利用机器学习解决实际问题。</p><p><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/2.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/3.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/4.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/5.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/6.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/7.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/8.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/9.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/10.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/11.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/12.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/13.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/14.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/15.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/16.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/17.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/18.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/19.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/20.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/21.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/22.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/23.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/24.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/25.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/09/9787121331107/26.jpg" /></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/09/9787121331107/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《机器学习：Python实践》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：机器学习：Python实践&lt;br /&gt;
作者：魏贞原出版社：电子工业出版社&lt;br /&gt;
出版时间：2018-01&lt;br /&gt;
ISBN：9787121331107&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="ML" scheme="https://2020.iosdevlog.com/tags/ML/"/>
    
      <category term="Python" scheme="https://2020.iosdevlog.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>2020年每位数据科学家必读的50本免费书籍</title>
    <link href="https://2020.iosdevlog.com/2020/03/09/50-books/"/>
    <id>https://2020.iosdevlog.com/2020/03/09/50-books/</id>
    <published>2020-03-09T14:35:40.000Z</published>
    <updated>2020-03-09T15:53:54.853Z</updated>
    
    <content type="html"><![CDATA[<p>数据科学是一个跨学科领域，包含来自统计，机器学习，贝叶斯等领域的方法和技术。它们都旨在从数据中产生特定的见解。在本文中，我们列出了一些出色的数据科学书籍，其中涵盖了数据科学下的各种主题。</p><a id="more"></a><h3 id="数据分析风格的要素-the-element-of-data-analytic-style">1. <a href="https://leanpub.com/datastyle" target="_blank" rel="noopener">数据分析风格的要素 / The Element of Data Analytic Style</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/B1.png" alt="" /><figcaption>The Element of Data Analytic Style</figcaption></figure><p>本书概述了数据科学。数据科学是一个很大的概括性术语，对于那些初次尝试涉足这一领域的人来说，这本书非常有用。阅读它以了解什么是数据科学，什么是一些常规任务和算法，以及一些常规技巧和窍门。</p><h3 id="数据科学基础-foundations-of-data-science">2. <a href="https://www.cs.cornell.edu/jeh/book.pdf" target="_blank" rel="noopener">数据科学基础 / Foundations of Data Science</a></h3><p>数据科学的基础是一些选定领域的论文，这些领域构成了数据科学的基础，例如线性代数，LDA，马尔可夫链，机器学习基础和统计。本书的理想读者是希望使他们在数学和理论上更好地掌握该领域的初学者数据科学家。</p><h3 id="海量数据集的挖掘-mining-of-massive-datasets">3. <a href="http://infolab.stanford.edu/~ullman/mmds/book0n.pdf" target="_blank" rel="noopener">海量数据集的挖掘 / Mining of Massive Datasets</a></h3><p>本书基于斯坦福大学CS246和CS35A课程，可帮助用户学习在大型数据集上进行数据挖掘的主题。数据科学家通常需要解决的一个非常普遍的问题是在非常大的数据集上执行简单的数字任务（您可以通过编写小程序来完成）。MMDS正是为此而努力。除此之外，您还有诸如降维和推荐系统之类的主题，可以帮助您了解线性代数和度量距离在现实世界中的应用。所有数据科学家的必读资料。</p><h3 id="python数据科学手册-python-data-science-handbook">4. <a href="https://jakevdp.github.io/PythonDataScienceHandbook/" target="_blank" rel="noopener">Python数据科学手册 / Python Data Science Handbook</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/PDSH-cover.png" alt="" /><figcaption>PDSH-cover</figcaption></figure><p>Python数据科学手册教了各种数据科学概念在Python中的应用。也许这是学习Python数据科学的最好书（仅相当于  <a href="https://github.com/wesm/pydata-book" target="_blank" rel="noopener">Wes McKinney的鼠标书</a>），这本书也可以在Github上免费阅读。因此，您无需花任何钱就可以学习。</p><h3 id="动手机器学习和大数据-hands-on-machine-learning-and-big-data">5. <a href="http://www.kareemalkaseer.com/books/ml/" target="_blank" rel="noopener">动手机器学习和大数据 / Hands-on Machine Learning and Big Data</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/Hands_on.png" alt="" /><figcaption>Hands_on</figcaption></figure><h3 id="统计思想-think-stats">6. <a href="http://greenteapress.com/thinkstats/" target="_blank" rel="noopener">统计思想 / Think Stats</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/think_stats_comp.png" alt="" /><figcaption>think_stats_comp</figcaption></figure><p>Think Stats教给读者统计学的基础知识，也就是说，读者将在现实世界的数据集上应用统计学的概念和分布，并尝试使用数学特征来了解有关数据的更多信息。如果您想使用Python学习统计信息，可能是最好的入门书籍之一。</p><h3 id="贝叶斯思想-think-bayes">7. <a href="https://greenteapress.com/wp/think-bayes/" target="_blank" rel="noopener">贝叶斯思想 / Think Bayes</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/think_bayes_cover_medium.png" alt="" /><figcaption>think_bayes_cover_medium</figcaption></figure><p>贝叶斯统计的工作原理与正常统计有所不同。不确定性和对真实数据集的拟合分布的概念使贝叶斯方法更适合于学习真实数据集。唐尼教授非常酷的“通过使用Python进行编程学习”的风格使这本书成为那些开始使用贝叶斯方法的人的不二之选。</p><h3 id="线性动力系统简介-introduction-to-linear-dynamical-systems">8. <a href="http://ee263.stanford.edu/" target="_blank" rel="noopener">线性动力系统简介 / Introduction to Linear Dynamical Systems</a></h3><p>这本书讲授了实际系统中的应用线性代数。这些应用程序涉及电路，信号处理，通信和控制系统。可在<a href="https://web.stanford.edu/class/archive/ee/ee263/ee263.1082/notes/ee263coursereader.pdf" target="_blank" rel="noopener">此处</a>找到与博伊德教授前几年课程笔记的链接  。</p><h3 id="凸优化-convex-optimization">9. <a href="http://stanford.edu/~boyd/cvxbook/" target="_blank" rel="noopener">凸优化 / Convex Optimization</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/bv_cvxbook_cover.jpg" alt="" /><figcaption>bv_cvxbook_cover</figcaption></figure><p>凸优化是许多机器学习（以及几乎所有深度学习算法）算法在后台用于获得最佳参数集的功能。</p><h3 id="启发式方法基础-essentials-of-metaheuristics">10. <a href="https://cs.gmu.edu/~sean/book/metaheuristics/" target="_blank" rel="noopener">启发式方法基础 / Essentials of Metaheuristics</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/Luke2013.jpg" alt="" /><figcaption>Luke2013</figcaption></figure><p>启发式方法是一种快速学习概率方法来完成任务的方法，这些方法可能需要您编写程序才能使用蛮力搜索。对于也许较小的数据，蛮力方法的实现工作量较小，但是随着添加的数据量的增加，它们将很快耗尽。这本书可能是遗传算法，爬山，协同进化和（基本）强化学习等元启发式方法的最佳介绍。</p><h3 id="python中的机器学习数据科学机器学习和人工智能的主要发展和技术趋势-machine-learning-in-python-main-developments-and-technology-trends-in-data-science-machine-learning-and-artificial-intelligence">11. <a href="https://arxiv.org/abs/2002.04803" target="_blank" rel="noopener">Python中的机器学习：数据科学，机器学习和人工智能的主要发展和技术趋势 / Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence</a></h3><p>数据科学中的Python工具的良好概述。对于想要进入数据科学领域的高级Python开发人员或从R for Data Science进入Python的人员来说，这是一个非常不错的文档。总体而言，如果您想了解Python对数据科学的作用，则应阅读本文。</p><h3 id="应用数据科学-applied-data-science">12. <a href="https://columbia-applied-data-science.github.io/appdatasci.pdf" target="_blank" rel="noopener">应用数据科学 / Applied Data Science</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/ads.png" alt="" /><figcaption>ads</figcaption></figure><p>Langmore和Krasner撰写的Applied Data Science是一本采用非常实用的方法教授Data Science的书。通过使用Git进行基础Python的教学，本书继续构建了各种算法的基础知识，这些算法在数据科学领域中经​​常使用。</p><h3 id="强盗书-bandit-book">13. <a href="https://tor-lattimore.com/downloads/book/book.pdf" target="_blank" rel="noopener">强盗书 / Bandit Book</a></h3><p>随着越来越多的数据积累，决策不再只是直觉的功能，而是所收集数据的功能。电子商务网站上用于进行药物测试和财务投资组合决策的“购买”按钮的正确颜色是什么，随处都使用强盗算法？一本很好的让自己熟悉“匪徒”的书！</p><h3 id="注释算法-annotated-algorithms">14. <a href="https://tor-lattimore.com/downloads/book/book.pdf" target="_blank" rel="noopener">注释算法 / Annotated Algorithms</a></h3><p>教您用Python编写许多数值算法的书。如果您想学习数学程序的实现方式，或者想通过有趣的问题陈述学习Python，这是一个极好的资源。</p><h3 id="计算机时代统计推断-computer-age-statistical-inference">15. <a href="https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf" target="_blank" rel="noopener">计算机时代统计推断 / Computer Age Statistical Inference</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/casi.png" alt="" /><figcaption>casi</figcaption></figure><p>埃夫隆（Efron）和传奇的哈斯提（Hastie）所著的一本书，思考了如何利用当今可用的计算能力，而不是大多数其他书籍所采用的笔纸方法，在现代进行统计推断（常客和贝叶斯）。这是打算在现实生活中使用“统计信息”的任何人（初学者或有经验的人）必须阅读的内容。</p><h3 id="因果推论书-causal-inference-book">16. <a href="https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/" target="_blank" rel="noopener">因果推论书 / Causal Inference Book</a></h3><p>“关联不是因果关系”是数据科学家经常使用的短语。但是如何将两者分开？本书通过向数据科学家介绍因果推理技术来提供答案。您将需要良好的概率基础来阅读它，而不是针对初学者。</p><h3 id="计算最优运输-computational-optimal-transport">17. <a href="https://arxiv.org/abs/1803.00567" target="_blank" rel="noopener">计算最优运输 / Computational Optimal Transport</a></h3><p>最优运输是从一组分布到另一组分布的分配数学。这可能是数据科学领域中获得过多个菲尔兹奖章（数学领域的最高荣誉）的少数领域之一。数学概念已在许多机器学习和深度学习算法中用作距离度量和分配问题解决方案。</p><h3 id="计算机科学与机器学习的代数拓扑微积分和优化理论-algebra-topology-differential-calculus-and-optimization-theory-for-computer-science-and-machine-learning">18. <a href="https://www.cis.upenn.edu/~jean/math-deep.pdf" target="_blank" rel="noopener">计算机科学与机器学习的代数，拓扑，微积分和优化理论 / Algebra, Topology, Differential Calculus and Optimization Theory for Computer Science and Machine Learning</a></h3><p>该书旨在教授计算机科学和机器学习所需的各种数学领域。对于那些想从数学重领域进入数据科学领域的人来说，这是相当不错的数学知识和丰富的资源。</p><h3 id="数据挖掘与分析-data-mining-and-analysis">19. <a href="http://www.dataminingbook.info/pmwiki.php" target="_blank" rel="noopener">数据挖掘与分析 / Data Mining and Analysis</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/bookpic.jpg" alt="" /><figcaption>bookpic</figcaption></figure><p>正如您在前面提到的更著名的MMDS书中可能已经看到的那样，数据挖掘是一种有效地对大型数据集进行计算的方法。这些计算可以通过蛮力方法完成，并且在小型数据集上可能效果很好，但是在大型数据集上运行可能要花很长时间。很好的数据挖掘入门和参考书。</p><h3 id="计算与推理-computational-and-inferential-thinking">20. <a href="https://www.inferentialthinking.com/chapters/intro.html" target="_blank" rel="noopener">计算与推理 / Computational and Inferential Thinking</a></h3><p>从使用Python进行编程，因果关系，表，可视化和基本统计​​信息的角度研究数据科学的各个方面。从加州大学伯克利分校的一门基础课程开始，对初学者来说是一个很好的资源。</p><h3 id="数据科学的数学基础-mathematical-foundations-of-data-science">21. <a href="https://mathematical-tours.github.io/book-sources/FundationsDataScience.pdf" target="_blank" rel="noopener">数据科学的数学基础 / Mathematical Foundations of Data Science</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/mfd.png" alt="" /><figcaption>mfd</figcaption></figure><p>顾名思义，本书给出并解释了诸如凸优化和降维之类的数据科学概念背后的数学论文。如果您喜欢数学，或者特别想学习这些概念背后的数学，则推荐这本书。</p><h3 id="聪明人的信息论-information-theory-for-intelligent-people">22. <a href="http://tuvalu.santafe.edu/~simon/it.pdf" target="_blank" rel="noopener">聪明人的信息论 / Information Theory for Intelligent People</a></h3><p>信息论是与线性代数，凸优化和统计一起在数据科学中发现的四种数学理论之一。这是理解理论的好教程。好消息是，初学者可以使用本教程。</p><h3 id="应用线性代数简介-vmls书-introduction-to-applied-linear-algebra-the-vmls-book">23. <a href="http://vmls-book.stanford.edu/" target="_blank" rel="noopener">应用线性代数简介– VMLS书 / Introduction to Applied Linear Algebra – The VMLS Book</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/vmls_cover.jpg" alt="" /><figcaption>vmls_cover</figcaption></figure><p>在此列表中我将提到的众多书中，我最喜欢的线性代数书。初学者可以使用它，并具有非常实用的感觉，而不会使读者迷失于许多数学概念中。</p><h3 id="线性代数-hefferon-linear-algebra-hefferon">24. <a href="http://joshua.smcvt.edu/linearalgebra/" target="_blank" rel="noopener">线性代数– Hefferon / Linear Algebra – Hefferon</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/bookcover.png" alt="" /><figcaption>bookcover</figcaption></figure><p>许多人认为，这是Strong圣经之后可获得的最好的初学者线性代数资源。（在SAGE中进行编程练习，基本上是Python）也很实用，但是对于初学者来说，比从业者更多。</p><h3 id="线性代数作为抽象数学的入门-linear-algebra-as-an-introduction-to-abstract-mathematics">25. <a href="https://www.math.ucdavis.edu/~anne/linear_algebra/" target="_blank" rel="noopener">线性代数–作为抽象数学的入门 / Linear Algebra – As An Introduction to Abstract Mathematics</a></h3><p>这本书感觉就像我的大学线性代数书（许多与我一起学习工程学的学生都喜欢它）。当数学过多而应用程序略少时，我会迷茫，但很多人会喜欢这类书的优雅。</p><h3 id="线性代数的基础和最优化-fundamentals-of-linear-algebra-and-optimizations">26. <a href="https://www.seas.upenn.edu/~cis515/linalg.pdf" target="_blank" rel="noopener">线性代数的基础和最优化 / Fundamentals of Linear Algebra and Optimizations</a></h3><p>本书将线性代数与优化算法结合在一起。同样，面向喜欢该样式的人的更多面向数学的书籍。</p><h3 id="线性代数讲义-lerner-linear-algebra-lecture-notes-lerner">27. <a href="https://www-labs.iro.umontreal.ca/~grabus/courses/ift6760_files/LANotes.lerner.pdf" target="_blank" rel="noopener">线性代数讲义– Lerner / Linear Algebra Lecture Notes – Lerner</a></h3><p>我发现它真的很好，就像向您展示了多个已解决的问题以使您学习一样。不像以前的书那样严格，而是通过表演来学习。对于长时间不接触线性代数的人来说，是不错的复习。</p><h3 id="随机线性代数的讲义-lecture-notes-on-randomized-linear-algebra">28. <a href="https://arxiv.org/abs/1608.04481" target="_blank" rel="noopener">随机线性代数的讲义 / Lecture Notes on Randomized Linear Algebra</a></h3><p>并非所有人都需要阅读本书，因为本书涉及解决线性代数问题的概率算法。如果您要处理大型矩阵和向量，而简单的算法将无法使用，则很有用。</p><h3 id="通过外部产品的线性代数-linear-algebra-via-exterior-products">29. <a href="https://www.academia.edu/32968283/Linear_Algebra_via_Exterior_Products" target="_blank" rel="noopener">通过外部产品的线性代数 / Linear Algebra via Exterior Products</a></h3><p>线性代数的另一种截然不同的方式。如果您发现线性代数很棒，则应尝试以这种新方式可视化问题。</p><h3 id="线性代数-linear-algebra-cherney-et-al">30. <a href="https://www.math.ucdavis.edu/~linear/" target="_blank" rel="noopener">线性代数 / Linear Algebra – Cherney et al</a></h3><p>另一本针对大学级线性代数的免费书籍。适合初学者。如果您想练习，它也会带来作业问题。</p><h3 id="深度学习所需的矩阵微积分-matrix-calculus-you-need-for-deep-learning">31. <a href="https://arxiv.org/abs/1802.01528" target="_blank" rel="noopener">深度学习所需的矩阵微积分 / Matrix Calculus you need for Deep Learning</a></h3><p>顾名思义，本教程可帮助您了解深度学习所需的矩阵演算。</p><h3 id="优化简介-optimization-an-introduction">32. <a href="http://www3.imperial.ac.uk/pls/portallive/docs/1/7288263.PDF" target="_blank" rel="noopener">优化：简介 / Optimization: An Introduction</a></h3><p>在整个工程领域的问题中都需要优化参数。虽然在许多深度学习算法中使用了凸优化，但了解线性算法等其他算法后，单纯形却开阔了眼界。</p><h3 id="scipy讲义scipy-lecture-notes">33. Scipy讲义/Scipy Lecture Notes</h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/Scipy.png" alt="" /><figcaption>Scipy</figcaption></figure><p>如果您打算从事数据科学工作，则需要学习科学的Python堆栈。可能是学习Numpy，Scipy，Scikit-Learn，Scikit-Image以及所需的所有库的最佳通用教程。</p><h3 id="熊猫超级教程-pandas-mega-tutorial">34. <a href="https://pandas.pydata.org/docs/pandas.pdf" target="_blank" rel="noopener">熊猫超级教程 / Pandas Mega Tutorial</a></h3><p>这个庞大的教程是由Pandas开发团队学习和理解该库的。如果您从事数据科学工作，Pandas是一个必须学习的图书馆。跑不了的。</p><h3 id="python中的卡尔曼和贝叶斯过滤器-kalman-and-bayesian-filters-in-python">35. <a href="https://drive.google.com/file/d/0By_SW19c1BfhSVFzNHc0SjduNzg/view" target="_blank" rel="noopener">Python中的卡尔曼和贝叶斯过滤器 / Kalman and Bayesian Filters in Python</a></h3><p>当处理随时间而来的噪声数据时，卡尔曼滤波器和其他贝叶斯滤波器非常有用，这些噪声数据可以拟合到具有推论参数的特定模型。这些模型的双重作用是推导参数以及对噪声进行建模。尽管最常用的示例是位置数据，但是类似的过滤器也可以很好地进行预测。（也可以在  <a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python" target="_blank" rel="noopener">Github上获得</a>）</p><h3 id="数据科学的统计推断-statistical-inference-for-data-science">36. <a href="https://leanpub.com/LittleInferenceBook" target="_blank" rel="noopener">数据科学的统计推断 / Statistical Inference for Data Science</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/hero.png" alt="" /><figcaption>hero</figcaption></figure><p>在此之前，我们已经看过多本《统计推断》书籍，但是写这本书的时候尤其要牢记数据科学家的思想。如果您是一名数据科学家，并且试图快速掌握统计推断，那么这就是您的书。</p><h3 id="机器学习数学-mathematics-for-machine-learning">37. <a href="https://mml-book.github.io/" target="_blank" rel="noopener">机器学习数学 / Mathematics for Machine Learning</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/mml-book-cover.jpg" alt="" /><figcaption>mml-book-cover</figcaption></figure><p>一本详细的教您数学的书需要理解其中的大多数机器学习算法。初学者的友好。</p><h3 id="看见统计---基础概率论-seeing-theory">38. <a href="https://seeing-theory.brown.edu/#firstPage" target="_blank" rel="noopener">看见统计 - 基础概率论 / Seeing Theory</a></h3><p>通过使用交互式可视化使学习概率容易的书。</p><h3 id="统计基础-basics-of-statistics">39. <a href="https://www.semanticscholar.org/paper/Basics-of-Statistics-Isotalo/c3cc90f6e11e9554f3de2c0da26e44ac22f8a1ff" target="_blank" rel="noopener">统计基础 / Basics of Statistics</a></h3><p>一本书向您介绍统计研究。从未学习过统计学的初学者应该从这里开始。</p><h3 id="公开统计-open-statistics">40. <a href="http://14.139.232.166/opstat/" target="_blank" rel="noopener">公开统计 / Open Statistics</a></h3><p>书和视频讲座相结合，向读者介绍统计学。</p><h3 id="从基本角度看高级数据分析-advanced-data-analysis-from-an-elementary-point-of-view">41. <a href="https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/" target="_blank" rel="noopener">从基本角度看高级数据分析 / Advanced Data analysis From an Elementary Point of View</a></h3><p>数据科学的不同概念的一般介绍。这包括因果模型，回归模型，因子模型等。示例程序在R中。</p><h3 id="快速智能大规模-fast-data-smart-and-at-scale">42. <a href="https://www.voltdb.com/wp-content/uploads/2017/03/hv-ebook-fast-data-smart-and-at-scale.pdf" target="_blank" rel="noopener">快速，智能，大规模 / Fast Data, Smart and at Scale</a></h3><p>本书介绍了如何优化数据库以实现快速查询。它讲述了现实世界中的各种可能模型。</p><h3 id="多武装匪徒简介-introduction-to-multi-armed-bandits">43. <a href="https://arxiv.org/abs/1904.07272" target="_blank" rel="noopener">多武装匪徒简介 / Introduction to Multi-Armed Bandits</a></h3><p>多臂强盗是在不确定性下会随着时间做出决策的算法。这本书是多臂匪的入门论文。</p><h3 id="量化经济学讲座-quant-economics-lectures">44. <a href="https://quantecon.org/lectures/" target="_blank" rel="noopener">量化经济学讲座 / Quant Economics Lectures</a></h3><p>使用您喜欢的编程语言进行的定量经济学和代码讲座：Python或Julia。</p><h3 id="julia-统计-statistics-with-julia">45. <a href="https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf" target="_blank" rel="noopener">Julia 统计 / Statistics With Julia</a></h3><p>统计学家学习Julia还是（不太可能）Julia程序员学习统计学？试试这本书。</p><h3 id="信息论推理与学习算法-information-theory-inference-and-learning-algorithms">46. <a href="https://www.inference.org.uk/itprnn/book.pdf" target="_blank" rel="noopener">信息论，推理与学习算法 / Information Theory, Inference and Learning Algorithms</a></h3><p>信息理论和推理通常以不同的方式处理，但已故的MacKay教授的书试图解决这两个问题。</p><h3 id="科学改进决策和风险管理-scientific-improvement-of-decision-making-and-risk-management">47. <a href="https://yngve.hoiseth.net/Empiricast_White_Paper.pdf" target="_blank" rel="noopener">科学改进决策和风险管理 / Scientific Improvement of Decision Making and Risk Management</a></h3><p>关于概率决策的技术性不太强的教程。</p><h3 id="三十三个缩图线性代数的数学和算法应用-thirty-three-miniatures-mathematical-and-algorithmic-applications-of-linear-algebra">48. <a href="https://kam.mff.cuni.cz/~matousek/stml-53-matousek-1.pdf" target="_blank" rel="noopener">三十三个缩图：线性代数的数学和算法应用 / Thirty-three Miniatures: Mathematical and Algorithmic Applications of Linear Algebra</a></h3><p>这实际上不是一本关于线性代数的书，而是一本汇编成书的线性书的一些很酷的应用程序。</p><h3 id="遗传算法教程-a-genetic-algorithm-tutorial">49. <a href="https://www.cs.colostate.edu/~genitor/MiscPubs/tutorial.pdf" target="_blank" rel="noopener">遗传算法教程 / A Genetic Algorithm Tutorial</a></h3><p>遗传算法是所有数据科学家一生中都需要使用的工具。本教程可帮助初学者了解遗传算法的工作原理。</p><h3 id="使用-julia-在运营研究中计算-computing-in-operations-research-using-julia">50. <a href="https://arxiv.org/abs/1312.1431" target="_blank" rel="noopener">使用 Julia 在运营研究中计算 / Computing in Operations Research using Julia</a></h3><p>如果您正在处理排队或其他运营研究问题，Julia可能是您很喜欢的一种编程语言。这些程序像Python一样易于读取，运行速度非常快。</p><p>原文：<a href="https://www.kdnuggets.com/2020/03/50-must-read-free-books-every-data-scientist-2020.html" target="_blank" rel="noopener">50 Must-Read Free Books For Every Data Scientist in 2020</a></p><p><strong>By <a href="https://blog.paralleldots.com/author/reashikaa%20/" target="_blank" rel="noopener">Reashikaa Verma</a>, <a href="https://www.paralleldots.com/" target="_blank" rel="noopener">ParellelDots</a></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据科学是一个跨学科领域，包含来自统计，机器学习，贝叶斯等领域的方法和技术。它们都旨在从数据中产生特定的见解。在本文中，我们列出了一些出色的数据科学书籍，其中涵盖了数据科学下的各种主题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="DS" scheme="https://2020.iosdevlog.com/tags/DS/"/>
    
      <category term="book" scheme="https://2020.iosdevlog.com/tags/book/"/>
    
  </entry>
  
  <entry>
    <title>节选：常见机器学习算法列表（Python 调用）</title>
    <link href="https://2020.iosdevlog.com/2020/03/08/algorithms/"/>
    <id>https://2020.iosdevlog.com/2020/03/08/algorithms/</id>
    <published>2020-03-08T14:49:41.000Z</published>
    <updated>2020-03-08T15:48:54.648Z</updated>
    
    <content type="html"><![CDATA[<ol type="1"><li>线性回归</li><li>逻辑回归</li><li>决策树</li><li>支持向量机</li><li>朴素贝叶斯</li><li>神经网络</li><li>K均值</li><li>随机森林</li><li>降维算法</li><li>梯度提升算法<ol type="1"><li>GBM</li><li>XGBoost</li><li>LightGBM</li><li>Catboost</li></ol></li></ol><a id="more"></a><p>大致而言，共有 3 种类型的机器学习算法</p><ul><li>监督学习</li></ul><p>工作原理：此算法由目标/结果变量（或因变量）组成，该目标/结果变量将从给定的一组预测变量（因变量）中进行预测。使用这些变量集，我们生成了一个将输入映射到所需输出的函数。训练过程将继续进行，直到模型在训练数据上达到所需的准确性水平为止。</p><p>监督学习的例子：线性回归，决策树，随机森林，KNN，逻辑回归等。</p><ul><li>无监督学习</li></ul><p>工作原理： 在此算法中，我们没有任何目标或结果变量可以预测/估算。它用于对不同组中的人群进行聚类，广泛用于对不同组中的客户进行细分以进行特定干预。</p><p>无监督学习的示例：Apriori算法，K均值。</p><ul><li>强化学习：</li></ul><p>工作原理：使用此算法，机器经过训练后可以做出特定决策。它是这样工作的：机器处于反复试验不断训练自身的环境中。该机器将从过去的经验中学习，并尝试捕获最佳的知识以做出准确的业务决策。</p><p>强化学习的例子：马尔可夫决策过程</p><h2 id="常见机器学习算法列表">常见机器学习算法列表</h2><h3 id="线性回归">1.线性回归</h3><p>它用于根据连续变量估算实际价值（房屋成本，通话次数，总销售额等）。在这里，我们通过拟合一条最佳线来建立自变量和因变量之间的关系。该最佳拟合线称为回归线，并由线性方程 <span class="math inline">\(Y = a * X + b\)</span> 表示。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Linear_Regression.webp" alt="" /><figcaption>Linear_Regression</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for the Linear Regression</span></span><br><span class="line"><span class="string">Created by- ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line"></span><br><span class="line">print(train_data.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'\nShape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'\nShape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Item_Outlet_Sales</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Item_Outlet_Sales'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Item_Outlet_Sales'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Item_Outlet_Sales'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Item_Outlet_Sales'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the Linear Regression model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : fit_intercept and normalize</span></span><br><span class="line"><span class="string">Documentation of sklearn LinearRegression: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># coefficeints of the trained model</span></span><br><span class="line">print(<span class="string">'\nCoefficient of model :'</span>, model.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># intercept of the model</span></span><br><span class="line">print(<span class="string">'\nIntercept of model'</span>,model.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'\nItem_Outlet_Sales on training data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Root Mean Squared Error on training dataset</span></span><br><span class="line">rmse_train = mean_squared_error(train_y,predict_train)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on train dataset : '</span>, rmse_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the testing dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'\nItem_Outlet_Sales on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Root Mean Squared Error on testing dataset</span></span><br><span class="line">rmse_test = mean_squared_error(test_y,predict_test)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on test dataset : '</span>, rmse_test)</span><br></pre></td></tr></table></figure><h3 id="logistic回归">2. Logistic回归</h3><p>不要被它的名字弄糊涂了！它是一种分类，而不是回归算法。它用于基于给定的一组独立变量来估计离散值（二进制值，如 <code>0/1，yes/no，true/false</code>）。简而言之，它通过将数据拟合到logit函数来预测事件发生的可能性。因此，这也称为 <strong>对数回归</strong>。由于可以预测概率，因此其输出值介于0和1之间（如预期）。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Logistic_Regression.webp" alt="" /><figcaption>Logistic_Regression</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Logistic Regression</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(train_data.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the Logistic Regression model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : fit_intercept and penalty</span></span><br><span class="line"><span class="string">Documentation of sklearn LogisticRegression: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># coefficeints of the trained model</span></span><br><span class="line">print(<span class="string">'Coefficient of model :'</span>, model.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># intercept of the model</span></span><br><span class="line">print(<span class="string">'Intercept of model'</span>,model.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'Target on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'accuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'Target on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'accuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="决策树">3. 决策树</h3><p>这是我最喜欢的算法之一，我经常使用它。它是一种监督学习算法，主要用于分类问题。令人惊讶的是，它适用于分类因变量和连续因变量。在此算法中，我们将总体分为两个或多个同构集合。这是基于最重要的属性/自变量来完成的，以尽可能地形成不同的组。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Decision_Tree.webp" alt="" /><figcaption>Decision Tree</figcaption></figure><p>理解决策树如何工作的最好方法是玩 Jezzball，这是 Microsoft 的经典游戏（下图）。本质上，您有一间活动墙的房间，您需要创建墙以使最大的区域在没有球的情况下被清除。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Jezzball.jpg" alt="" /><figcaption>Jezzball</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Decision Tree</span></span><br><span class="line"><span class="string">Created by - Analytics Vidhya</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the Decision Tree model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : max_depth and max_features</span></span><br><span class="line"><span class="string">Documentation of sklearn DecisionTreeClassifier: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = DecisionTreeClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># depth of the decision tree</span></span><br><span class="line">print(<span class="string">'Depth of the Decision Tree :'</span>, model.get_depth())</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'Target on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'accuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'Target on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'accuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="svm支持向量机">4. SVM（支持向量机）</h3><p>这是一种分类方法。在此算法中，我们将每个数据项绘制为n维空间（其中n是您拥有的特征数）中的一个点，其中每个特征的值就是特定坐标的值。</p><p>例如，如果我们只有两个特征，例如一个人的身高和头发长度，我们首先将这两个变量绘制在二维空间中，其中每个点都有两个坐标（这些坐标称为支持向量）</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/SVM1.webp" alt="" /><figcaption>SVM1</figcaption></figure><p>现在，我们将找到一行将数据划分为两个不同分类的数据组。这条线将使距两组中最近点的距离最远。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/SVM2.webp" alt="" /><figcaption>SVM2</figcaption></figure><p>在上面显示的示例中，将数据分为两个不同类别的组的线是黑线，因为两个最接近的点距离该线最远。这行是我们的分类器。然后，根据测试数据在行两边的位置，可以将新数据归类为该类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Support Vector Machines</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the Support Vector Classifier model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : kernal and degree</span></span><br><span class="line"><span class="string">Documentation of sklearn Support Vector Classifier: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = SVC()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'Target on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'accuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'Target on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'accuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="朴素贝叶斯">5. 朴素贝叶斯</h3><p>这是一种基于贝叶斯定理的分类技术，假设预测变量之间具有独立性。简单来说，朴素贝叶斯分类器假定类中某个特定功能的存在与任何其他功能的存在无关。例如，如果水果是红色，圆形且直径约3英寸，则可以将其视为苹果。即使这些特征相互依赖或依赖于其他特征的存在，朴素的贝叶斯分类器也会考虑所有这些特征，以独立地有助于该果实是苹果的可能性。</p><p>朴素贝叶斯模型易于构建，对于非常大的数据集特别有用。除简单之外，朴素的贝叶斯（Naive Bayes）甚至胜过非常复杂的分类方法。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Bayes_rule.webp" alt="" /><figcaption>Bayes_rule</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Naive Bayes</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the Naive Bayes model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : var_smoothing</span></span><br><span class="line"><span class="string">Documentation of sklearn GaussianNB: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = GaussianNB()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'Target on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'accuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'Target on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'accuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="knnk-最近邻">6. kNN（k-最近邻）</h3><p>它可以用于分类和回归问题。但是，它更广泛地用于行业中的分类问题。K个最近邻居是一种简单的算法，可以存储所有可用案例，并通过其k个邻居的多数票对新案例进行分类。在通过距离函数测得的K个最近邻居中，分配给该类别的案例最为常见。</p><p>这些距离函数可以是欧几里得距离，曼哈顿距离，明可夫斯基距离和汉明距离。前三个函数用于连续函数，第四个函数（汉明）用于分类变量。如果K = 1，则将案例简单分配给其最近邻居的类别。有时，执行kNN建模时选择K确实是一个挑战。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/KNN.webp" alt="" /><figcaption>KNN</figcaption></figure><p>KNN可以轻松地映射到我们的现实生活。如果您想了解一个没有信息的人，则可能想了解他的密友和他所进入的圈子并获得他/她的信息！</p><p>选择kNN之前要考虑的事项：</p><ol type="1"><li>KNN在计算上很昂贵</li><li>变量应归一化，否则范围较大的变量会对其产生偏差</li><li>在进行kNN处理之前（如离群值，噪声消除）在预处理阶段进行更多工作</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for the K-Nearest Neighbors</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the K-Nearest Neighbor model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : n_neighbors, leaf_size</span></span><br><span class="line"><span class="string">Documentation of sklearn K-Neighbors Classifier: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = KNeighborsClassifier()  </span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of Neighbors used to predict the target</span></span><br><span class="line">print(<span class="string">'\nThe number of neighbors used to predict the target : '</span>,model.n_neighbors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'\nTarget on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'accuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'Target on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'accuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="k-均值聚类">7. K-均值聚类</h3><p>这是一种无监督算法，可以解决聚类问题。它的过程遵循一种简单的方法，可以通过一定数量的聚类（假设k个聚类）对给定的数据集进行分类。集群中的数据点对同级组是同质的，并且是异构的。</p><p>还记得从墨水印迹中找出形状吗？k表示此活动有点类似。您查看形状并展开以解释存在多少个不同的群集/种群！</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Ink.jpg" alt="" /><figcaption>Ink</figcaption></figure><p>K-均值如何形成聚类：</p><ol type="1"><li>K均值为每个聚类选择k个点，称为质心。</li><li>每个数据点形成一个具有最接近质心的聚类，即k个聚类。</li><li>根据现有集群成员查找每个集群的质心。在这里，我们有了新的质心。</li><li>当我们有了新的质心时，请重复步骤2和3。找到每个数据点与新质心的最近距离，并与新的k簇相关联。重复此过程，直到会聚即质心不变为止。</li></ol><p>如何确定K的值：</p><p>在K均值中，我们有簇，每个簇都有自己的质心。质心和群集中数据点之间的差平方和构成该群集的平方值之和。同样，当所有聚类的平方和相加时，它成为聚类解的平方和之和。</p><p>我们知道，随着簇数的增加，该值会不断减少，但是如果绘制结果，您可能会看到平方距离的总和急剧减小，直至达到某个k值，然后逐渐减小。在这里，我们可以找到最佳的群集数量。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Kmenas.webp" alt="" /><figcaption>Kmenas</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for the K-Means</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to divide the training data into differernt clusters</span></span><br><span class="line"><span class="comment"># and predict in which cluster a particular data point belongs.  </span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the K-Means model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : n_clusters and max_iter</span></span><br><span class="line"><span class="string">Documentation of sklearn KMeans: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html</span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line"></span><br><span class="line">model = KMeans()  </span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of Clusters</span></span><br><span class="line">print(<span class="string">'\nDefault number of Clusters : '</span>,model.n_clusters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the clusters on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_data)</span><br><span class="line">print(<span class="string">'\nCLusters on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_data)</span><br><span class="line">print(<span class="string">'Clusters on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we will train a model with n_cluster = 3</span></span><br><span class="line">model_n3 = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model_n3.fit(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of Clusters</span></span><br><span class="line">print(<span class="string">'\nNumber of Clusters : '</span>,model_n3.n_clusters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the clusters on the train dataset</span></span><br><span class="line">predict_train_3 = model_n3.predict(train_data)</span><br><span class="line">print(<span class="string">'\nCLusters on train data'</span>,predict_train_3) </span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test_3 = model_n3.predict(test_data)</span><br><span class="line">print(<span class="string">'Clusters on test data'</span>,predict_test_3)</span><br></pre></td></tr></table></figure><h3 id="随机森林">8. 随机森林</h3><p>随机森林是决策树集合的商标术语。在随机森林中，我们收集了决策树（也称为“森林”）。为了基于属性对新对象进行分类，每棵树都进行了分类，我们称该树为该类“投票”。森林选择投票最多的类别（在森林中的所有树木上）。</p><p>每棵树的种植和生长如下：</p><ol type="1"><li>如果训练集中的病例数为N，则随机抽取N个病例作为样本，并进行替换。该样本将成为树木生长的训练集。</li><li>如果有M个输入变量，则指定数字m &lt;&lt; M，以便在每个节点上从M个中随机选择m个变量，并使用对这m个变量的最佳分割来分割节点。在森林生长过程中，m的值保持恒定。</li><li>每棵树都尽可能地生长。没有修剪。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for the Random Forest</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># view the top 3 rows of the dataset</span></span><br><span class="line">print(train_data.head(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'\nShape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'\nShape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Create the object of the Random Forest model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : n_estimators and max_depth</span></span><br><span class="line"><span class="string">Documentation of sklearn RandomForestClassifier: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">model = RandomForestClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># number of trees used</span></span><br><span class="line">print(<span class="string">'Number of Trees used : '</span>, model.n_estimators)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'\nTarget on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'\naccuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'\nTarget on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'\naccuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="降维算法">9.降维算法</h3><p>在过去的4-5年中，每个可能阶段的数据捕获都呈指数级增长。公司/政府机构/研究组织不仅提供了新的来源，而且还非常详细地捕获数据。</p><p>例如：电子商务公司正在捕获有关客户的更多详细信息，例如他们的人口统计信息，网络爬网历史记录，他们喜欢或不喜欢的东西，购买历史记录，反馈以及许多其他信息，这些东西比最近的杂货店店主更能给予他们个性化的关注。</p><p>作为数据科学家，我们提供的数据还包含许多功能，这对于构建良好的鲁棒模型听起来不错，但仍存在挑战。您如何识别1000或2000中的高有效变量？在这种情况下，降维算法可与其他各种算法（例如决策树，随机森林，PCA，因子分析，基于相关矩阵识别，缺失值比率等）一起帮助我们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Principal Component Analysis (PCA)</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error  </span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># view the top 3 rows of the dataset</span></span><br><span class="line">print(train_data.head(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'\nShape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'\nShape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line"><span class="comment"># target variable - Item_Outlet_Sales</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Item_Outlet_Sales'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Item_Outlet_Sales'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Item_Outlet_Sales'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Item_Outlet_Sales'</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\nTraining model with &#123;&#125; dimensions.'</span>.format(train_x.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># create object of model</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">rmse_train = mean_squared_error(train_y,predict_train)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on train dataset : '</span>, rmse_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">rmse_test = mean_squared_error(test_y,predict_test)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on test dataset : '</span>, rmse_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create the object of the PCA (Principal Component Analysis) model</span></span><br><span class="line"><span class="comment"># reduce the dimensions of the data to 12</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : svd_solver, iterated_power</span></span><br><span class="line"><span class="string">Documentation of sklearn PCA:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">model_pca = PCA(n_components=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">new_train = model_pca.fit_transform(train_x)</span><br><span class="line">new_test  = model_pca.fit_transform(test_x)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\nTraining model with &#123;&#125; dimensions.'</span>.format(new_train.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># create object of model</span></span><br><span class="line">model_new = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model_new.fit(new_train,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the new train dataset</span></span><br><span class="line">predict_train_pca = model_new.predict(new_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">rmse_train_pca = mean_squared_error(train_y,predict_train_pca)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on new train dataset : '</span>, rmse_train_pca)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the new test dataset</span></span><br><span class="line">predict_test_pca = model_new.predict(new_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">rmse_test_pca = mean_squared_error(test_y,predict_test_pca)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on new test dataset : '</span>, rmse_test_pca)</span><br></pre></td></tr></table></figure><h3 id="梯度提升算法">10. 梯度提升算法</h3><h4 id="gbm">10.1 GBM</h4><p>当我们处理大量数据以进行具有高预测能力的预测时，GBM是一种增强算法。Boosting实际上是一种学习算法的集合，该算法结合了多个基本估计量的预测，以提高单个估计量的鲁棒性。它将多个弱或平均预测变量组合为一个构建强的预测变量。这些增强算法在Kaggle，AV Hackathon，CrowdAnalytix等数据科学竞赛中始终能很好地发挥作用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Gradient Boosting</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the GradientBoosting Classifier model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : learning_rate, n_estimators</span></span><br><span class="line"><span class="string">Documentation of sklearn GradientBoosting Classifier: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">model = GradientBoostingClassifier(n_estimators=<span class="number">100</span>,max_depth=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'\nTarget on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'\naccuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'\nTarget on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'\naccuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h4 id="xgboost">10.2 XGBoost</h4><p>在某些Kaggle比赛中，另一种经典的梯度提升算法是决定胜负的决定性选择。</p><p>XGBoost具有极高的预测能力，这使其成为事件准确性的最佳选择，因为它同时具有线性模型和树学习算法，这使得该算法比现有的梯度增强技术快了近10倍。</p><p>支持包括各种目标功能，包括回归，分类和排名。</p><p>关于XGBoost的最有趣的事情之一是，它也被称为正则化增强技术。这有助于减少过拟合模型，并且对Scala，Java，R，Python，Julia和C ++等多种语言提供了广泛的支持。</p><p>支持在包含GCE，AWS，Azure和Yarn群集的许多计算机上进行分布式和广泛的培训。XGBoost还可以与Spark，Flink和其他云数据流系统集成，在提升过程的每次迭代中都具有内置的交叉验证。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for XGBoost</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the XGBoost model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : max_depth and n_estimators</span></span><br><span class="line"><span class="string">Documentation of xgboost:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://xgboost.readthedocs.io/en/latest/</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">model = XGBClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'\nTarget on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'\naccuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'\nTarget on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'\naccuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h4 id="lightgbm">10.3 LightGBM</h4><p>LightGBM 是使用基于树的学习算法的梯度增强框架。它被设计为分布式且高效的，具有以下优点：</p><ul><li>更快的训练速度和更高的效率</li><li>降低内存使用量</li><li>精度更高</li><li>支持并行和GPU学习</li><li>能够处理大规模数据</li></ul><p>该框架是基于决策树算法的一种快速，高性能的梯度提升算法，用于排名，分类和许多其他机器学习任务。它是在Microsoft的分布式机器学习工具包项目下开发的。</p><p>由于LightGBM基于决策树算法，因此它以最佳拟合的方式对树进行拆分，而其他增强算法则对树的深度或层次进行拆分，而不是对叶进行拆分。因此，当在Light GBM中的同一叶上生长时，与逐级算法相比，逐叶算法可以减少更多的损失，因此可以得到更好的精度，而现有的任何增强算法都很少达到这种精度。</p><p>而且，它出奇地快，因此是“ Light”一词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">data = np.random.rand(<span class="number">500</span>, <span class="number">10</span>) <span class="comment"># 500 entities, each contains 10 features</span></span><br><span class="line">label = np.random.randint(<span class="number">2</span>, size=<span class="number">500</span>) <span class="comment"># binary target</span></span><br><span class="line"></span><br><span class="line">train_data = lgb.Dataset(data, label=label)</span><br><span class="line">test_data = train_data.create_valid(<span class="string">'test.svm'</span>)</span><br><span class="line"></span><br><span class="line">param = &#123;<span class="string">'num_leaves'</span>:<span class="number">31</span>, <span class="string">'num_trees'</span>:<span class="number">100</span>, <span class="string">'objective'</span>:<span class="string">'binary'</span>&#125;</span><br><span class="line">param[<span class="string">'metric'</span>] = <span class="string">'auc'</span></span><br><span class="line"></span><br><span class="line">num_round = <span class="number">10</span></span><br><span class="line">bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])</span><br><span class="line"></span><br><span class="line">bst.save_model(<span class="string">'model.txt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7 entities, each contains 10 features</span></span><br><span class="line">data = np.random.rand(<span class="number">7</span>, <span class="number">10</span>)</span><br><span class="line">ypred = bst.predict(data)</span><br></pre></td></tr></table></figure><h4 id="catboost">10.4 Catboost</h4><p>CatBoost是Yandex最近提供的开源机器学习算法。它可以轻松地与深度学习框架（如Google的TensorFlow和Apple的Core ML）集成。</p><p>关于CatBoost的最好之处在于，它不需要像其他ML模型一样进行大量的数据培训，并且可以处理多种数据格式。不会破坏它的坚固性。</p><p>在继续实施之前，请确保处理好丢失的数据。</p><p>Catboost可以自动处理分类变量，而不会显示类型转换错误，这可以帮助您专注于更好地调整模型，而不是解决琐碎的错误。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> catboost <span class="keyword">import</span> CatBoostRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment">#Read training and testing files</span></span><br><span class="line">train = pd.read_csv(<span class="string">"train.csv"</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">"test.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Imputing missing values for both train and test</span></span><br><span class="line">train.fillna(<span class="number">-999</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test.fillna(<span class="number">-999</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Creating a training set for modeling and validation set to check model performance</span></span><br><span class="line">X = train.drop([<span class="string">'Item_Outlet_Sales'</span>], axis=<span class="number">1</span>)</span><br><span class="line">y = train.Item_Outlet_Sales</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=<span class="number">0.7</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">categorical_features_indices = np.where(X.dtypes != np.float)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#importing library and building model</span></span><br><span class="line"><span class="keyword">from</span> catboost <span class="keyword">import</span> CatBoostRegressormodel=CatBoostRegressor(iterations=<span class="number">50</span>, depth=<span class="number">3</span>, learning_rate=<span class="number">0.1</span>, loss_function=<span class="string">'RMSE'</span>)</span><br><span class="line"></span><br><span class="line">model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">submission = pd.DataFrame()</span><br><span class="line"></span><br><span class="line">submission[<span class="string">'Item_Identifier'</span>] = test[<span class="string">'Item_Identifier'</span>]</span><br><span class="line">submission[<span class="string">'Outlet_Identifier'</span>] = test[<span class="string">'Outlet_Identifier'</span>]</span><br><span class="line">submission[<span class="string">'Item_Outlet_Sales'</span>] = model.predict(test)</span><br></pre></td></tr></table></figure><p>节选自：<a href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms" target="_blank" rel="noopener">Commonly used Machine Learning Algorithms (with Python and R Codes)</a><br />作者：<a href="https://www.analyticsvidhya.com/blog/author/sunil-ray/" target="_blank" rel="noopener">SUNIL RAY</a></p>]]></content>
    
    <summary type="html">
    
      &lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;线性回归&lt;/li&gt;
&lt;li&gt;逻辑回归&lt;/li&gt;
&lt;li&gt;决策树&lt;/li&gt;
&lt;li&gt;支持向量机&lt;/li&gt;
&lt;li&gt;朴素贝叶斯&lt;/li&gt;
&lt;li&gt;神经网络&lt;/li&gt;
&lt;li&gt;K均值&lt;/li&gt;
&lt;li&gt;随机森林&lt;/li&gt;
&lt;li&gt;降维算法&lt;/li&gt;
&lt;li&gt;梯度提升算法
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;GBM&lt;/li&gt;
&lt;li&gt;XGBoost&lt;/li&gt;
&lt;li&gt;LightGBM&lt;/li&gt;
&lt;li&gt;Catboost&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="algorithm" scheme="https://2020.iosdevlog.com/categories/algorithm/"/>
    
    
      <category term="ml" scheme="https://2020.iosdevlog.com/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>《神经网络与深度学习》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/03/07/nn/"/>
    <id>https://2020.iosdevlog.com/2020/03/07/nn/</id>
    <published>2020-03-07T11:53:40.000Z</published>
    <updated>2020-03-08T15:02:17.574Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/07/nn/1.png" alt="" /><figcaption>神经网络与深度学习</figcaption></figure><a id="more"></a><h2 id="使用神经网络识别手写数字">使用神经网络识别手写数字</h2><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/001014.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000774.jpg" /></p><h3 id="感知机">感知机</h3><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/001124.jpg" /></p><ul><li>输入</li><li>输出</li><li>权重（weight）</li><li>阈值（threshold value）</li></ul><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000011.jpg" /></p><ul><li><p>偏置（bias）</p></li><li><p>与非门 (NAND gate)</p></li></ul><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000155.jpg" /></p><p>感知机网络计算任何逻辑函数</p><figure><img src="https://2020.iosdevlog.com/2020/03/07/nn/000495.jpg" alt="" /><figcaption>NAND</figcaption></figure><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000706.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000303.jpg" /></p><p>一个没有输入的感知机，那么加权和恒为 0。</p><p>所以，我们最好不要将输入感知机当做感知机，而是理解为一个特殊的单元，它能够输出我们想要的值。</p><figure><img src="https://2020.iosdevlog.com/2020/03/07/nn/000617.jpg" alt="" /><figcaption>输入感知机</figcaption></figure><h3 id="sigmoid神经元">sigmoid神经元</h3><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000110.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000277.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000127.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000625.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000586.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000463.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/001018.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000626.jpg" /></p><h3 id="神经网络的结构">神经网络的结构</h3><h3 id="用简单的网络结构解决手写数字识别">用简单的网络结构解决手写数字识别</h3><h3 id="通过梯度下降法学习参数">通过梯度下降法学习参数</h3><h3 id="实现我们的神经网络来分类数字">实现我们的神经网络来分类数字</h3><h3 id="迈向深度学习">迈向深度学习</h3><h2 id="反向传播算法是如何工作的">反向传播算法是如何工作的</h2><h3 id="热身一个基于矩阵的快速计算神经网络输出的方法">热身：一个基于矩阵的快速计算神经网络输出的方法</h3><h3 id="关于代价函数的两个假设">关于代价函数的两个假设</h3><h3 id="hadamard积st">Hadamard积，s⨀t</h3><h3 id="反向传播背后的四个基本等式">反向传播背后的四个基本等式</h3><h3 id="四个基本方程的证明自选">四个基本方程的证明（自选）</h3><h3 id="反向传播算法">反向传播算法</h3><h3 id="反向传播算法代码">反向传播算法代码</h3><h3 id="为什么说反向传播算法很高效">为什么说反向传播算法很高效</h3><h3 id="反向传播整体描述">反向传播：整体描述</h3><h2 id="改进神经网络的学习方法">改进神经网络的学习方法</h2><h3 id="交叉熵代价函数">交叉熵代价函数</h3><h3 id="用交叉熵解决手写数字识别问题">用交叉熵解决手写数字识别问题</h3><h3 id="交叉熵的意义是什么它又是怎么来的">交叉熵的意义是什么？它又是怎么来的？</h3><h3 id="softmax">Softmax</h3><h3 id="过拟合和正则化">过拟合和正则化</h3><ul><li><p>正则化</p></li><li><p>为什么正则化能够降低过拟合</p></li></ul><h3 id="其它正则化技术">其它正则化技术</h3><h3 id="参数初始化">参数初始化</h3><h3 id="重温手写数字识别代码">重温手写数字识别：代码</h3><h3 id="如何选择神经网络的超参数">如何选择神经网络的超参数</h3><h3 id="其它技术">其它技术</h3><h2 id="神经网络可以计算任何函数的可视化证明">神经网络可以计算任何函数的可视化证明</h2><h3 id="两个预先声明">两个预先声明</h3><h3 id="一个输入和一个输出的普遍性">一个输入和一个输出的普遍性</h3><h3 id="多个输入变量">多个输入变量</h3><h3 id="s型神经元的延伸">S型神经元的延伸</h3><h3 id="修补阶跃函数">修补阶跃函数</h3><h3 id="结论">结论</h3><h2 id="为什么深度神经网络的训练是困难的">为什么深度神经网络的训练是困难的</h2><h3 id="梯度消失问题">梯度消失问题</h3><h3 id="什么导致了消失的梯度问题深度神经网络中的梯度不稳定性">什么导致了消失的梯度问题？深度神经网络中的梯度不稳定性</h3><h3 id="在更加复杂网络中的不稳定梯度">在更加复杂网络中的不稳定梯度</h3><h3 id="其他深度学习的障碍">其他深度学习的障碍</h3><h2 id="深度学习">深度学习</h2><h3 id="介绍卷积网络">介绍卷积网络</h3><h3 id="卷积神经网络在实际中的应用">卷积神经网络在实际中的应用</h3><h3 id="卷积网络的代码">卷积网络的代码</h3><h3 id="图像识别领域中的近期进展">图像识别领域中的近期进展</h3><h3 id="其他的深度学习模型">其他的深度学习模型</h3><h3 id="神经网络的未来">神经网络的未来</h3><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000058.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000073.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000077.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000085.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000217.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000264.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000275.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000280.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000294.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000296.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000476.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000508.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000543.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000546.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000570.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000610.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000657.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000700.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000729.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000742.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000771.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000805.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000817.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000883.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000894.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000906.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000911.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000916.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001008.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001035.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001047.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001059.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001070.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001089.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001110.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001119.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001120.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001145.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/.jpg" /></p><p>源码：<a href="https://github.com/mnielsen/neural-networks-and-deep-learning" target="_blank" rel="noopener" class="uri">https://github.com/mnielsen/neural-networks-and-deep-learning</a></p><p>英文：<a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="noopener" class="uri">http://neuralnetworksanddeeplearning.com/</a></p><p>中文：<a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/" target="_blank" rel="noopener" class="uri">https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/07/nn/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;神经网络与深度学习&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="dl" scheme="https://2020.iosdevlog.com/tags/dl/"/>
    
      <category term="nn" scheme="https://2020.iosdevlog.com/tags/nn/"/>
    
  </entry>
  
  <entry>
    <title>深度学习简介</title>
    <link href="https://2020.iosdevlog.com/2020/03/06/dl/"/>
    <id>https://2020.iosdevlog.com/2020/03/06/dl/</id>
    <published>2020-03-06T14:23:02.000Z</published>
    <updated>2020-03-06T14:27:19.310Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/06/dl/1.png" alt="" /><figcaption>深度学习简介</figcaption></figure><a id="more"></a><h2 id="统计学基础随机性是如何改变数据拟合的本质的">统计学基础：随机性是如何改变数据拟合的本质的？</h2><h3 id="出发点">出发点</h3><ul><li><p>数学定理表明，任何一个函数都可以用多项式无限接近的拟合。</p></li><li><p>为什么我们不用多项式呢？</p></li></ul><h3 id="随机性是如何改变数据拟合本质的">随机性是如何改变数据拟合本质的</h3><ul><li><p>数据的拟合有两种随机性：</p><ul><li><p>噪声-&gt;无法消除</p></li><li><p>函数拟合的随机性-&gt;可以提升</p></li></ul></li><li><p>过拟合和欠拟合</p></li><li><p>引入其他信息的必要性</p></li><li><p>多角度考虑问题</p></li></ul><h3 id="随机性对算法工程师意味着什么">随机性对算法工程师意味着什么</h3><ul><li><p>过拟合和欠拟合是对神经网络设计和训练很重要的一点，但不是全部</p></li><li><p>能否解决问题在很大程度上取决于数据是否有足够信息</p><ul><li><p>引入结构化数据的必要性</p></li><li><p>为什么人解决不了的问题机器也解决不了</p></li></ul></li><li><p>算法除了考虑数学之外，还需要考虑实际数据的情况</p></li><li><p>训练集和测试集不同是机器学习算法最大的坑</p></li></ul><h2 id="神经网络基础神经网络还是复合函数">神经网络基础：神经网络还是复合函数</h2><h3 id="关于神经网络错误的说法">关于神经网络错误的说法</h3><ul><li>神经网络是大家根据神经科学得到的最伟大的发明</li></ul><h3 id="神经网络的数学本质">神经网络的数学本质</h3><ul><li><p>由于神经网络复合函数的本质，使得神经网络可以很方便地组合出很多复杂的函数</p></li><li><p>由于复合函数求导法则，所以大部分神经网络的训练过程可以自动化（反向梯度传递）</p></li></ul><h3 id="一些神经网络术语">一些神经网络术语</h3><ul><li><p>神经网络的训练</p></li><li><p>神经层</p></li><li><p>激活函数</p></li><li><p>隐含层</p></li></ul><h2 id="神经网络基础训练神经网络">神经网络基础：训练神经网络</h2><h3 id="传统优化求解方法的问题">传统优化求解方法的问题</h3><ul><li><p>传统的求解方法：</p><ul><li><p>拟牛顿法等</p></li><li><p>Proximal Methods</p></li></ul></li><li><p>传统求解方法的问题</p><ul><li>传统方法需要根据全部样本计算梯度（导数），这导致对于非常复杂的网络，求解计算上根本不可行</li></ul></li></ul><h3 id="基础的梯度下降算法">基础的梯度下降算法</h3><ul><li><p>经典的优化算法</p><ul><li><p>SGD</p></li><li><p>SGD with Momentum</p></li><li><p>Adagrad</p></li><li><p>Adam</p></li></ul></li></ul><h3 id="梯度消失和梯度爆炸">梯度消失和梯度爆炸</h3><ul><li><p>由于求解过程的复杂性，这使得神经网络的求解并不一定会收敛到最优解</p></li><li><p>对于神经网络训练最大的敌人是梯度消失和梯度爆炸</p></li><li><p>解决梯度消失和梯度爆炸往往是网络设计和优化算法需要考虑的问题</p></li></ul><h2 id="神经网络基础神经网络的基础构成">神经网络基础：神经网络的基础构成</h2><h3 id="全连接层">全连接层</h3><h3 id="激活函数">激活函数</h3><h3 id="dropout">Dropout</h3><h3 id="batch-normalization">Batch Normalization</h3><h2 id="embedding-简介">Embedding 简介</h2><h3 id="什么是-embedding">什么是 Embedding</h3><h3 id="为什么我们需要-embedding">为什么我们需要 Embedding</h3><h3 id="embedding-是怎么训练的">Embedding 是怎么训练的</h3><h2 id="rnn-简介">RNN 简介</h2><h3 id="rnn">RNN</h3><h3 id="lstm">LSTM</h3><h3 id="马尔科夫过程">马尔科夫过程</h3><h3 id="隐马尔科夫过程">隐马尔科夫过程</h3><h2 id="cnn-简介">CNN 简介</h2><h3 id="cnn">CNN</h3><h3 id="cnn-如何应用在文本当中">CNN 如何应用在文本当中</h3><ul><li><p>传统来说，NLP 当中的 CNN 一般较浅，但有证据表明更深的 CNN 更有效。</p></li><li><p>在传统文本分类模型当中，CNN 效果往往要比 LSTM 分类效果好（但不一定）。</p></li></ul><p>参考：<a href="https://github.com/geektime-geekbang/NLP" target="_blank" rel="noopener" class="uri">https://github.com/geektime-geekbang/NLP</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/06/dl/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;深度学习简介&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="dl" scheme="https://2020.iosdevlog.com/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>反向图灵测试 Reverse Turing test</title>
    <link href="https://2020.iosdevlog.com/2020/03/05/Reverse-Turing-test/"/>
    <id>https://2020.iosdevlog.com/2020/03/05/Reverse-Turing-test/</id>
    <published>2020-03-05T13:27:58.000Z</published>
    <updated>2020-03-05T13:54:26.717Z</updated>
    
    <content type="html"><![CDATA[<p>如果漫画家手冢治虫还活着，会在漫画中描绘出什么样的未来？AI 是否能够帮他呈现？</p><figure><img src="https://2020.iosdevlog.com/2020/03/05/Reverse-Turing-test/1.gif" alt="" /><figcaption>AI复活已故漫画家手冢治虫</figcaption></figure><p>通过分析其作品，人工智能产生了角色设计和基本故事情节。据悉，新漫画的主人公是 AI 学习了 6000 张角色图像之后生成的。之后由专业创作者添加诸如服装和对话之类的元素以完善作品。</p><a id="more"></a><h2 id="验证码-captcha">验证码 / CAPTCHA</h2><p>全自动区分计算机和人类的公开图灵测试（英语：Completely Automated Public Turing test to tell Computers and Humans Apart，简称CAPTCHA），俗称验证码，是一种区分用户是计算机或人的公共全自动程序。在 CAPTCHA 测试中，作为服务器的计算机会自动生成一个问题由用户来解答。这个问题可以由计算机生成并评判，但是必须只有人类才能解答。由于计算机无法解答 CAPTCHA 的问题，所以回答出问题的用户就可以被认为是人类。</p><p>一种常用的 CAPTCHA 测试是让用户输入一个扭曲变形的图片上所显示的文字或数字，扭曲变形是为了避免被 <code>光学字符识别</code>（<strong>OCR</strong>, Optical Character Recognition）之类的计算机程序自动识别出图片上的文数字而失去效果。由于这个测试是由计算机来考人类，而不是标准图灵测试中那样由人类来考计算机，人们有时称 CAPTCHA 是一种 <em>反向图灵测试</em>。</p><h2 id="图灵测试">图灵测试</h2><p>图灵测试（英语：Turing test，又译图灵试验）是图灵于1950年提出的一个关于判断机器是否能够思考的著名思想实验，测试某机器是否能表现出与人等价或无法区分的智能。测试的谈话仅限于使用唯一的文本管道，例如计算机键盘和屏幕，这样的结果不依赖于计算机把单词转换为音频的能力。</p><h3 id="测试内容">测试内容</h3><blockquote><p>如果一个人（代号C）使用测试对象皆理解的语言去询问两个他不能看见的对象任意一串问题。对象为：一个是正常思维的人（代号B）、一个是机器（代号A）。如果经过若干询问以后，C不能得出实质的区别来分辨A与B的不同，则此机器A通过图灵测试。</p></blockquote><h3 id="完成图灵测试涉及的技术课题">完成图灵测试涉及的技术课题</h3><p>根据人们的大体判断，达成能够通过图灵测试的技术涉及以下课题</p><ul><li>自然语言处理</li><li>知识表示</li><li>自动推理</li><li>机器学习</li></ul><p>但是为了通过完全图灵测试，还需要另外两项额外技术课题：</p><ul><li>计算机视觉</li><li>机器人学</li></ul><h3 id="反向图灵测试和验证码">反向图灵测试和验证码</h3><p>验证码（CAPTCHA）是一种反向图灵测试。在网站上运行一些动作之前，用户被给予一个扭曲的图形，并要求用户输入图中的字母或数字。这是为了防止网站被自动化系统用来滥用。理由是能够精细地阅读和准确地重现扭曲的形象的系统并不存在（或不提供给普通用户），所以能够做到这一点的任何系统可能是一个人类。</p><p>可以破解验证码的软件正在被积极开发，软件拥有一个有一定的准确性的验证码分析模式生成引擎。而在破解验证码软件被积极开发的同时，另一种通过反向图灵测试的准则也被提出来。其认为即使破解验证码软件被成功研发，也只是具有智能的人类透过编程对验证码所作出的破解手段而已，并非真正通过反向图灵测试或图灵测试。而如果一台机器能够规划出如同验证码一类的防止自动化系统的规避程序，此台机器才算是真正通过了反向图灵测试。</p><h3 id="完全图灵测试">完全图灵测试</h3><p>普通的图灵测试一般避免审问者与被测试计算机发生物理上的互动，因为物理上模拟人（比如像模拟人的外表）并不是人工智能的研究范畴。然而一些人工智能可能涉及一些人机在物理上的交互，所以人们又拓展出了“完全图灵测试”。在完全图灵测试中，可以包含必要的人机在物理层面上的交互。但是为了通过完全图灵测试，还需要在普通图灵测试之外另外两项额外技术课题。询问者还可以测试的受试者感知能力（需要计算机视觉），和受试者操纵物体的能力（需要机器人学）。</p><h2 id="反向图灵测试-reverse-turing-test">反向图灵测试 Reverse Turing test</h2><p>可以说，反向图灵测试的标准形式是受试者试图表现为计算机而非人类的形式。</p><p>在反向图灵测试中表现最佳的人员是最了解计算机的人员，因此知道计算机在对话中可能会犯的错误的类型。</p><p>在计算机编程尤其是调试过程中，反向图灵测试的技巧与思维上模拟程序操作的技巧之间有着很多共识。结果，<code>程序员（尤其是黑客）</code> 有时会沉迷于非正式的反向图灵测试中以进行娱乐。</p><p>参考：<a href="https://www.wikipedia.org" target="_blank" rel="noopener" class="uri">https://www.wikipedia.org</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果漫画家手冢治虫还活着，会在漫画中描绘出什么样的未来？AI 是否能够帮他呈现？&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/05/Reverse-Turing-test/1.gif&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;AI复活已故漫画家手冢治虫&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;通过分析其作品，人工智能产生了角色设计和基本故事情节。据悉，新漫画的主人公是 AI 学习了 6000 张角色图像之后生成的。之后由专业创作者添加诸如服装和对话之类的元素以完善作品。&lt;/p&gt;
    
    </summary>
    
    
      <category term="game" scheme="https://2020.iosdevlog.com/categories/game/"/>
    
    
      <category term="Godot" scheme="https://2020.iosdevlog.com/tags/Godot/"/>
    
  </entry>
  
  <entry>
    <title>线性回归  Manim 演示</title>
    <link href="https://2020.iosdevlog.com/2020/03/04/manim/"/>
    <id>https://2020.iosdevlog.com/2020/03/04/manim/</id>
    <published>2020-03-04T14:00:06.000Z</published>
    <updated>2020-03-04T14:06:20.253Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/03/dl/0.jpg" alt="" /><figcaption>Linear Regression Overview</figcaption></figure><p><a href="https://youtube.com/watch?v=3vl17ymkODg" target="_blank" rel="noopener" class="uri">https://youtube.com/watch?v=3vl17ymkODg</a></p><p>机器学习算法如果能向 3B1B 一样展示，必能加深对算法的理解。</p><p>准备做一些视频展示，可以直接调用 sklearn，当然也可以自己写一套算法，为了了解原理，还是直接调用 sklearn等成熟的算法库好了（自己写有点慢，把代码看懂还是有必要的）。</p><p>Manim 非官方文档：<a href="https://elteoremadebeethoven.github.io/manim_3feb_docs.github.io/html/index.html" target="_blank" rel="noopener" class="uri">https://elteoremadebeethoven.github.io/manim_3feb_docs.github.io/html/index.html</a></p><a id="more"></a><p><img src="https://2020.iosdevlog.com/2020/03/03/dl/1.jpg" alt="1" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/2.jpg" alt="2" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/3.jpg" alt="3" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/4.jpg" alt="4" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/5.jpg" alt="5" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/6.jpg" alt="6" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/7.jpg" alt="7" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/8.jpg" alt="8" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/9.jpg" alt="9" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/10.jpg" alt="10" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/11.jpg" alt="11" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/12.jpg" alt="12" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/13.jpg" alt="13" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/14.jpg" alt="14" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/15.jpg" alt="15" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/16.jpg" alt="16" /><br /><img src="https://2020.iosdevlog.com/2020/03/03/dl/17.jpg" alt="17" /></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/03/dl/0.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;Linear Regression Overview&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;a href=&quot;https://youtube.com/watch?v=3vl17ymkODg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://youtube.com/watch?v=3vl17ymkODg&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;机器学习算法如果能向 3B1B 一样展示，必能加深对算法的理解。&lt;/p&gt;
&lt;p&gt;准备做一些视频展示，可以直接调用 sklearn，当然也可以自己写一套算法，为了了解原理，还是直接调用 sklearn等成熟的算法库好了（自己写有点慢，把代码看懂还是有必要的）。&lt;/p&gt;
&lt;p&gt;Manim 非官方文档：&lt;a href=&quot;https://elteoremadebeethoven.github.io/manim_3feb_docs.github.io/html/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://elteoremadebeethoven.github.io/manim_3feb_docs.github.io/html/index.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="dl" scheme="https://2020.iosdevlog.com/categories/dl/"/>
    
    
      <category term="-lr -manim" scheme="https://2020.iosdevlog.com/tags/lr-manim/"/>
    
  </entry>
  
  <entry>
    <title>我的微信公众号头像侵权</title>
    <link href="https://2020.iosdevlog.com/2020/03/03/copyright/"/>
    <id>https://2020.iosdevlog.com/2020/03/03/copyright/</id>
    <published>2020-03-03T15:23:24.000Z</published>
    <updated>2020-03-03T15:55:29.538Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/03/copyright/1.jpg" alt="" /><figcaption>侵权</figcaption></figure><a id="more"></a><p>今天需要上传 <code>ipa</code> 到 <code>App Store</code>，使用 <code>Apple</code> 最近推出的上传工具 <code>Transporter</code> 上传。</p><p>期间遇到卡住问题，于是更新了一篇 Blog，准备也发到我的公众号：iOSDevLog。</p><p>好长时间没有登录这个帐号群发消息，一进入就发现我公众号的头像侵权了。</p><p>你的帐号经查涉嫌头像侵权，违规内容已清空处理。</p><p>你可重新进行设置但请遵守规范。如果再有类似违规情况，将加重处罚甚至永久性屏蔽所有功能。</p><p>违反规范：《微信公众平台运营规范》4.1.1条规定</p><p>微信公众平台已依法进行侵权投诉处理，法定的平台义务已经履行完毕。若你对于投诉方的意见有异议，建议你另行通过行政投诉、诉讼等方式与投诉方解决。</p><table><colgroup><col style="width: 21%" /><col style="width: 21%" /><col style="width: 15%" /><col style="width: 21%" /><col style="width: 21%" /></colgroup><tbody><tr class="odd"><td style="text-align: left;">权利人</td><td style="text-align: left;">姓名/名称</td><td style="text-align: left;">苹果</td><td style="text-align: left;">有效证件（复印件附后）</td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">通讯地址</td><td style="text-align: left;">中国 - 上海 - 浦东新区 外高桥保税区马吉路88号C区6号楼全幢</td><td style="text-align: left;">邮编</td><td style="text-align: left;">200000</td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;">联系人</td><td style="text-align: left;">O********</td><td style="text-align: left;">电话</td><td style="text-align: left;">182211*****</td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">E-mail</td><td style="text-align: left;">e******<span class="citation" data-cites="apple.com">@apple.com</span></td><td style="text-align: left;"></td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;">投诉帐号</td><td style="text-align: left;">iOS开发日志 （iOSDevLog）</td><td style="text-align: left;"></td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">投诉类型</td><td style="text-align: left;">头像侵权</td><td style="text-align: left;"></td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;">被侵权内容</td><td style="text-align: left;">商标</td><td style="text-align: left;"></td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">投诉描述</td><td style="text-align: left;">以下内容非微信官方提供，由权利人投诉时填写，请谨慎操作。</td><td style="text-align: left;">被投诉公众号未经苹果公司授权注册或运营。其账号、名称、头像及内容等多处未经授权使用苹果公司注册商标，侵犯我公司商标权，严重误导消费者。权利人要求立即停止上述侵权行为。</td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;">证明资料</td><td style="text-align: left;">商标注册书</td><td style="text-align: left;">营业执照</td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">保证声明</td><td style="text-align: left;">权利人及其代理人（统称为：声明人）诚意作如下声明</td><td style="text-align: left;">声明人在通知书中的陈述和提供的相关材料皆是真实、有效、合法的，并保证承担和赔偿腾讯因根据声明人的通知书对相关帐号的处理而给腾讯造成的任何损失，包括但不限于腾讯因向被投诉方或用户赔偿而产生的损失及腾讯名誉、商誉损害等。</td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr></tbody></table><p>对此我的态度就是 <strong>认错</strong>，要改头像。一时间又没有想好想改成什么样，就直接用我 <code>AIDevLog</code> 的二维码好了。</p><h2 id="苹果不让坏人用-iphone-好莱坞导演透露电影业潜规则1">苹果不让坏人用 iPhone !好莱坞导演透露电影业潜规则<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h2><p>瑞安·约翰逊（Rian Johnson），最受欢迎的电影制片人之一（布鲁克，布卢姆兄弟，洛珀（2012），《星球大战：最后的绝地武士》，《刀叉》）进行了有趣的采访 “名利场”。 他特别提到了安置协议 iPhone 在电影院里。 据约翰逊说， η 苹果公司 允许 使用 iPhone，但不允许坏人拥有它们 电影.</p><p>瑞安·约翰逊（Rian Johnson）具体说：</p><blockquote><p>“另外一件有趣的事，我不知道是否要说……不是因为它不好或什么，而是因为它会影响下一个 电影 我正在写一个谜..算了！ 我会说。 非常有趣</p><p>苹果…允许使用iPhone，但是-如果您正在观看一部神秘电影，这非常重要- 电影中的坏人无法拥有iPhone。</p><p>所以...哦，不！ 每一个 创造者 他的电影中有小人，他现在想杀了我！”</p></blockquote><p>众所周知 <strong>苹果公司</strong> 有 <strong>严格的规则</strong> 关于如何使用，显示和拍照iPhone和其他产品 家电 的。 例如，苹果公司报告说 该 制品 它应该只在“尽可能好的光线下”出现， 以便以最佳方式展示 iPhone。</p><p>过去许多人已经注意到，只有“好人”才能在电视节目和电影中使用Apple产品。 播放“ 24”时， 有线 写一个 理论 粉丝曾经说过 好孩子用 苹果电脑 而坏人会用它 个人电脑。 该理论似乎是正确的。</p><p>约翰逊发表声明后，观众肯定会更加专心，并观看 家电 演员用来了解坏人在电影中的适时出现。</p><p>看来 <strong>苹果公司</strong> 是非常注重它的品牌，以后对头像，图片，Code，数据等使用，最好选择无版权的或者是公开的，避免陷入不必要的麻烦（大公司让个人免费用盗版，等公司能交得起时...）。</p><section class="footnotes" role="doc-endnotes"><hr /><ol><li id="fn1" role="doc-endnote"><p>苹果不允许坏人在电影中使用iPhone！：<a href="https://zh-cn.secnews.gr/213337/iphone苹果酱kakoi/" target="_blank" rel="noopener" class="uri">https://zh-cn.secnews.gr/213337/iphone苹果酱kakoi/</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/03/copyright/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;侵权&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="运营" scheme="https://2020.iosdevlog.com/categories/%E8%BF%90%E8%90%A5/"/>
    
    
      <category term="公众号" scheme="https://2020.iosdevlog.com/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"/>
    
      <category term="Apple" scheme="https://2020.iosdevlog.com/tags/Apple/"/>
    
      <category term="版权" scheme="https://2020.iosdevlog.com/tags/%E7%89%88%E6%9D%83/"/>
    
  </entry>
  
  <entry>
    <title>Transporter 上传慢解决方案</title>
    <link href="https://2020.iosdevlog.com/2020/03/03/Transporter/"/>
    <id>https://2020.iosdevlog.com/2020/03/03/Transporter/</id>
    <published>2020-03-03T14:56:11.000Z</published>
    <updated>2020-03-03T15:08:08.776Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/03/Transporter/1.jpg" alt="" /><figcaption>Transporter</figcaption></figure><a id="more"></a><p>解决使用 Transporter 上传 <strong>ipa</strong> 到 App Store 时，有时间会卡住或者非常慢。</p><h2 id="使用">使用</h2><ol type="1"><li>下载 <code>Release</code> 下面的 <code>zip</code> 包，或者 <code>git clone https://github.com/iOSDevLog/com.apple.amp.itmstransporter</code></li><li>替换<code>~/Library/Caches/com.apple.amp.itmstransporter</code></li></ol><h2 id="原因">原因</h2><p>Transporter 安装上第一次打开后，会在硬盘 <code>~/Library/Caches/com.apple.amp.itmstransporter</code> 目录下下载一些缓存文件，这些缓存文件没有下载完，或者下载失败没下载完时，使用Transporter去提交应用这个页面就会卡住或者这个页面很慢。</p><h2 id="解决方案">解决方案</h2><p>终端运行命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/Applications/Transporter.app/Contents/itms/bin/iTMSTransporter</span><br></pre></td></tr></table></figure><p>查看 <code>~/Library/Caches/com.apple.amp.itmstransporter</code> 变化</p><p>如果有报错信息 <code>https://...jar</code>，把 <code>jar</code> 文件下载下来，放入 <code>~/Library/Caches/com.apple.amp.itmstransporter/obr/2.2.0/</code></p><p>再次运行</p><p><code>/Applications/Transporter.app/Contents/itms/bin/iTMSTransporter</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[2020-03-03 12:28:39 CST] &lt;main&gt;  INFO: Configuring logging...</span><br><span class="line">[2020-03-03 12:28:39 CST] &lt;main&gt;  INFO: Logging level <span class="built_in">set</span> to eXtreme</span><br><span class="line">[2020-03-03 12:28:39 CST] &lt;main&gt;  INFO: Transporter is searching <span class="keyword">for</span> new software components.</span><br><span class="line">[2020-03-03 12:28:39 CST] &lt;main&gt;  INFO: INFO: using cached repository.xml file.</span><br><span class="line">[2020-03-03 12:28:40 CST] &lt;main&gt;  INFO: Update check complete.</span><br><span class="line">[2020-03-03 12:28:41 CST] &lt;main&gt; DEBUG: Attempting refresh of configuration data from https://contentdelivery.itunes.apple.com/transporter/Defaults.properties</span><br><span class="line">[2020-03-03 12:28:41 CST] &lt;main&gt; DEBUG: Configuration refresh successful.</span><br><span class="line">[2020-03-03 12:28:41 CST] &lt;main&gt; DEBUG: Saving configuration to <span class="built_in">local</span> path: /Users/iosdevlog/Library/Caches/com.apple.amp.itmstransporter/Defaults.properties</span><br><span class="line">usage: iTMSTransporter [-<span class="built_in">help</span> &lt;arg&gt; | -info | -m &lt;arg&gt; | -version]   [-o &lt;arg&gt;] [-v</span><br><span class="line">       &lt;arg&gt;]  [-WONoPause &lt;arg&gt;] [-Xmx4096m]</span><br><span class="line">iTMSTransporter : iTunes Store Transporter 2.0.0</span><br><span class="line"> -<span class="built_in">help</span> &lt;arg&gt;        Show this <span class="built_in">help</span>.  If a mode value is specified, show <span class="built_in">help</span> specific</span><br><span class="line">                    to that mode.</span><br><span class="line"> -info              The -info option should be used by itself and returns the</span><br><span class="line">                    copyright notice and acknowledgements.</span><br><span class="line"> -m &lt;arg&gt;           The -m option specifies the tool<span class="string">'s mode.  The valid values are:</span></span><br><span class="line"><span class="string">                    verify, upload, provider, diagnostic, lookupMetadata,</span></span><br><span class="line"><span class="string">                    createArtist, lookupArtist, status, statusAll,</span></span><br><span class="line"><span class="string">                    createMetadataTicket, queryTickets, generateSchema, transferTest,</span></span><br><span class="line"><span class="string">                    downloadMetadataGuides, listReports, requestReport</span></span><br><span class="line"><span class="string"> -o &lt;arg&gt;           The -o option specifies the directory and filename you want to use</span></span><br><span class="line"><span class="string">                    to log output information.  By default, Transporter logs output</span></span><br><span class="line"><span class="string">                    information to standard out. If you specify a filename,</span></span><br><span class="line"><span class="string">                    Transporter logs the output to the specified file, as well as to</span></span><br><span class="line"><span class="string">                    standard out.</span></span><br><span class="line"><span class="string"> -v &lt;arg&gt;           The -v option specifies the level of logging.  The five values</span></span><br><span class="line"><span class="string">                    are: off, detailed, informational, critical, eXtreme.</span></span><br><span class="line"><span class="string"> -version           The -version option should be used by itself and returns the</span></span><br><span class="line"><span class="string">                    version of the tool.</span></span><br><span class="line"><span class="string"> -WONoPause &lt;arg&gt;   The -WONoPause option is only valid on Windows and its value can</span></span><br><span class="line"><span class="string">                    be '</span><span class="literal">true</span><span class="string">' or '</span><span class="literal">false</span><span class="string">'.  If an error occurs during script execution,</span></span><br><span class="line"><span class="string">                    the process idles because the message '</span>Press any key...<span class="string">' is</span></span><br><span class="line"><span class="string">                    displayed on the console and the system awaits a keypress. To</span></span><br><span class="line"><span class="string">                    avoid this behavior, set this property to true</span></span><br><span class="line"><span class="string"> -Xmx4096m          Specifies that you want to change the Java Virtual Machine'</span>s (JVM)</span><br><span class="line">                    allocated memory by increasing the JVM heap size.  By default,</span><br><span class="line">                    Transporter uses a 2048MB heap size. You can use the -Xmx4096m</span><br><span class="line">                    option to specify a 4-gigabyte (GB) heap size. Apple recommends,</span><br><span class="line">                    <span class="keyword">if</span> needed, increasing the heap size to 4096MB by specifying the</span><br><span class="line">                    -Xmx4096m (or -Xmx4g) option and adjusting as needed.</span><br><span class="line">[2020-03-03 12:28:41 CST] &lt;main&gt; DBG-X: Returning 0</span><br></pre></td></tr></table></figure><p>出现以上结果说明正常。</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/03/Transporter/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;Transporter&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Tool" scheme="https://2020.iosdevlog.com/categories/Tool/"/>
    
    
      <category term="iOS" scheme="https://2020.iosdevlog.com/tags/iOS/"/>
    
  </entry>
  
  <entry>
    <title>《Google工作法》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/03/02/9787540492908/"/>
    <id>https://2020.iosdevlog.com/2020/03/02/9787540492908/</id>
    <published>2020-03-02T12:01:27.000Z</published>
    <updated>2020-03-02T15:15:07.579Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/1.jpg" alt="" /><figcaption>Google工作法</figcaption></figure><p>书名：Google工作法<br />作者：[波]彼得·费利克斯·格日瓦奇<br />译者：朱悦玮<br />出版社：湖南文艺出版社<br />出版时间：2019-08<br />ISBN：9787540492908</p><blockquote><p>明明很努力地工作，但工作总是做不完。<br />明明很努力地工作，但工作总是不顺利。<br />而且，这样的状态还一直在持续...</p></blockquote><p>——这本书将彻底解决你的烦恼。</p><a id="more"></a><h2 id="简介">简介</h2><p>《Google工作法》是一本介绍Google内部高效工作法的经管图书。 工作高效的人为什么不爱用邮件？为什么明明很努力工作，却怎么也做不完，而且还不顺利？本书传授57个核心技巧，一次性把Google的高效秘密倾囊相授。</p><p>AI时代来临，与其担心工作是否被取代，不如改变低效的工作方式，找到让个人或者企业立足的强有力资本。所谓高效，并不是快速完成某项工作而已，而是把更多时间留给更有价值的工作。把握这个核心，就能很好地理解Google为什么能成为令全世界侧目的高科技企业。</p><p>本书适合企业中各个层次的读者阅读，在快速变化的时代找到自己的核心价值。 Google“每一分钟”的使用方法都不一样！ 不把工作带回去。明白就是明白，不明白就是不明白。不要过分依赖邮件。不做过多分析。需要休息的时候就休息一下。考虑让自己的工作消失。</p><h2 id="为什么日本的企业生产效率低下">为什么日本的企业生产效率低下</h2><ol type="1"><li>过度推迟讨论</li><li>过分讨论</li><li>过度的交流</li></ol><h2 id="改变工作方式方法才是生存之道">改变工作方式方法才是生存之道</h2><p>很多人都害怕自己的工作会被IT（信息技术）和AI（人工智能）所取代。</p><p>我们不应该害怕“自己的工作消失”，而是应该思考“怎样做才能够用IT来代替自己的工作”“怎样做才能够更加自动化、省力化”。</p><p>当时我最深刻的感受就是 <strong>变化突然就到来了</strong>，并且认识到 <strong>自己也必须做出改变才行</strong>。</p><h2 id="让你比世界更快的工作术-不要依赖邮件">让你比世界更快的工作术 不要依赖邮件</h2><p>我认为决定工作效率的关键在于对“现在”的使用方法。</p><p>谷歌为了取得10倍的成果，“现在做”和“现在不做的话那要等到什么时候做”的意识非常强。</p><ul><li>过度“推迟”会浪费许多人的时间<ul><li>明明当场就能够做完的事情，很多人却用“我以后再做”将这件事推迟到后面去。</li></ul></li><li>不要“推迟讨论”<ul><li>“如果现在有必要的话，现在就联系。”</li><li>“如果现在应该决定的话，现在就决定。”</li><li>通过谷歌文档（Google Docs，一款可以在网页上共享文件的文件制作软件）等共享工具，能够实现随时随地访问自己的文件，从而增加你“立刻能做”的事情。</li></ul></li><li>当场做出决定<ul><li>一定要给“做出决定”规定期限</li></ul></li><li>活用“身边的人”<ul><li>要想解决这个问题，需要收集这些必要的资源</li></ul></li><li>即便“不知道应该怎样做才好”仍然能够采取行动的人才会成功<ul><li>将“知道的内容”和“不知道的内容”区分开</li><li>提出问题</li><li>留出时间</li></ul></li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/2.jpg" alt="" /><figcaption>在“不知道应该怎样做才好”的时候仍然能做的事</figcaption></figure><ul><li>增加“现在”的密度<ul><li>一次结束</li><li>当场做完</li></ul></li><li>不用邮件，所有人同时工作的话可以将工作时间缩短到原本的十分之一<ul><li>当场一次完成</li><li>微软的Office 365</li><li>苹果的iCloud</li><li>谷歌文档</li></ul></li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/3.jpg" alt="" /><figcaption>利用谷歌文档共享资料</figcaption></figure><ul><li>绝对不要用邮件来进行日程调整<ul><li>谷歌日历（Google Calendar）</li></ul></li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/4.jpg" alt="" /><figcaption>谷歌日历可以添加附件</figcaption></figure><ul><li>为了提高效率，英语必不可少<ul><li>翻译：时间和成本的二次浪费</li></ul></li><li>直接见面最有效率！</li><li>邮件是“等待文化”，即时通信是“实时文化”<ul><li>当场将问题全部解决。这种速度上的差异会非常明显地表现在工作成果上。</li></ul></li><li>限制访问是阻碍竞争的主要因素</li><li><strong>给“尽快”规定一个期限</strong><ul><li>规定期限，集中精神</li></ul></li><li>由委托人来决定优先顺序<ul><li>明确工作的优先顺序和品质要求是专业人士的基本素养。</li></ul></li><li>创意性工作也需要规定期限<ul><li>如果不规定一个期限，工作就很容易停滞不前。</li></ul></li><li>大胆地将期限提前</li><li>不要把时间浪费在选择衣服上</li><li>对“此时此刻”持有明确的目的</li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/5.jpg" alt="" /><figcaption>会议前的准备列表</figcaption></figure><ul><li>事先预测、控制局面</li><li>将一周每天要做的事情区分开</li><li>选择舒适的工作环境</li><li>如果想提高工作效率，一个舒适的工作环境十分重要。</li></ul><h3 id="总结">总结</h3><ul><li>想办法“一次结束”</li><li>先将能够当场确定的事情确定下来，切实地取得进展</li><li>思考不用邮件而让所有人一次做完的方法</li><li>给所有的工作都规定期限</li><li>将精力集中于“此时此刻”</li><li>选择一个能够让自己集中精神工作的环境</li></ul><h2 id="没时间去进行逻辑分析用集体智慧来进行思考">没时间去进行逻辑分析！用集体智慧来进行思考</h2><ul><li>没有结论的分析毫无意义<ul><li>不知道自己究竟为什么要调查</li><li>也就是没有明确调查的目的</li></ul></li><li>分析的目的是什么<ul><li>创意思维需要的是灵感以及来自丰富经验的直觉</li><li>多准备一些能够激发灵感的材料</li></ul></li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/6.jpg" alt="" /><figcaption>线索卡的示例</figcaption></figure><ul><li>通过制造混乱让大脑活跃起来<ul><li>通过人为地制造混乱，可以使潜意识活性化，从而更容易思考出新的创意</li></ul></li><li>靠集体智慧才能产生创意思维<ul><li>集体智慧（Collective intelligence）是产生创意的秘诀</li></ul></li><li>只对竞争对手进行“分析”不可能开发出新产品<ul><li>只对竞争对手的商品进行分析无法实现差异化</li></ul></li><li>一味地模仿不可能实现差异化</li><li>企划会议不需要总结报告</li><li>总结报告式的会议无法拓展思考</li><li>需要的不是“评价”而是“成果”</li><li>独自思考不如大家一起思考</li><li>积极听取不同类型和立场的意见</li></ul><h2 id="总结-1">总结</h2><ul><li>与逻辑分析相比“灵感”更加重要</li><li>灵活利用线索卡，大家一起进行思考</li><li>将企划会议变成大家一起思考的会议</li><li>积极听取其他部门和其他领域的人的意见</li></ul><h2 id="第三章取得10倍成果的方法-以10倍的速度思考就能更快地取得成果">第三章取得10倍成果的方法 以10倍的速度思考，就能更快地取得成果</h2><ul><li>目标不是提高 <code>10%</code>，而是提高到 <code>10</code> 倍</li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/7.jpg" alt="" /><figcaption>以取得10倍的成果为目标，就算没达成目标也是成功</figcaption></figure><ul><li>不打破规则就不可能取得10倍的成果</li><li>做一个敢于打破规则的人</li><li>承担风险是为了取得成功</li><li>“比去年提高10%”这一目标的错误之处</li><li>活用“20%规则”的方法</li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/8.jpg" alt="" /><figcaption>应该自己做的工作与应该交给别人的工作</figcaption></figure><ul><li>为了走入新的阶段必须“让自己的工作消失”</li><li>Think like an owner</li><li>成功取得10倍成果的人的共同点<ol type="1"><li>拥有预见性<ul><li>预见机会与威胁</li><li>寻找周期、趋势以及规律</li><li>短期、中期与长期思考</li></ul></li><li>换位思考</li><li>敢于提出自己的见解</li><li>敢说真话</li><li>积极参与交流</li><li>倾听自己内心的声音</li><li>打破常规</li><li>不害怕失败</li><li>勤于思考、保持怀疑</li><li>改变视角<ul><li>整体视角</li><li>局部视角</li><li>反面视角</li><li>未来视角</li><li>顾客视角</li><li>竞争对手视角</li><li>特殊视角（一般情况下、更深层次的情况下、反常的情况下）</li></ul></li></ol></li></ul><h3 id="总结-2">总结</h3><ul><li>思考如何取得10倍的成果</li><li>为了取得10倍的成果必须要打破规则</li><li>为了进入下一个阶段，必须“让自己的工作消失”</li><li>像公司的所有者那样思考</li></ul><h2 id="创建提高工作效率的人际关系的方法-能够让每个人都发挥出全部实力的心理安全究竟是什么">创建提高工作效率的人际关系的方法 能够让每个人都发挥出全部实力的“心理安全”究竟是什么</h2><ul><li>用“实物”说话</li></ul><blockquote><p>与对程序员说“我想在这里增加一个这样的功能……”相比，将拥有这个功能的程序实际运行起来给对方看更加便于理解。</p></blockquote><ul><li>让自己平易近人</li><li>告诉部下“上司的使用方法”<ul><li>自己能解决的事情请自己解决。</li><li>不要只带着问题来找我，同时还要带来解决办法。</li><li>遇到无法解决的问题，请告诉我你需要什么（比如需要建议、决定，还是需要我出面动用权限）。</li></ul></li><li>如何创建心理安全程度较高的环境</li><li>创建“反馈渠道”</li><li>绝对不能完全否定对方的意见</li><li>提高工作效率的不是流程而是“人”</li><li>你的人际圈将改变你的人生</li><li>发现最有能力的人</li><li>改变人际关系的优先顺序</li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/11.jpg" alt="" /><figcaption>人际关系的优先顺序</figcaption></figure><ul><li>与关键人物建立联系</li></ul><h3 id="总结-3">总结</h3><ul><li>用“实物”说话可以使工作更有效率</li><li>取消多余的会议</li><li>与部下的交流每周一次就够了</li><li>在工作之外也建立起人际关系，可以使工作更有效率</li><li>优先与“新结识的人”“不断变化的人”“高水平的人”交流</li></ul><h2 id="迅速学习必要技能的方法-去学校学习不如向同事学习">迅速学习必要技能的方法 去学校学习不如向同事学习</h2><ul><li>应该学习的不是知识而是经验</li><li>“检索时代”学习的基本原则</li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/12.jpg" alt="" /><figcaption>新的“学习循环”</figcaption></figure><ul><li>与学习相关的“询问”规则<ul><li>向别人询问的时候，一定要提出自己的假设</li></ul></li><li>向擅长工作的人询问</li><li>在职场中“学习”</li><li>利用反馈获得自己意想不到的情报</li><li>工作前进行“前馈”</li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/13.jpg" alt="" /><figcaption>反馈与前馈</figcaption></figure><ul><li>提问 4 要素<ol type="1"><li>具体来说</li><li>要在什么地方</li><li>改变什么</li><li>怎样做，才能让工作比较顺利</li></ol></li><li>实践比研修更容易获得自信</li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/14.jpg" alt="" /><figcaption>NLP（神经语言程序学）行动金字塔</figcaption></figure><ul><li>通过交流学习</li><li>多参加交流</li><li>不要排斥不同领域的人</li><li>孤身一人不如齐心协力</li><li>“for”与“with”</li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/15.jpg" alt="" /><figcaption>“for”与“with”</figcaption></figure><ul><li>为什么要学习<ul><li>拥有的“选项”越多，在竞争中生存下来的可能性就越大</li><li>不一定是最强大，但一定是最有适应性</li></ul></li><li>思维模式<ul><li><strong>成长型思维</strong></li><li>学习型思维</li><li>回避型思维</li><li>证明型思维。</li></ul></li><li>不断改变，坚持学习</li></ul><h3 id="总结-4">总结</h3><ul><li>学习=检索+询问专业人士·询问他人·询问同事</li><li>只有反馈远远不够，还要灵活利用“前馈”</li><li>预先建立一个能够轻松询问的交流关系</li><li>不断改变，坚持学习</li></ul><h2 id="谷歌的轻松工作方法-简化心灵">谷歌的轻松工作方法 简化心灵</h2><ul><li>留出关闭电脑的时间</li><li>将同时进行多项工作的时间与专心致志的时间区分开</li><li>同时进行多项工作的技<ul><li>冲刺工作法</li></ul></li><li>应对感情波动<ul><li><strong>你发现自己现在正在生气，那就将这个情绪状态说出来</strong></li></ul></li><li>睡午觉、吃零食、放松是自己的责任</li><li>用性善论来管理企业</li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/16.jpg" alt="" /><figcaption>日本人拥有“工作价值”的比率较低</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/17.jpg" alt="" /><figcaption>new work rules（新的工作要求）</figcaption></figure><h3 id="总结-5">总结</h3><ul><li>偶尔关闭电脑</li><li>一分钟的冥想就能够改变注意力</li><li>在不同的时间段集中精力做一件事</li><li>不要尝试消灭感情，要保持中庸</li><li>休息也是自己的责任</li></ul><h2 id="破坏自己工作的人将创建下一个时代">破坏自己工作的人，将创建下一个时代</h2><ul><li>不让AI抢走自己的工作</li><li>分析时代的发展变化</li><li>如何掌握最新的科技</li><li>积极尝试热门应用程序</li><li>就算对技术细节不了解，也要跟上趋势<ul><li>自动化</li><li>就算自己做不到，也可以去找相应领域的工程师或者程序员来帮忙</li></ul></li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/02/9787540492908/18.jpg" alt="" /><figcaption>就连专家也无法准确预测手机市场的发展趋势</figcaption></figure><ul><li>不要害怕变化</li><li>你是否成了习惯的奴隶<ul><li>人一旦习惯了每天的行动模式，就会不再思考自己为什么工作以及怎样做才能让工作变得更有效率。</li><li>而一旦停止思考，就不会有新的发现。没有新的发现，当然也不会出现改变。</li><li>利用IT实现自动化，找别人帮忙，将工作分解成许多小任务分派下去，利用外部资源。</li></ul></li><li>工作不能“和昨天一样”</li><li>现在的世界绝对不是理所当然的</li></ul><h3 id="总结-6">总结</h3><ul><li>思考如何用IT代替自己工作</li><li>站在革新的一侧，不能袖手旁观</li><li>就算对技术细节不了解，也要跟上趋势</li><li>工作不能“和昨天一样”</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/02/9787540492908/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;Google工作法&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：Google工作法&lt;br /&gt;
作者：[波]彼得·费利克斯·格日瓦奇&lt;br /&gt;
译者：朱悦玮&lt;br /&gt;
出版社：湖南文艺出版社&lt;br /&gt;
出版时间：2019-08&lt;br /&gt;
ISBN：9787540492908&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;明明很努力地工作，但工作总是做不完。&lt;br /&gt;
明明很努力地工作，但工作总是不顺利。&lt;br /&gt;
而且，这样的状态还一直在持续...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;——这本书将彻底解决你的烦恼。&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="Google" scheme="https://2020.iosdevlog.com/tags/Google/"/>
    
  </entry>
  
  <entry>
    <title>Manim 分析</title>
    <link href="https://2020.iosdevlog.com/2020/03/01/manim-turorial/"/>
    <id>https://2020.iosdevlog.com/2020/03/01/manim-turorial/</id>
    <published>2020-03-01T07:19:30.000Z</published>
    <updated>2020-03-02T12:18:15.401Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/01/manim-turorial/0.png" alt="" /><figcaption>manim</figcaption></figure><p><a href="https://github.com/3b1b/manim" target="_blank" rel="noopener" class="uri">https://github.com/3b1b/manim</a></p><p>Installation 安装</p><p>Manim runs on <code>Python 3.7</code>. You can install it from PyPI via pip:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install manimlib</span><br></pre></td></tr></table></figure><p>System requirements are cairo, ffmpeg, sox, latex (optional, if you want to use LaTeX).</p><p>You can now use it via the manim command. For example:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">manim my_project.py MyScene</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="manim">Manim</h2><p>manim 库的大部分功能都分解为功能不同的文件夹。用于编写动画脚本的主要工具是 scene, mobject, camera, 和 animation 文件夹，其中包含它们各自的类。在这四个文件夹中，mobject 文件夹是用户大多数时间与之交互的文件夹。它包含动画中使用的所有几何图形，文本和图形的类。</p><p>程序的实际结构很容易掌握。通过在 python 文件中声明 Scene 类的子类来创建动画或场景。动画的实际代码进入一种称为 “construct 构造” 的方法。这是manim场景的关键字。每当 Scene 类中的对象被初始化时，它都会调用 <code>self.setup()</code> 和 <code>self.construct()</code> 方法。如果未实现后者，则会出现错误。但是，前者是可选的，但在同时处理多个场景时很有用。</p><p>播放和保存动画是从命令行进行的。上面显示了播放动画的基本命令，并将python文件的名称和场景子类传递给 <code>manim.py</code>（实际上是 <code>manimlib/__init__.py</code>）。<code>-pm</code> 标志将预览中等（720p）品质的动画并将其保存在媒体文件夹中。在 <code>config.py</code> 中可以找到更多标志类型，例如裁剪某些帧的标志或导出 <code>.gif</code> 动画的标志。</p><p>当命令行调用其文件时，也会在场景声明之外执行任何 python 代码。这意味着可以删除不明确依赖 <code>manim</code> 功能的代码 <code>construct(self)</code>。这很有用，因为它可以防止结构定义混乱。</p><p>manim 中的大多数类都带有 <code>CONFIG</code> 与之关联的字典。它始终出现在类定义的顶部。这是 <code>Camera</code> 类的 CONFIG：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Camera</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">  CONFIG = &#123;</span><br><span class="line">    <span class="string">"background_image"</span>: <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"pixel_height"</span>: DEFAULT_PIXEL_HEIGHT,</span><br><span class="line">    <span class="string">"pixel_width"</span>: DEFAULT_PIXEL_WIDTH,</span><br><span class="line">    <span class="string">"frame_rate"</span>: DEFAULT_FRAME_RATE,</span><br><span class="line">    <span class="string">"frame_height"</span>: FRAME_HEIGHT,</span><br><span class="line">    <span class="string">"frame_width"</span>: FRAME_WIDTH,</span><br><span class="line">    <span class="string">"frame_center"</span>: ORIGIN,</span><br><span class="line">    <span class="string">"background_color"</span>: BLACK,</span><br><span class="line">    <span class="string">"background_opacity"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"max_allowable_norm"</span>: FRAME_WIDTH,</span><br><span class="line">    <span class="string">"image_mode"</span>: <span class="string">"RGBA"</span>,</span><br><span class="line">    <span class="string">"n_channels"</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">"pixel_array_dtype"</span>: <span class="string">'uint8'</span>,</span><br><span class="line">    <span class="string">"z_buff_func"</span>: <span class="keyword">lambda</span> m: np.round(m.get_center()[<span class="number">2</span>], <span class="number">2</span>),</span><br><span class="line">    <span class="string">"cairo_line_width_multiple"</span>: <span class="number">0.01</span>,</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>初始化对象时，CONFIG 的条目用作要通过 <code>__init__</code> 传递的关键字参数。这对于创建场景非常方便，因为在命令行中将几个关键字参数传递给感兴趣的场景很麻烦。通常，这也是用户与 <code>Camera</code> 类进行交互的地方，因为 <code>camera</code> CONFIG是场景CONFIG中的一个条目。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Scene(Container):</span><br><span class="line"></span><br><span class="line">  CONFIG &#x3D; &#123;</span><br><span class="line">    &quot;camera_class&quot;: Camera,</span><br><span class="line">    &quot;camera_config&quot;: &#123;&#125;,</span><br><span class="line">    &quot;file_writer_config&quot;: &#123;&#125;,</span><br><span class="line">    &quot;skip_animations&quot;: False,</span><br><span class="line">    &quot;always_update_mobjects&quot;: False,</span><br><span class="line">    &quot;random_seed&quot;: 0,</span><br><span class="line">    &quot;start_at_animation_number&quot;: None,</span><br><span class="line">    &quot;end_at_animation_number&quot;: None,</span><br><span class="line">    &quot;leave_progress_bars&quot;: False,</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>所有大写变量都来自文件 <code>constants.py</code>，该文件可用作参考，因为许多类和方法都将这些常量用作默认参数。CONFIG 字典尊重类和子类之间的继承，因此 manim 中的许多类具有比其类定义中更大的 CONFIG 参数。这对于大量使用子类化的 mobjects 特别重要。例如，这是的子类结构 <code>mobject/geometry.py</code>。</p><figure><img src="https://2020.iosdevlog.com/2020/03/01/manim-turorial/1.png" alt="" /><figcaption>manim</figcaption></figure><h2 id="animationswithmanim1">AnimationsWithManim<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h2><figure><img src="https://2020.iosdevlog.com/2020/03/01/manim-turorial/2.png" alt="" /><figcaption>Manim</figcaption></figure><p><span class="math display">\[Manim = Python3（核心）+ latex（文字排版） + cairo（生成图形）+ ffmpeg（转码视频）+ sox（音频处理）\]</span></p><figure><img src="https://2020.iosdevlog.com/2020/03/01/manim-turorial/3.png" alt="" /><figcaption>优点</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/03/01/manim-turorial/4.png" alt="" /><figcaption>大纲</figcaption></figure><p>具体安装可参考：<a href="https://github.com/Elteoremadebeethoven/AnimationsWithManim/blob/master/English/0_instalation/macOS/INSTRUCTIONS.md" target="_blank" rel="noopener">Installation on MacOS</a></p><h3 id="mactex2">MacTeX<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/01/manim-turorial/5.png" alt="" /><figcaption>MacTeX</figcaption></figure><p>下载 <code>MacTeX.pkg</code> 安装</p><h3 id="homebrew3">Homebrew<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/ruby -e <span class="string">"<span class="variable">$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)</span>"</span></span><br></pre></td></tr></table></figure><h3 id="python34">Python3<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></h3><p>可以直接去官网官下载最新版的 <code>Python3</code></p><p>或者用 <code>brew</code> 安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install python3</span><br></pre></td></tr></table></figure><p><code>python3</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Python 3.7.4 (default, Sep 28 2019, 16:39:19) </span><br><span class="line">[Clang 11.0.0 (clang-1100.0.33.8)] on darwin</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><h3 id="下载cairoffmpegsoxlatex和其他的包">下载cairo，ffmpeg，sox，latex和其他的包</h3><p>返回终端的根目录（如果你之前进入了python3，输入exit() 退出，或者重新打开终端），终端输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brew install cairo</span><br><span class="line">brew install ffmpeg</span><br><span class="line">brew install sox</span><br></pre></td></tr></table></figure><h3 id="manim5">Manim<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></h3><p>Manim is an animation engine for explanatory math videos. It's used to create precise animations programmatically, as seen in the videos at 3Blue1Brown.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/3b1b/manim.git</span><br></pre></td></tr></table></figure><p>Install list requirements.txt</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install -r requirements.txt</span><br><span class="line">python3 -m pip install pyreadline</span><br><span class="line">python3 -m pip install pydub</span><br></pre></td></tr></table></figure><p>Run Manim</p><p>With the terminal in manim-master directory run this:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 -m manim example_scenes.py WriteStuff -pl</span><br><span class="line">python3 -m manim example_scenes.py SquareToCircle -pl</span><br></pre></td></tr></table></figure><p>The <code>-p</code> flag in the command above is for <strong>previewing</strong>, meaning the video file will automatically open when it is done rendering.</p><p>The <code>-l</code> flag is for a faster rendering at a <strong>lower quality</strong>.</p><p>Some other useful flags include:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-s to skip to the end and just show the final frame.</span><br><span class="line">-n &lt;number&gt; to skip ahead to the n<span class="string">'th animation of a scene.</span></span><br><span class="line"><span class="string">-f to show the file in finder (for OSX).</span></span><br></pre></td></tr></table></figure><p>Anaconda Install</p><p>Install sox and latex as above.<br />Create a conda environment using</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yml</span><br></pre></td></tr></table></figure><p>Using virtualenv and virtualenvwrapper</p><p>After installing virtualenv and virtualenvwrapper</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip3 install virtualenvwrapper</span><br><span class="line">VIRTUALENVWRAPPER_PYTHON=/usr/<span class="built_in">local</span>/bin/python3</span><br><span class="line"><span class="built_in">source</span> /usr/<span class="built_in">local</span>/bin/</span><br><span class="line">virtualenvwrapper.sh</span><br></pre></td></tr></table></figure><p>mk_manim</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkvirtualenv -r requirements.txt mk_manim</span><br><span class="line">(mk_manim) $ python -m pip install pycairo</span><br><span class="line">(mk_manim) $ python3 -m manim example_scenes.py SquareToCircle -pl</span><br></pre></td></tr></table></figure><h2 id="error">Error</h2><p>Manim ModuleNotFoundError: No module named 'cairo'<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install pycairo</span><br></pre></td></tr></table></figure><h2 id="参考">参考</h2><section class="footnotes" role="doc-endnotes"><hr /><ol><li id="fn1" role="doc-endnote"><p>AnimationsWithManim: <a href="https://github.com/Elteoremadebeethoven/AnimationsWithManim" target="_blank" rel="noopener" class="uri">https://github.com/Elteoremadebeethoven/AnimationsWithManim</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn2" role="doc-endnote"><p>MacTex: <a href="https://www.tug.org/mactex/mactex-download.html" target="_blank" rel="noopener" class="uri">https://www.tug.org/mactex/mactex-download.html</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn3" role="doc-endnote"><p>Homebrew: <a href="https://brew.sh" target="_blank" rel="noopener" class="uri">https://brew.sh</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn4" role="doc-endnote"><p>Python: <a href="https://www.python.org/downloads/" target="_blank" rel="noopener" class="uri">https://www.python.org/downloads/</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn5" role="doc-endnote"><p>Manim: <a href="https://github.com/3b1b/manim" target="_blank" rel="noopener" class="uri">https://github.com/3b1b/manim</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn6" role="doc-endnote"><p><a href="https://github.com/3b1b/manim/issues/392" target="_blank" rel="noopener" class="uri">https://github.com/3b1b/manim/issues/392</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/01/manim-turorial/0.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;manim&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/3b1b/manim&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://github.com/3b1b/manim&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Installation 安装&lt;/p&gt;
&lt;p&gt;Manim runs on &lt;code&gt;Python 3.7&lt;/code&gt;. You can install it from PyPI via pip:&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip3 install manimlib&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;System requirements are cairo, ffmpeg, sox, latex (optional, if you want to use LaTeX).&lt;/p&gt;
&lt;p&gt;You can now use it via the manim command. For example:&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;manim my_project.py MyScene&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="math" scheme="https://2020.iosdevlog.com/categories/math/"/>
    
    
      <category term="manim" scheme="https://2020.iosdevlog.com/tags/manim/"/>
    
  </entry>
  
  <entry>
    <title>《Python神经网络编程》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/02/29/9787115474810/"/>
    <id>https://2020.iosdevlog.com/2020/02/29/9787115474810/</id>
    <published>2020-02-29T14:47:41.000Z</published>
    <updated>2020-03-02T12:22:46.494Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/1.jpg" alt="" /><figcaption>《Python神经网络编程》</figcaption></figure><p>书名：Python神经网络编程<br />作者：[英]塔里克·拉希德（Tariq Rashid）<br />译者：林赐<br />出版社：人民邮电出版社<br />出版时间：2018-04<br />ISBN：9787115474810</p><p><strong>参照本书，自己可以动手写一个简单的神经网络，还不快来看看。</strong></p><a id="more"></a><h2 id="内容提要">内容提要</h2><p>神经网络是一种模拟人脑的神经网络，以期能够实现类人工智能的机器学习技术。</p><p>本书揭示神经网络背后的概念，并介绍如何通过Python实现神经网络。</p><p>全书分为3章和两个附录。</p><ol type="1"><li>第1章介绍了神经网络中所用到的数学思想。<ul><li>我们将如清风拂面般，一览在简单的神经网络中所用的数学思想。我们有意不介绍任何计算机编程知识，以避免喧宾夺主地干扰了本书的核心思想。</li></ul></li><li>第2章 介绍使用Python实现神经网络，识别手写数字，并测试神经网络的性能。<ul><li>我们将学习足以实现自己的神经网络的Python知识。我们将训练神经网络，识别手写数字，并且会测试神经网络的性能。</li></ul></li><li>第3章 带领读者进一步了解简单的神经网络，观察已受训练的神经网络内部，尝试进一步改善神经网络的性能，并加深对相关知识的理解。<ul><li>我们将进一步了解简单的神经网络，这超出了了解基本神经网络知识的范畴，但是我们这样做只是为了获得一些乐趣。我们将尝试一些想法，进一步改善神经网络的性能，我们将观察已受训练的神经网络内部，看看我们是否理解神经网络所学习到的知识，是否理解神经网络是如何做出决定进行回答的。</li></ul></li></ol><p>附录分别介绍了所需的微积分知识和树莓派知识。</p><h3 id="本书适合">本书适合</h3><ol type="1"><li>想要从事神经网络研究和探索的读者学习参考</li><li>对人工智能、机器学习和深度学习等相关领域感兴趣</li></ol><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/2.jpg" alt="" /><figcaption>国际象棋机器Turkey</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/3.jpg" alt="" /><figcaption>MNIST</figcaption></figure><h2 id="第1章-神经网络如何工作">第1章 神经网络如何工作</h2><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/4.jpg" alt="" /><figcaption>处理图像</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/4.jpg" alt="" /><figcaption>对比</figcaption></figure><blockquote><p>有些任务，对传统的计算机而言很容易，对人类而言却很难。例如，对数百万个数字进行乘法运算。</p><p>另一方面，有些任务对传统的计算机而言很难，对人类而言却很容易。例如，从一群人的照片中识别出面孔。</p></blockquote><h3 id="一台简单的预测机">一台简单的预测机</h3><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/6.jpg" alt="" /><figcaption>机器</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/7.jpg" alt="" /><figcaption>计算</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/8.jpg" alt="" /><figcaption>一组加法</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/9.jpg" alt="" /><figcaption>转化</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/10.jpg" alt="" /><figcaption>真实情况</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/11.jpg" alt="" /><figcaption>常数C</figcaption></figure><p><span class="math display">\[误差值=真实值-计算值=62.137-50=12.137\]</span></p><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/12.jpg" alt="" /><figcaption>误差值</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/13.jpg" alt="" /><figcaption>误差值变小</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/14.jpg" alt="" /><figcaption>超调</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/15.jpg" alt="" /><figcaption>微调C</figcaption></figure><blockquote><p>所有有用的计算机系统都有一个输入和一个输出，并在输入和输出之间进行某种类型的计算。神经网络也是如此。</p><p>当我们不能精确知道一些事情如何运作时，我们可以尝试使用模型来估计其运作方式，在模型中，包括了我们可以调整的参数。</p><p>如果我们不知道如何将千米转换为英里，那么我们可以使用线性函数作为模型，并使用可调节的梯度值作为参数。改进这些模型的一种好方法是，基于模型和已知真实示例之间的比较，得到模型偏移的误差值，调整参数。</p></blockquote><h3 id="分类器与预测器并无太大差别">分类器与预测器并无太大差别</h3><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/16.jpg" alt="" /><figcaption>小虫子的宽度和长度</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/17.jpg" alt="" /><figcaption>分界线</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/18.jpg" alt="" /><figcaption>分界线</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/19.jpg" alt="" /><figcaption>分界线</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/20.jpg" alt="" /><figcaption>未知小虫</figcaption></figure><h3 id="训练简单的分类器">训练简单的分类器</h3><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/21.jpg" alt="" /><figcaption>实例</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/22.jpg" alt="" /><figcaption>可视化数据</figcaption></figure><p><span class="math display">\[y=A x\]</span></p><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/23.jpg" alt="" /><figcaption>斜率</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/24.jpg" alt="" /><figcaption>误差值</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/25.jpg" alt="" /><figcaption>ΔA)</figcaption></figure><p><span class="math display">\[\Delta \mathrm{A}=\mathrm{L}(\mathrm{E} / x)\]</span></p><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/26.jpg" alt="" /><figcaption>最终直线</figcaption></figure><blockquote><p>我们使用简单的数学，理解了线性分类器输出误差值和可调节斜率参数之间的关系。也就是说，我们知道了在何种程度上调整斜率，可以消除输出误差值。</p><p>使用朴素的调整方法会出现一个问题，即改进后的模型只与最后一次训练样本最匹配，“有效地”忽略了所有以前的训练样本。解决这个问题的一种好方法是使用学习率，调节改进速率，这样单一的训练样本就不能主导整个学习过程。</p><p>来自真实世界的训练样本可能充满噪声或包含错误。适度更新有助于限制这些错误样本的影响。</p></blockquote><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/27.jpg" alt="" /><figcaption>27</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/28.jpg" alt="" /><figcaption>28</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/29.jpg" alt="" /><figcaption>29</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/30.jpg" alt="" /><figcaption>30</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/31.jpg" alt="" /><figcaption>31</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/32.jpg" alt="" /><figcaption>32</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/33.jpg" alt="" /><figcaption>33</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/34.jpg" alt="" /><figcaption>34</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/35.jpg" alt="" /><figcaption>35</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/36.jpg" alt="" /><figcaption>36</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/37.jpg" alt="" /><figcaption>37</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/38.jpg" alt="" /><figcaption>38</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/39.jpg" alt="" /><figcaption>39</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/40.jpg" alt="" /><figcaption>40</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/41.jpg" alt="" /><figcaption>41</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/42.jpg" alt="" /><figcaption>42</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/43.jpg" alt="" /><figcaption>43</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/44.jpg" alt="" /><figcaption>44</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/45.jpg" alt="" /><figcaption>45</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/46.jpg" alt="" /><figcaption>46</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/47.jpg" alt="" /><figcaption>47</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/48.jpg" alt="" /><figcaption>48</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/49.jpg" alt="" /><figcaption>49</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/50.jpg" alt="" /><figcaption>50</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/51.jpg" alt="" /><figcaption>51</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/52.jpg" alt="" /><figcaption>52</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/53.jpg" alt="" /><figcaption>53</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/54.jpg" alt="" /><figcaption>54</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/55.jpg" alt="" /><figcaption>55</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/56.jpg" alt="" /><figcaption>56</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/57.jpg" alt="" /><figcaption>57</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/58.jpg" alt="" /><figcaption>58</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/59.jpg" alt="" /><figcaption>59</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/60.jpg" alt="" /><figcaption>60</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/61.jpg" alt="" /><figcaption>61</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/62.jpg" alt="" /><figcaption>62</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/63.jpg" alt="" /><figcaption>63</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/64.jpg" alt="" /><figcaption>64</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/65.jpg" alt="" /><figcaption>65</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/66.jpg" alt="" /><figcaption>66</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/67.jpg" alt="" /><figcaption>67</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/68.jpg" alt="" /><figcaption>68</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/69.jpg" alt="" /><figcaption>69</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/70.jpg" alt="" /><figcaption>70</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/71.jpg" alt="" /><figcaption>71</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/72.jpg" alt="" /><figcaption>72</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/73.jpg" alt="" /><figcaption>73</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/74.jpg" alt="" /><figcaption>74</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/75.jpg" alt="" /><figcaption>75</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/76.jpg" alt="" /><figcaption>76</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/77.jpg" alt="" /><figcaption>77</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/78.jpg" alt="" /><figcaption>78</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/79.jpg" alt="" /><figcaption>79</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/80.jpg" alt="" /><figcaption>80</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/81.jpg" alt="" /><figcaption>81</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/82.jpg" alt="" /><figcaption>82</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/83.jpg" alt="" /><figcaption>83</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/84.jpg" alt="" /><figcaption>84</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/85.jpg" alt="" /><figcaption>85</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/86.jpg" alt="" /><figcaption>86</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/87.jpg" alt="" /><figcaption>87</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/88.jpg" alt="" /><figcaption>88</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/89.png" alt="" /><figcaption>89</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/90.jpg" alt="" /><figcaption>90</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/91.jpg" alt="" /><figcaption>91</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/92.jpg" alt="" /><figcaption>92</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/93.jpg" alt="" /><figcaption>93</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/94.jpg" alt="" /><figcaption>94</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/95.jpg" alt="" /><figcaption>95</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/96.jpg" alt="" /><figcaption>96</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/97.jpg" alt="" /><figcaption>97</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/98.jpg" alt="" /><figcaption>98</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/99.jpg" alt="" /><figcaption>99</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/100.jpg" alt="" /><figcaption>100</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/101.jpg" alt="" /><figcaption>101</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/102.jpg" alt="" /><figcaption>102</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/103.jpg" alt="" /><figcaption>103</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/104.jpg" alt="" /><figcaption>104</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/105.jpg" alt="" /><figcaption>105</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/106.jpg" alt="" /><figcaption>106</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/107.jpg" alt="" /><figcaption>107</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/108.jpg" alt="" /><figcaption>108</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/109.jpg" alt="" /><figcaption>109</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/110.jpg" alt="" /><figcaption>110</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/111.jpg" alt="" /><figcaption>111</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/112.jpg" alt="" /><figcaption>112</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/113.jpg" alt="" /><figcaption>113</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/114.jpg" alt="" /><figcaption>114</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/115.jpg" alt="" /><figcaption>115</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/116.jpg" alt="" /><figcaption>116</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/117.jpg" alt="" /><figcaption>117</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/118.png" alt="" /><figcaption>118</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/119.png" alt="" /><figcaption>119</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/120.png" alt="" /><figcaption>120</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/121.png" alt="" /><figcaption>121</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/122.jpg" alt="" /><figcaption>122</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/123.jpg" alt="" /><figcaption>123</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/124.jpg" alt="" /><figcaption>124</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/125.jpg" alt="" /><figcaption>125</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/126.jpg" alt="" /><figcaption>126</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/29/9787115474810/127.jpg" alt="" /><figcaption>127</figcaption></figure>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/29/9787115474810/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《Python神经网络编程》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：Python神经网络编程&lt;br /&gt;
作者：[英]塔里克·拉希德（Tariq Rashid）&lt;br /&gt;
译者：林赐&lt;br /&gt;
出版社：人民邮电出版社&lt;br /&gt;
出版时间：2018-04&lt;br /&gt;
ISBN：9787115474810&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参照本书，自己可以动手写一个简单的神经网络，还不快来看看。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="DL" scheme="https://2020.iosdevlog.com/tags/DL/"/>
    
      <category term="NN" scheme="https://2020.iosdevlog.com/tags/NN/"/>
    
      <category term="Python" scheme="https://2020.iosdevlog.com/tags/Python/"/>
    
      <category term="CV" scheme="https://2020.iosdevlog.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>《神经网络与深度学习》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/02/28/9787121288692/"/>
    <id>https://2020.iosdevlog.com/2020/02/28/9787121288692/</id>
    <published>2020-02-28T15:02:30.000Z</published>
    <updated>2020-03-02T12:22:46.500Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/1.jpg" alt="" /><figcaption>《神经网络与深度学习》</figcaption></figure><p>书名：神经网络与深度学习<br />作者：吴岸城<br />出版社：电子工业出版社<br />出版时间：2016-06<br />ISBN：9787121288692</p><p>本书结合日常生活中的寻常小事，生动形象地阐述了神经网络与深度学习的基本概念、原理和实践，案例丰富，深入浅出。</p><p>对于正在进入人工智能时代的我们，这些内容无疑可以帮助我们更好地理解人工智能的原理，丰富我们对人类自身的认识，并启发我们对人机智能之争更深一层的思考与探索。</p><a id="more"></a><h2 id="介绍">介绍</h2><p>第0章，介绍机器学习、神经网络的历史，好让大家有基本的了解。<br />第1章，解释大脑的运作结构和如何利用仿生学产生逻辑上的神经元和神经网络。<br />第2章，我们用仿生学的知识试着构造一个神经网络（感知机）并使用它做些事情，解释了XOR问题。在2.6节给出一些例子，让我们能更好地了解神经网络是如何分类学习和预测的。<br />第3章，介绍深度学习的基本概念，深度学习和神经网络的联系。<br />第4章，介绍深度学习的常用方法。<br />第5章，介绍AlphaGo。<br />第6章，两个重要概念，迁移学习和概率图模型PGM。<br />第7章，给出了一些经验以加快大家学习和研究的效率。</p><h2 id="术语">术语</h2><ol type="1"><li><strong>图灵</strong>：全名艾伦·麦席森·图灵，英国人，因性倾向遭到当时的英国政府迫害，职业生涯尽毁。他可以说是人工智能之父，笔者十分佩服其才智。</li><li><strong>决策树</strong>：是一个预测模型；它代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表某个可能的属性值，每个叶节点对应从根节点到该叶节点所经历的路径所表示的对象的值。</li><li><strong>条件概率</strong>：就是事件A在另外一个事件B已经发生的条件下的发生概率。条件概率表示为P（A|B），读作“在B条件下A的概率”。</li><li><strong>树突、轴突</strong>：神经元的输入和输出部分。5. AND/XOR/OR：数学逻辑运算。</li><li><strong>人工神经元</strong>：是一种模仿生物神经元的结构和功能的数学模型或计算模型。</li><li><strong>感知机</strong>：它被视为是一种最简单的前馈神经网络，是一种二元线性分类器。</li><li><strong>前馈神经网络</strong>：最简单的人工神经网络类型。在它的内部，参数从输入层向输出层单向传播。</li><li><strong>特征</strong>：本书中指将现实生活中的事物的部分特点提取并抽象出一种数学或物理模型。</li><li><strong>特征粒度</strong>：提取特征的维度。</li><li><strong>浅层学习/深度学习</strong>：相对概念，深度学习相对浅层学习抽象层级要多。12. BP算法（反向传播算法）：是一种监督学习算法，常被用来训练多层感知机，利用反向传播原理修正权值。</li><li><strong>自动编码器（AE）</strong>：自动编码器就是一个运用了反向传播进行无监督学习的神经网络，学习的目的是为了让输出值和输入值相等。14. RBM（限制波兹曼机）：是一种可通过输入数据集学习概率分布的随机生成神经网络。</li><li><strong>概率模型</strong>：是用来描述不同随机变量之间关系的数学模型，通常情况下刻画了一个或多个随机变量之间的相互非确定性的概率关系。</li><li><strong>能量模型（EBM）</strong>：基于能量的模型，把我们关心的变量的各种组合和一个标量能量联系在一起。我们训练模型的过程就是不断改变标量能量的过程。</li><li><strong>DBN（深度信度网络）</strong>：通过自底向上组合多个RBM可以构建一个DBN，利用非监督贪心逐层训练算法，解决深层结构相关的优化问题。18. CNN（卷积神经网络/ConvNets）：是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。</li><li><strong>概率图</strong>：是一种使用图来表达随机变量之间条件独立性的概率模型。</li><li><strong>贝叶斯定理</strong>：事件A在事件B（发生）的条件下的概率。</li><li><strong>SVM（支持向量机）</strong>：监督学习方法，属于一般化线性分类器。这种分类器的特点是它们能够同时最小化经验误差与最大化几何边缘区，因此支持向量机也被称为最大边缘区分类器。</li><li><strong>K-Means</strong>：把n个点（可以是样本的一次观察或一个实例）划分到k个聚类中，使得每个点都属于离它最近的均值（此即聚类中心）对应的聚类。</li><li><strong>Java</strong>：一种面向对象的高级语言。</li><li><strong>Python</strong>：是一种解释型的计算机程序语言，具有近20年的发展历史。它包含了一组功能完备的标准库，能够轻松完成很多常见的任务。</li><li><strong>MATLAB</strong>：是一款由美国TheMathWorks公司出品的商业数学软件。MATLAB是一种用于算法开发、数据可视化、数据分析及数值计算的高级技术计算语言和交互式环境。</li><li><strong>C++</strong>：是一种广泛使用的计算机程序设计语言。</li><li><strong>并行计算</strong>：一般是指许多指令得以同时进行的计算模式。</li><li><strong>NASA</strong>：国家航空航天局（英语：National Aeronautics and SpaceAdministration，缩写为NASA），是美国联邦政府的一个行政机构，负责制定、实施美国的民用太空计划，并开展航空科学暨太空科学的研究。</li></ol><h2 id="写在前面神经网络的历史">0 写在前面：神经网络的历史</h2><p>神经网络，是机器学习的一个分支，学名应该叫人工神经网络，与之相对应的是生物神经网络（Biological Neural Networks, BNN），我们将模拟生物神经网络的数学模型统称为人工神经网络模型，简称人工神经网络或者神经网络。</p><p><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/2.jpg" alt="阿兰·麦席森·图灵" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/3.jpg" alt="约翰·麦卡锡" /></p><p>图灵测试（Turing test，又译图灵试验）是图灵提出的一个关于机器能否思考的著名判断原则。</p><figure><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/4.jpg" alt="" /><figcaption>麦卡洛可</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/5.jpg" alt="" /><figcaption>皮茨</figcaption></figure><p>麦卡洛可: 神经科学</p><p>+</p><p>皮茨: 数学</p><p>=</p><p>《神经活动中思想内在性的逻辑演算》（A LogicalCalculus of Ideas Immanent in Nervous Activity）</p><figure><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/6.jpg" alt="" /><figcaption>诺伯特·维纳</figcaption></figure><p>诺伯特·维纳：控制论</p><figure><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/7.jpg" alt="" /><figcaption>迈克尔·阿比卜</figcaption></figure><p>迈克尔·阿比卜：创立麻省理工学院的计算机</p><figure><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/8.jpg" alt="" /><figcaption>弗兰克·罗森布拉特</figcaption></figure><blockquote><p>“感知器最终将能够学习，做出决策和翻译语言”</p></blockquote><p>弗兰克·罗森布拉特：“感知机”（Perceptron）的神经网络模型</p><p>《神经动力学原理：感知机和大脑机制的理论》（Principles of Neurodynamics: Perceptrons and the Theory ofBrainMechanisms）</p><p>保罗·沃波斯（Paul Werbos）：“反向传播算法”（Backpropagation Algorithm，简称BP算法）</p><p>是一种监督学习算法，常被用来训练多层感知机。</p><figure><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/9.jpg" alt="" /><figcaption>霍普菲尔德</figcaption></figure><p>霍普菲尔德：递归神经网络</p><figure><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/10.jpg" alt="" /><figcaption>鲁姆哈特（David Rumelhart）</figcaption></figure><p>鲁姆哈特（David Rumelhart）：完整地提出了BP算法，完整的推导。</p><figure><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/11.jpg" alt="" /><figcaption>杰弗里·辛顿（Geoffrey Hinton）</figcaption></figure><p>杰弗里·辛顿（Geoffrey Hinton）：反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者</p><p>吴恩达：识别“猫”，“深度学习”领域的经典案例</p><h2 id="神经网络是个什么东西">1 神经网络是个什么东西</h2><p><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/12.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/13.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/14.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/15.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/16.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/17.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/18.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/19.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/20.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/21.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/22.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/23.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/24.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/25.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/26.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/27.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/28.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/29.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/30.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/31.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/32.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/33.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/34.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/35.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/36.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/37.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/38.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/39.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/40.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/41.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/42.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/43.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/44.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/45.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/46.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/47.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/48.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/49.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/50.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/51.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/52.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/53.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/54.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/55.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/56.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/57.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/58.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/59.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/60.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/61.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/62.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/63.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/64.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/65.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/66.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/67.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/68.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/69.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/70.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/71.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/72.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/73.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/74.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/75.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/76.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/77.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/78.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/79.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/80.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/81.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/82.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/83.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/84.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/85.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/86.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/87.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/88.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/89.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/90.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/91.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/92.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/93.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/94.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/95.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/96.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/97.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/98.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/99.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/100.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/101.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/102.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/103.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/104.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/105.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/106.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/107.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/108.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/109.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/110.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/111.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/112.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/113.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/114.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/115.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/116.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/117.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/118.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/119.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/120.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/121.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/122.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/123.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/124.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/125.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/126.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/127.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/128.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/129.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/130.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/131.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/132.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/133.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/134.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/135.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/136.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/137.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/138.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/139.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/140.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/141.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/142.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/143.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/144.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/145.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/146.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/147.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/148.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/149.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/150.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/151.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/152.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/153.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/154.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/155.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/156.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/157.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/158.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/159.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/160.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/28/9787121288692/161.jpg" /></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/28/9787121288692/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《神经网络与深度学习》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：神经网络与深度学习&lt;br /&gt;
作者：吴岸城&lt;br /&gt;
出版社：电子工业出版社&lt;br /&gt;
出版时间：2016-06&lt;br /&gt;
ISBN：9787121288692&lt;/p&gt;
&lt;p&gt;本书结合日常生活中的寻常小事，生动形象地阐述了神经网络与深度学习的基本概念、原理和实践，案例丰富，深入浅出。&lt;/p&gt;
&lt;p&gt;对于正在进入人工智能时代的我们，这些内容无疑可以帮助我们更好地理解人工智能的原理，丰富我们对人类自身的认识，并启发我们对人机智能之争更深一层的思考与探索。&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="DL" scheme="https://2020.iosdevlog.com/tags/DL/"/>
    
      <category term="CV" scheme="https://2020.iosdevlog.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>《深度学习与图像识别：原理与实践》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/02/27/9787111630036/"/>
    <id>https://2020.iosdevlog.com/2020/02/27/9787111630036/</id>
    <published>2020-02-27T11:41:38.000Z</published>
    <updated>2020-02-27T15:39:26.676Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/1.jpg" alt="" /><figcaption>《深度学习与图像识别：原理与实践》</figcaption></figure><p>书名：深度学习与图像识别：原理与实践<br />作者：魏溪含，涂铭，张修鹏<br />出版社：机械工业出版社<br />出版时间：2019-06<br />ISBN：9787111630036</p><a id="more"></a><h2 id="介绍">介绍</h2><p>第 1 章 介绍图像识别的一些应用场景，让读者对图像识别有个初步的认识。<br />第 2 章 主要对图像识别的工程背景做简单介绍，同时介绍了本书后续章 节实战案例中会用到的环境，因此该章 是实战的基础。<br />第 3～6 章 是图像识别的技术基础，包括机器学习、神经网络等。该部分的代码主要使用Python实现。没有机器学习基础的同学需要理解这几章 之后再往下看，有机器学习基础的同学可以有选择地学习。<br />第 7 章 是一个过渡章 节，虽然<br />第 6 章 中手动用Python实现了神经网络，但由于本书后面的图像识别部分主要使用PyTorch实现，因此使用该章 作为过渡，介绍如何使用PyTorch来搭建神经网络。<br />第 8～12章 为图像识别的核心。<br />第 8 章 首先介绍了图像中的卷积神经网络与普通神经网络的异同，并给出了常见的卷积神经网络结构。接下来的<br />第 9 ～12章 分别介绍了图像识别中的检测、分割、产生式模型以及可视化的问题，并在每章 后面给出相应的实战案例。<br />第 13 章 简单介绍了图像识别的工业部署模式，以帮助读者构建一个更完整的知识体系。</p><h2 id="第1章-机器视觉在行业中的应用">第1章 机器视觉在行业中的应用</h2><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/2.jpg" alt="" /><figcaption>人工智能相关领域关系图</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/3.jpg" alt="" /><figcaption>人工智能的第三个“春天”</figcaption></figure><h3 id="机器视觉的主要应用场景">机器视觉的主要应用场景</h3><h4 id="人脸识别face-recognition">人脸识别（Face Recognition）</h4><p>处理过程</p><p>人脸图像采集及检测<br />人脸图像预处理<br />人脸图像特征提取<br />匹配与识别</p><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/4.jpg" alt="" /><figcaption>人脸识别的主要应用场景</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/5.jpg" alt="" /><figcaption>人脸识别应用场景</figcaption></figure><h4 id="视频监控分析">视频监控分析</h4><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/6.jpg" alt="视频监控分析的应用场景" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/7.jpg" alt="交通异常事件监测" /></p><h4 id="工业瑕疵检测">工业瑕疵检测</h4><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/8.jpg" alt="" /><figcaption>工业瑕疵诊断应用场景</figcaption></figure><h4 id="图片识别分析">图片识别分析</h4><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/9.jpg" alt="" /><figcaption>图片识别应用效果</figcaption></figure><h4 id="自动驾驶驾驶辅助">自动驾驶/驾驶辅助</h4><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/10.jpg" alt="" /><figcaption>自动驾驶汽车应用场景</figcaption></figure><p>技术链</p><ul><li>感知阶段<ol type="1"><li>使用机器视觉获取场景中的深度信息，以帮助进行后续的图像语义理解，在自动驾驶中帮助探索可行驶区域和目标障碍物。</li><li>通过视频预估每一个像素的运动方向和运动速度。</li><li>对物体进行检测与追踪。在无人驾驶中，检测与追踪的目标主要是各种车辆、行人、非机动车。</li><li>对于整个场景的理解。最重要的有两点，第一是道路线检测，其次是在道路线检测下更进一步，即将场景中的每一个像素都打成标签，这也称为场景分割或场景解析。</li><li>同步地图构建和定位技术。</li></ol></li><li>规划阶段</li><li>控制阶段</li></ul><h4 id="三维图像视觉">三维图像视觉</h4><h4 id="医疗影像诊断">医疗影像诊断</h4><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/11.jpg" alt="医疗影像诊断的应用场景" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/12.jpg" alt="肝脏及结节分割技术" /></p><h4 id="文字识别ocr">文字识别（OCR）</h4><p>计算机文字识别，俗称光学字符识别（Optical Character Recognition），是利用光学扫描技术将票据、报刊、书籍、文稿及其他印刷品的文字转化为图像信息，再利用文字识别技术将图像信息转化为可以使用的计算机输入技术。</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/13.jpg" alt="文字识别技术的应用场景" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/14.jpg" alt="文字识别技术的应用场景" /></p><h4 id="图像视频的生成及设计">图像/视频的生成及设计</h4><h2 id="第2章-图像识别前置技术">第2章 图像识别前置技术</h2><h3 id="深度学习框架">深度学习框架</h3><ul><li>Theano</li><li>Tensorflow</li><li>MXNet</li><li>Keras</li><li>PyTorch</li><li>Caffe</li></ul><h3 id="搭建图像识别开发环境">搭建图像识别开发环境</h3><p>Anaconda</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/16.jpg" alt="Anaconda的下载" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/17.jpg" alt="打开Anaconda进入Jupyter" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/18.jpg" alt="Jupyter notebook界面" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/19.jpg" alt="Anaconda环境测试界面" /></p><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/20.jpg" alt="" /><figcaption>通过conda搜索beautifulsoup</figcaption></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建第2~7章代码运行的环境：</span></span><br><span class="line">conda create -n basic_env  python=3.7        <span class="comment"># 创建一个名为basic_env的环境</span></span><br><span class="line"><span class="built_in">source</span> activate basic_env                <span class="comment"># 激活这个环境—Linux和macOS代码</span></span><br><span class="line">activate basic_env                        <span class="comment"># 激活这个环境—Windows代码</span></span><br><span class="line"><span class="comment"># 创建第8~12章代码运行的环境：</span></span><br><span class="line">conda create -n imgrecognition_env  python=3.7</span><br><span class="line">                                                <span class="comment"># 创建一个名为imgrecognition _env的环境</span></span><br><span class="line"><span class="built_in">source</span> activate imgrecognition _env        <span class="comment"># 激活这个环境—Linux和macOS代码</span></span><br><span class="line">activate imgrecognition_env                <span class="comment"># 激活这个环境—Windows代码</span></span><br></pre></td></tr></table></figure><p>Pytorch 的下载与安装</p><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/22.jpg" alt="" /><figcaption>PyTorch安装界面</figcaption></figure><h3 id="numpy">Numpy</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/24.jpg" alt="创建数组" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/25.jpg" alt="创建数组" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/26.jpg" alt="在Notebook中引入Numpy" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/28.jpg" alt="Numpy预置函数及说明" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/31.jpg" alt="KNN例子" /></p><h2 id="第3章-图像分类之knn算法">第3章 图像分类之KNN算法</h2><h3 id="knn的理论基础与实现">KNN的理论基础与实现</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/33.jpg" alt="KNN例子的散点图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/34.jpg" alt="电脑看到的图片均为0～255的数字" /></p><h3 id="图像分类识别预备知识">图像分类识别预备知识</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/36.jpg" alt="归一化图示" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/37.jpg" alt="数字5" /></p><h3 id="knn实战">KNN实战</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/38.jpg" alt="两张图片曼哈顿距离的计算方法" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/39.jpg" alt="数字7" /></p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/40.jpg" alt="Cifar10数据集示例" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/41.jpg" alt="青蛙图片" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/42.jpg" alt="整个数据集" /></p><h3 id="模型参数调优">模型参数调优</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/43.jpg" alt="整个数据集拆分成训练集和测试集" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/44.jpg" alt="训练集、验证集和测试集" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/45.jpg" alt="交叉验证的数据拆分方法" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/46.jpg" alt="图像中具体某个像素值的无意义性" /></p><h2 id="第4章-机器学习基础">第4章 机器学习基础</h2><h3 id="线性回归模型">线性回归模型</h3><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/48.jpg" alt="" /><figcaption>线性回归拟合直线</figcaption></figure><h3 id="逻辑回归模型">逻辑回归模型</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/68.jpg" alt="逻辑回归分类示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/69.jpg" alt="Sigmoid函数" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/73.jpg" alt="Sigmoid函数图像" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/74.jpg" alt="损失函数示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/75.jpg" alt="一元二次损失函数梯度下降过程示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/76.jpg" alt="学习率η=0.01时，一元二次损失函数梯度下降过程示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/77.jpg" alt="学习率η=0.8时，一元二次损失函数梯度下降过程示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/78.jpg" alt="学习率η=1.1时，一元二次损失函数不收敛" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/82.jpg" alt="损失函数if y=1" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/84.jpg" alt="损失函数if y=0" /></p><h2 id="第5章-神经网络基础">第5章 神经网络基础</h2><h3 id="神经网络">神经网络</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/95.jpg" alt="神经网络全连接结构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/96.jpg" alt="多隐藏层结构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/97.jpg" alt="神经元结构图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/98.jpg" alt="简单神经元" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/99.jpg" alt="训练网络" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/100.jpg" alt="神经元个数较少" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/101.jpg" alt="神经元个数较多" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/102.jpg" alt="神经元个数更多" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/103.jpg" alt="线性分类图1" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/104.jpg" alt="线性分类图2" /></p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/105.jpg" alt="线性不可分" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/106.jpg" alt="激活函数表达能力" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/108.jpg" alt="Sigmoid函数" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/110.jpg" alt="Tanh函数" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/111.jpg" alt="ReLU函数" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/112.jpg" alt="前向传播 1" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/113.jpg" alt="节点1节点2" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/116.jpg" alt="前向传播 2" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/118.jpg" alt="增加bias" /></p><h3 id="输出层">输出层</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/119.jpg" alt="Softmax" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/124.jpg" alt="猫狗小鸡分类" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/127.jpg" alt="输出层的神经元个数" /></p><h3 id="批处理">批处理</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/128.jpg" alt="单个处理" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/129.jpg" alt="批处理" /></p><h3 id="广播原则">广播原则</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/130.jpg" alt="广播原则1" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/131.jpg" alt="广播原则2" /></p><h3 id="损失函数">损失函数</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/132.jpg" alt="均方误差" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/136.jpg" alt="交叉熵误差" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/137.jpg" alt="带入Loss函数" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/138.jpg" alt="Mini-batch" /></p><h3 id="最优化">最优化</h3><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/139.jpg" alt="" /><figcaption>一维函数求导</figcaption></figure><h2 id="第6章-误差反向传播">第6章 误差反向传播</h2><h3 id="激活函数层的实现">激活函数层的实现</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/144.jpg" alt="x+y计算图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/145.jpg" alt="（x+y）*z的计算图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/148.jpg" alt="ReLU反向传播实现" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/149.jpg" alt="导数" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/150.jpg" alt="Sigmoid反向传播实现" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/151.jpg" alt="Sigmoid计算图" /></p><h3 id="affine层的实现">Affine层的实现</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/152.jpg" alt="152" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/153.jpg" alt="153" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/154.jpg" alt="154" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/155.jpg" alt="155" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/162.jpg" alt="162" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/163.jpg" alt="163" /></p><h3 id="softmaxwithloss层的实现">Softmaxwithloss层的实现</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/164.jpg" alt="164" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/165.jpg" alt="165" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/166.jpg" alt="166" /></p><h3 id="正则化惩罚">正则化惩罚</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/167.jpg" alt="167" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/169.jpg" alt="169" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/171.jpg" alt="正则化项在神经网络中的重要作用" /></p><h2 id="第7章-pytorch实现神经网络图像分类">第7章 PyTorch实现神经网络图像分类</h2><h3 id="pytorch的使用">PyTorch的使用</h3><p>Variable</p><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/172.jpg" alt="" /><figcaption>Variable</figcaption></figure><p>激活函数</p><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/173.jpg" alt="" /><figcaption>激活函数可视化</figcaption></figure><h3 id="pytorch实战">PyTorch实战</h3><h2 id="第8章-卷积神经网络">第8章 卷积神经网络</h2><p>卷积神经网络（Convolutional Neural Network，CNN）是一种深度前馈神经网络，目前在图片分类、图片检索、目标检测、目标分割、目标跟踪、视频分类、姿态估计等图像视频相关领域中已有很多较为成功的应用。</p><h3 id="卷积神经网络基础">卷积神经网络基础</h3><p>全连接层（Fully Connected Layer）</p><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/174.jpg" alt="" /><figcaption>全连接示意图</figcaption></figure><p>卷积层（Convolution Layer）</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/175.jpg" alt="一维卷积kernel=1*3，stride=1计算过程示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/176.jpg" alt="一维卷积kernel=1*3，stride=2计算过程示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/177.jpg" alt="二维卷积，kernel=3*3，stride=1计算过程示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/178.jpg" alt="二维卷积，kernel=3*3，stride=2计算过程示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/179.jpg" alt="三维卷积kernel=553，stride=1，计算过程示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/181.jpg" alt="卷积神经网络示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/182.jpg" alt="kernel=3*3，pad=1示意图" /></p><p>池化层</p><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/182.jpg" alt="" /><figcaption>池化filter=2*2，stride=2的最大池化（max pooling）操作</figcaption></figure><h3 id="常见卷积神经网络结构">常见卷积神经网络结构</h3><p>AlexNet</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/187.jpg" alt="AlexNet" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/188.jpg" alt="ReLUs与tanh作为激活函数在4层卷积神经网络中的收敛速度对比" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/189.jpg" alt="ILSVRC图像识别分类比赛优胜情况" /></p><p>VGGNet</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/190.jpg" alt="AlexNet和VGGNet网络结构对比" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/191.jpg" alt="一维卷积中3组33与1组77kernel效果相同的原理解说图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/192.jpg" alt="VGG16Net网络结构" /></p><p>GoogLeNet</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/193.jpg" alt="矩阵转换方式" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/194.jpg" alt="简单的inception结构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/195.jpg" alt="简单inception结构对应计算量" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/196.jpg" alt="降维的inception结构及计算量推导" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/197.jpg" alt="GoogLeNet网络结构图" /></p><p>ResNet</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/198.jpg" alt="一个20层和56层卷积神经网络中训练和预测过程中的误差情况" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/201.jpg" alt="普通卷积层与残差卷积层" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/202.jpg" alt="ResNet网络结构缩略图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/203.jpg" alt="不同网络结构性能对比" /></p><p>ResNeXT</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/204.jpg" alt="加宽的残差网络模块" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/205.jpg" alt="ResNeXT网络模块" /></p><p>DenseNet</p><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/206.jpg" alt="" /><figcaption>DenseNet核心网络结构</figcaption></figure><h3 id="vgg16实现cifar10分类">VGG16实现Cifar10分类</h3><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/207.jpg" alt="" /><figcaption>VGG16训练Cifar10过程输出</figcaption></figure><h2 id="第9章-目标检测">第9章 目标检测</h2><h3 id="定位分类">定位+分类</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/208.jpg" alt="检测问题定义" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/209.jpg" alt="分类问题vs定位问题" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/210.jpg" alt="分类+定位网络结构设计" /></p><h3 id="目标检测">目标检测</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/211.jpg" alt="使用定位+分类解决目标检测存在的问题" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/212.jpg" alt="使用滑窗方法做目标检测存在的问题：滑窗的尺寸、大小、位置不同将产生非常大的计算量" /></p><p>R-CNN</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/213.jpg" alt="R-CNN训练过程" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/214.jpg" alt="不同压缩方法图示" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/215.jpg" alt="IOU图示" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/217.jpg" alt="R-CNN中的ROI结果微调" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/218.jpg" alt="Fast R-CNN训练和预测过程示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/219.jpg" alt="Fast R-CNN中的ROI Pooling" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/220.jpg" alt="R-CNN和Fast R-CNN训练和测试时间对比" /></p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/221.jpg" alt="Faster R-CNN训练流程" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/222.jpg" alt="RPN原理" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/223.jpg" alt="RCNN、Fast R-CNN、Faster R-CNN模型耗时对比" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/224.jpg" alt="RCNN、Fast R-CNN、Faster R-CNN模型对比" /></p><p>YOLO</p><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/225.jpg" alt="" /><figcaption>基于PASCAL VOC2012目标检测数据集的YOLO示意图</figcaption></figure><p>SSD</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/226.jpg" alt="SSD特征层与anchor示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/227.jpg" alt="SSD结构图" /></p><h3 id="ssd实现voc目标检测">SSD实现VOC目标检测</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/228.jpg" alt="原始图片" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/229.jpg" alt="语义分割的真实label图片" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/230.jpg" alt="实例分割的真实label图片" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/231.jpg" alt="ResNet50训练PASCAL VOC过程部分打印结果展示" /></p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/232.jpg" alt="SSD效果示意图（未完全迭代的结果）" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/233.jpg" alt="SSD作者在VOC2007数据集上达到的效果" /></p><h2 id="第10章-分割">第10章 分割</h2><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/234.jpg" alt="分割问题定义" /></p><p>FCN</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/235.jpg" alt="最简单直观的语义分割方法" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/236.jpg" alt="改良后的CNN语义分割网络结构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/237.jpg" alt="Unpooling的几种方法" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/238.jpg" alt="卷积和反卷积图例" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/239.jpg" alt="kernel为3、stride为2的1维反卷积计算过程" /></p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/240.jpg" alt="U-Net结构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/241.jpg" alt="CrackForest训练数据展示" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/242.jpg" alt="U-Net预测CrackForest结果" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/243.jpg" alt="SegNet的网络结构" /></p><p>PSPNet</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/244.jpg" alt="语义分割容易出现的问题" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/245.jpg" alt="PSPNet的网络结构" /></p><h3 id="实例分割">实例分割</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/246.jpg" alt="检测、分割任务对比" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/247.jpg" alt="多任务学习中“head”的设定方法" /></p><p>层叠式</p><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/248.jpg" alt="" /><figcaption>层叠式实例分割网络结构</figcaption></figure><p>扁平式</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/249.jpg" alt="Mask R-CNN的网络结构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/250.jpg" alt="Mask-RCNN的网络head的设计细节" /></p><h2 id="第11章-产生式模型">第11章 产生式模型</h2><p>机器学习</p><ol type="1"><li>有监督学习</li><li>无监督学习</li><li>强化学习</li></ol><p>数据集</p><ol type="1"><li>数据x</li><li>标签y</li></ol><h3 id="自编码器autoencoder">自编码器（Autoencoder）</h3><h3 id="对抗生成网络">对抗生成网络</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/251.jpg" alt="Autoencoder学习过程" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/252.jpg" alt="GAN的训练结构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/254.jpg" alt="GAN最终使用的产生器" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/255.jpg" alt="产生器生成的一些假图的例子" /></p><h3 id="dcgan及实战">DCGAN及实战</h3><p>DCGAN（Deep Convolutional Generative Adversarial Network）由Radford等人提出，结合了深度卷积神经网络和GAN，并对上述GAN进行了扩展。DCGAN将GAN中的产生器G和判别器D都换成了卷积神经网络，并对其中的卷积做了一些改动以提高收敛速度，具体如下。</p><ol type="1"><li>用不同步长的卷积层替换所有Pooling层。</li><li>在D和G中均使用BatchNorm层。</li><li>在G网络中，除最后一层使用tanh以外，其余层均使用ReLU作为激活函数。</li><li>D网络均使用LeakyRelu作为激活函数。</li></ol><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/256.jpg" alt="DCGAN结构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/257.jpg" alt="CelebFaces一些数据的展示" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/258.jpg" alt="产生网络和判别网络的Loss变化情况" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/259.jpg" alt="真假数据对比图" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/260.jpg" alt="DCGAN在LSUN上生成的卧室图片" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/261.jpg" alt="GAN和DCGAN在MNIST上的生成效果" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/262.jpg" alt="生成向量包含的数学信息" /></p><h3 id="lsgan">LSGAN</h3><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/263.jpg" alt="" /><figcaption>不同Loss差异图示</figcaption></figure><h3 id="wgan">WGAN</h3><h3 id="pg-gan">PG-GAN</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/265.jpg" alt="PG-GAN思想" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/266.jpg" alt="PG-GAN中从1616到3232的转换过程" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/267.jpg" alt="PG-GAN产生的高清图片" /></p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/268.jpg" alt="各种GAN方法效果对比" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/269.jpg" alt="一些扩展的GAN可以实现风格转换效果" /></p><h2 id="第12章-神经网络可视化">第12章 神经网络可视化</h2><h3 id="卷积核">卷积核</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/270.jpg" alt="ConvNetJS在Cifar10上训练得到的参数" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/271.jpg" alt="几种常见网络结构的第一层卷积" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/272.jpg" alt="特征层表征" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/273.jpg" alt="卷积神经网络特征层可视化工具" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/274.jpg" alt="不同图片在conv5151上的激活情况，每个特征层都是13*13个像素" /></p><p>通过重构观测</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/275.jpg" alt="“逆”卷积神经网络结构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/276.jpg" alt="反向池化" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/278.jpg" alt="Layer3左上角第一张图的重构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/279.jpg" alt="完全训练的AlexNet在1～5个卷积层中选取被激活最强的9个通道复原后的图片" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/280.jpg" alt="对图12-4a进行重构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/281.jpg" alt="5个特征层经过不同迭代次数的重构效果" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/282.jpg" alt="末端CNN特征层的激活情况" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/283.jpg" alt="遮挡不同区域对图片分类的影响" /></p><p>特征层的作用</p><figure><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/284.jpg" alt="" /><figcaption>利用CNN做特征提取可实现图像搜索功能</figcaption></figure><p>图片风格化</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/285.jpg" alt="图片风格化效果" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/290.jpg" alt="CNN在不同层上风格和内容重构的表现" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/291.jpg" alt="示例中的风格图片和内容图片" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/292.jpg" alt="输入的内容图片" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/293.jpg" alt="风格化后的图片" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/294.jpg" alt="风格和内容权重的比例对生成图片效果的影响" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/295.jpg" alt="通过大量图片训练得到“风格网络”，从而对输入图片进行快速预测的方法" /></p><h2 id="图像识别算法的部署模式">图像识别算法的部署模式</h2><h3 id="图像算法部署模式介绍">图像算法部署模式介绍</h3><p>基于公共云云计算的计算机集群</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/296.jpg" alt="阿里巴巴云计算公司提供的人脸识别服务" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/297.jpg" alt="百度云计算公司提供的图像审核服务" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/298.jpg" alt="腾讯云计算公司提供的图像文字识别OCR服务" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/299.jpg" alt="AWS云计算公司提供的图像识别服务" /></p><p>基于私有云云计算的计算机集群</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/300.jpg" alt="图像识别算法基于云计算架构的系统架构" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/301.jpg" alt="图像识别算法基于私有云容器的架构" /></p><p>X86架构单机+备份模式</p><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/302.jpg" alt="算法文件封装" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/303.jpg" alt="图像识别算法基于普通X86服务器的部署架构" /></p><h3 id="实际应用场景和部署模式的匹配">实际应用场景和部署模式的匹配</h3><p><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/304.jpg" alt="百度云、阿里云、腾讯云人脸属性识别公有云服务测试响应速度表" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/305.jpg" alt="百度云计算公司在其公有云上提供的图像相关服务" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/306.jpg" alt="阿里云计算公司在其公有云上提供的图像相关服务" /><br /><img src="https://2020.iosdevlog.com/2020/02/27/9787111630036/307.jpg" alt="腾讯云计算公司在其公有云上提供的图像相关服务" /></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/27/9787111630036/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《深度学习与图像识别：原理与实践》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：深度学习与图像识别：原理与实践&lt;br /&gt;
作者：魏溪含，涂铭，张修鹏&lt;br /&gt;
出版社：机械工业出版社&lt;br /&gt;
出版时间：2019-06&lt;br /&gt;
ISBN：9787111630036&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="DL" scheme="https://2020.iosdevlog.com/tags/DL/"/>
    
      <category term="CV" scheme="https://2020.iosdevlog.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>《深度学习之图像识别：核心技术与案例实战》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/02/26/9787111624721/"/>
    <id>https://2020.iosdevlog.com/2020/02/26/9787111624721/</id>
    <published>2020-02-26T11:41:38.000Z</published>
    <updated>2020-03-02T12:22:48.947Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/1.jpg" alt="" /><figcaption>《深度学习之图像识别：核心技术与案例实战》</figcaption></figure><p>书名：深度学习之图像识别：核心技术与案例实战<br />作者：言有三<br />出版社：机械工业出版社<br />出版时间：2019-04<br />ISBN：9787111624721</p><a id="more"></a><h2 id="本书读者对象">本书读者对象</h2><ul><li>图像处理技术人员</li><li>深度学习技术人员</li><li>深度学习技术爱好者</li><li>深度学习技术研究人员</li><li>相关院校的学生和老师</li><li>相关培训机构的学生和老师</li></ul><h2 id="介绍">介绍</h2><p>配书资源：<a href="http://www.hzcourse.com/web/refbook/detail/8278/226" target="_blank" rel="noopener" class="uri">http://www.hzcourse.com/web/refbook/detail/8278/226</a></p><p>机器学习、深度学习、人工智能，这些关键词在最近几年“声名鹊起”。以深度学习为代表的无监督机器学习技术在图像处理、语音识别和自然语言处理等领域里频频取得新的突破。但深度学习其实并不是一门全新的学科，其历史可以追溯到20世纪40年代。</p><p>深度学习背后的核心技术包括神经网络的结构设计和最优化方法等，其理论体系虽然有一定进展但是尚不完备。可以说，当前的主流深度学习技术是一门应用性极强的工程技术，这种尚不完备的理论加上具有较高门槛的应用工程特点，对于初学者来说具有一定的困难。如何系统性地了解理论知识，又能够紧随理论进行全面的实践，成为一名合格的图像处理领域的深度学习算法工程师，这是本书所要解决的问题。</p><p>本书内容由浅入深，讲解图文并茂，紧随工业界和学术界的最新发展，理论和实践紧密结合，给出了大量的图表与案例分析。本书抛开了过多的数学理论，完整地剖析了深度学习在图像处理领域中各个维度的重要技术，而不是只停留于理论的阐述和简单的结果展示，更是从夯实理论到完成实战一气呵成。相信读者跟随着本书进行学习，将会对深度学习领域的图像处理技术和其在实际开发中的应用有更深的理解。</p><h2 id="本书特色">本书特色</h2><ol type="1"><li>内容全面，紧跟最新技术<br />发展本书内容涵盖了深度学习的理论知识、数据获取与增强，以及深度学习在图像分类、分割和检测这三大基础研究领域中的发展、数据与模型的可视化、优化目标、模型压缩与部署等相关知识，基本上囊括了深度学习在图像开发中所必须要掌握的大部分基础知识。</li><li>深度与广度兼具<br />本书在讲解每个知识点时力求详尽，而且紧密结合了学术界与工业界相关技术的最新发展。这样的安排既注重知识的广度，也兼具知识的深度，可以为图像处理领域中的从业者提供系统性的学习指导。</li><li>理论与实践案例紧密结合<br />本书不仅对理论知识进行了阐述，而且还给出了大量的实践内容，以帮助读者提高实际的动手能力。除了第 1、2章 主要介绍了深度学习的基础理论外，后续章节则大多采用了先系统介绍该章涉及知识的发展现状，然后有针对性地设计了一到两个实践案例带领读者学习，有较好的学习效果。</li><li>参考了不同层次学习者的意见<br />本书若干内容的简化版本已在笔者运营的公众号平台上接受了不同层次读者的反馈，力求知识的完备性和准确性；另外，本书有多位编写者参与，他们或理论见长，或善于动手，让本书从不同层面得到了广泛的意见，可以满足不同人群的学习需求。</li></ol><h2 id="本书内容">本书内容</h2><p>第1章 神经网络基础</p><p>首先介绍了神经网络的生物基础与数学模型，然后介绍了卷积神经网络的基础知识，这也是当前深度学习模型的基础。</p><p>第2章 深度学习优化基础</p><p>首先介绍了深度学习主流开源框架，如Caffe、TensorFlow、Pytorch和Theano等，并对其特点与性能做了对比；然后介绍了网络优化参数，包括激活函数、正则化方法和归一化方法等。本章旨在让大家对深度卷积神经网络有一个较为全面的认识，给后续章节的学习打好基础。</p><p>第3章 深度学习中的数据</p><p>首先介绍了深度学习发展过程中的几个数据集，给读者展示了数据集对深度学习的重要性；然后介绍了几大重要发展方向中的数据集；接着讲述了数据增强的方法；最后讲述了数据的收集、整理及标注等相关问题。</p><p>第4章 图像分类</p><p>首先介绍了图像分类的基础、基于深度学习的图像分类研究方法及发展现状，以及图像分类任务中的挑战；然后以一个移动端的基准模型为例，展示了图像分类任务的实践流程；最后介绍了一个细粒度级别的图像分类任务，以一个较高的基准模型，展示了较难的图像分类任务训练参数的调试技巧。</p><p>第5章 图像分割</p><p>首先介绍了从阈值法到活动轮廓模型的传统图像分割方法；然后介绍了基于深度学习的图像分割方法的基本原理与核心技术；接着讲述了一个移动端的实时图像分割任务，该任务以MobileNet为基准模型，展示了图像硬分割任务实践的完整流程；最后介绍了一个更加复杂的肖像换背景的任务，展示了图像软分割任务的基本流程和应用场景。</p><p>第6章 目标检测</p><p>首先介绍了目标检测的基础和基本流程，并讲述了一个经典的V-J目标检测框架；然后介绍了基于深度学习的目标检测任务的研究方法与发展现状，并分析了目标检测中的核心技术；最后给出了一个目标检测任务实例，通过分析faster rcnn的源代码，使用该框架自带的VGG CNN 1024网络完成训练和测试，并总结目标检测中的难点。</p><p>第7章 数据与模型可视化</p><p>首先对包括低维与高维数据的可视化做了简单介绍；然后对深度学习中的模型可视化做了详细介绍，包括模型的结构和权重可视化；最后介绍了一个基于Tensorflow和Tensorboard的完整案例。</p><p>第8章 模型压缩</p><p>首先详细介绍了模型压缩的方法，然后以一个典型的模型压缩实战案例来阐述项目中的模型压缩上线。</p><p>第9章 损失函数</p><p>首先介绍了分类任务的损失函数；然后介绍了回归任务的损失函数；最后介绍了这些损失函数在几大经典图像任务中的使用。</p><p>第10章 模型部署与上线</p><p>依托微信小程序平台从3个方面介绍了模型部署的问题。首先介绍了微信小程序的前端开发，然后介绍了微信小程序的服务端开发，最后介绍了Caffe的环境配置。</p><h2 id="第1章-神经网络基础">第1章 神经网络基础</h2><p>神经元</p><p>神经元人工神经网络（Artificial Neural Network）简称神经网络（Neural Network,NN），是人类模拟生物神经网络的结构和功能提出的数学模型，广泛应用于计算机视觉等领域。</p><p>感知机</p><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/2.jpg" alt="" /><figcaption>感知机（Perceptron）</figcaption></figure><p>假设我们有一个 <span class="math inline">\(n\)</span> 维输入的单层感知机，<span class="math inline">\(x_{1}\)</span> 至 <span class="math inline">\(x_{n}\)</span> 为 <span class="math inline">\(n\)</span> 维输入向量的各个分量，<span class="math inline">\(w_{1}\)</span> 至 <span class="math inline">\(w_{n}\)</span> 为各个输入分量连接到感知机的权量（或称权值）, <span class="math inline">\(θ\)</span> 为阈值，<span class="math inline">\(f\)</span> 为激活函数（又称为激励函数或传递函数）,<span class="math inline">\(y\)</span> 为标量输出。理想的激活函数 <span class="math inline">\(f(·)\)</span> 通常为阶跃函数或者 <span class="math inline">\(sigmoid\)</span> 函数。感知机的输出是输入向量 <span class="math inline">\(X\)</span> 与权重向量 <span class="math inline">\(W\)</span> 求得内积后，经激活函数 <span class="math inline">\(f\)</span> 所得到的标量：</p><p><span class="math display">\[y=f\left(\sum_{i=1}^{n} w_{i} x_{i}-\theta\right)\]</span></p><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/3.jpg" alt="" /><figcaption>公式</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">n = <span class="number">0</span>  <span class="comment"># 迭代次数</span></span><br><span class="line">lr = <span class="number">0.10</span>  <span class="comment"># 学习速率</span></span><br><span class="line"><span class="comment"># 输入数据</span></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="comment"># 标签</span></span><br><span class="line">Y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>])</span><br><span class="line"><span class="comment"># 权重初始化，取值范围-1到1</span></span><br><span class="line">W = (np.random.random(X.shape[<span class="number">1</span>])<span class="number">-0.5</span>)*<span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_show</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 正样本</span></span><br><span class="line">    all_x = X[:, <span class="number">2</span>]</span><br><span class="line">    all_y = X[:, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 负样本</span></span><br><span class="line">    all_negative_x = [<span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">    all_negative_y = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算分界线斜率与截距</span></span><br><span class="line">    k = -W[<span class="number">2</span>] / W[<span class="number">3</span>]</span><br><span class="line">    b = -(W[<span class="number">0</span>] + W[<span class="number">1</span>]) / W[<span class="number">3</span>]</span><br><span class="line">    <span class="comment"># 生成x刻度</span></span><br><span class="line">    xdata = np.linspace(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(xdata, xdata*k+b, <span class="string">'r'</span>)</span><br><span class="line">    plt.plot(all_x, all_y, <span class="string">'bo'</span>)</span><br><span class="line">    plt.plot(all_negative_x, all_negative_y, <span class="string">'yo'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新权值函数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_update</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 定义所有全局变量</span></span><br><span class="line">    <span class="keyword">global</span> X, Y, W, lr, n</span><br><span class="line">    n += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 计算符号函数输出</span></span><br><span class="line">    new_output = np.sign(np.dot(X, W.T))</span><br><span class="line">    <span class="comment"># 更新权重</span></span><br><span class="line">    new_W = W + lr*((Y-new_output.T).dot(X))/int(X.shape[<span class="number">0</span>])</span><br><span class="line">    W = new_W</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        get_update()</span><br><span class="line">        new_output = np.sign(np.dot(X, W.T))</span><br><span class="line">        <span class="keyword">if</span> (new_output == Y.T).all():</span><br><span class="line">            print(<span class="string">"迭代次数："</span>, n)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    get_show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/4.jpg" alt="" /><figcaption>感知机模型分割结果示意图</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/5.jpg" alt="" /><figcaption>单隐层前馈网络示意图</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/6.jpg" alt="" /><figcaption>多层前馈网络示意图</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/7.png" alt="" /><figcaption>BP算法公式</figcaption></figure><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/8.png" alt="BP算法公式" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/9.png" alt="9" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/10.png" alt="10" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/11.jpg" alt="11" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/12.jpg" alt="12" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/13.jpg" alt="13" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/14.png" alt="14" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/15.jpg" alt="15" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/16.jpg" alt="16" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/17.jpg" alt="17" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/18.png" alt="18" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/19.png" alt="19" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/20.jpg" alt="20" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/21.png" alt="21" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/22.jpg" alt="22" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/23.jpg" alt="23" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/24.jpg" alt="24" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/25.jpg" alt="25" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/26.jpg" alt="26" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/27.jpg" alt="27" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/28.jpg" alt="28" /></p><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/29.jpg" alt="" /><figcaption>感知机模型解决NOR问题结果示意图</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/30.jpg" alt="" /><figcaption>不同风格的手写字符示意图</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/31.jpg" alt="" /><figcaption>31</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/32.jpg" alt="" /><figcaption>32</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/33.jpg" alt="" /><figcaption>33</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/34.jpg" alt="" /><figcaption>二维卷积的实例</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/35.jpg" alt="" /><figcaption>感受野示例图</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/36.jpg" alt="" /><figcaption>最大池化操作示意图</figcaption></figure><h2 id="第2章-深度学习优化基础">第2章 深度学习优化基础</h2><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/37.jpg" alt="深度学习主流框架参数对比" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/38.jpg" alt="Sigmoid" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/39.jpg" alt="tanh" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/40.jpg" alt="tanh函数示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/41.jpg" alt="ReLU" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/42.jpg" alt="ReLU函数示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/43.jpg" alt="Leaky ReLU" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/44.jpg" alt="Leaky ReLU函数示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/45.jpg" alt="指数线性单元ELU" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/46.jpg" alt="SELU" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/47.jpg" alt="Softmax" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/48.jpg" alt="48" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/49.png" alt="49" /></p><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/50.png" alt="50" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/51.jpg" alt="51" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/52.jpg" alt="基于动量项的SGD示意图" /></p><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/53.jpg" alt="Nesterov加速梯度下降法（Nesterov Accelerated Gradient, NAG）" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/54.jpg" alt="NAG" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/55.jpg" alt="Adagrad" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/56.jpg" alt="Adagrad" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/57.jpg" alt="57" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/58.jpg" alt="58" /></p><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/59.jpg" alt="Adam" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/60.jpg" alt="60" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/61.jpg" alt="61" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/62.jpg" alt="62" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/63.jpg" alt="63" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/64.jpg" alt="64" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/65.jpg" alt="65" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/66.jpg" alt="66" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/67.png" alt="67" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/68.png" alt="68" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/69.png" alt="69" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/70.jpg" alt="70" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/71.jpg" alt="71" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/72.jpg" alt="72" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/73.jpg" alt="73" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/74.jpg" alt="74" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/75.jpg" alt="75" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/76.jpg" alt="76" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/77.jpg" alt="77" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/78.jpg" alt="78" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/79.jpg" alt="79" /></p><h2 id="第3章-深度学习中的数据">第3章 深度学习中的数据</h2><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/80.jpg" alt="人脸检测结果" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/81.jpg" alt="人脸检测提取出的脸部和关键点信息" /></p><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/82.png" /></p><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/83.jpg" alt="原图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/84.jpg" alt="数据增强示意图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/85.jpg" alt="水平翻转和垂直翻转图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/86.jpg" alt="随机旋转图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/87.jpg" alt="随机裁剪图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/88.jpg" alt="竖直方向与水平方向缩放变形图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/89.jpg" alt="原图与添加随机高斯噪声图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/90.jpg" alt="Coarse Dropout添加噪声图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/91.jpg" alt="随机擦除法添加噪声图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/92.jpg" alt="不同程度的高斯模糊图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/93.jpg" alt="颜色扰动图" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/94.jpg" alt="SMOTE方法" /></p><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/95.jpg" alt="mixup方法" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/96.jpg" alt="生成对抗网络GAN" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/97.jpg" alt="DCGAN生成的嘟嘴嘴唇样本图" /></p><h2 id="第4章-图像分类">第4章 图像分类</h2><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/98.jpg" alt="LeNet5网络结构" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/99.jpg" alt="单标签分类" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/100.jpg" alt="100" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/101.jpg" alt="101" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/102.jpg" alt="多标签分类 平均分类准确度" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/103.jpg" alt="汉明距离" /></p><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/104.jpg" alt="原图（左）, laplacian of gaussian边缘检测结果（右）" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/105.jpg" alt="不同表情的图片" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/106.jpg" alt="百度AI小程序识别结果" /></p><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/107.jpg" alt="" /><figcaption>表情样本</figcaption></figure><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/108.jpg" alt="MobileNet基本组成单元Depthwise Separable Convolutions" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/109.jpg" alt="训练损失" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/110.jpg" alt="训练精度" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/111.jpg" alt="双线性网络结构" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/112.jpg" alt="共享部分权重和共享全部权重" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/113.jpg" alt="双线性部分网络示意图" /></p><figure><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/114.jpg" alt="" /><figcaption>Signed_sqrt_layer是进行带符号的归一化操作</figcaption></figure><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/115.jpg" alt="准确率曲线" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/116.jpg" alt="损失曲线" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/117.jpg" alt="损失曲线" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/118.jpg" alt="准确率曲线" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/119.jpg" alt="损失曲线" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/120.jpg" alt="精度曲线" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/121.jpg" alt="测试不同batch size的对精度的影响" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/122.jpg" alt="测试不同分辨率的影响" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/123.jpg" alt="测试不同正则化因子的影响" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/124.jpg" alt="损失曲线" /></p><h2 id="第5章-图像分割">第5章 图像分割</h2><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/125.png" alt="125" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/126.png" alt="126" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/127.png" alt="127" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/128.png" alt="128" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/129.jpg" alt="129" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/130.jpg" alt="130" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/131.jpg" alt="131" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/132.jpg" alt="132" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/133.jpg" alt="133" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/134.jpg" alt="134" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/135.jpg" alt="135" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/136.jpg" alt="136" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/137.jpg" alt="137" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/138.jpg" alt="138" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/139.jpg" alt="139" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/140.jpg" alt="140" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/141.jpg" alt="141" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/142.jpg" alt="142" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/143.jpg" alt="143" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/144.jpg" alt="144" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/145.jpg" alt="145" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/146.jpg" alt="146" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/147.jpg" alt="147" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/148.jpg" alt="148" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/149.jpg" alt="149" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/150.jpg" alt="150" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/151.jpg" alt="151" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/152.jpg" alt="152" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/153.jpg" alt="153" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/154.jpg" alt="154" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/155.jpg" alt="155" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/156.jpg" alt="156" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/157.jpg" alt="157" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/158.jpg" alt="158" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/159.jpg" alt="159" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/160.jpg" alt="160" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/161.jpg" alt="161" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/162.jpg" alt="162" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/163.jpg" alt="163" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/164.jpg" alt="164" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/165.jpg" alt="165" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/166.jpg" alt="166" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/167.jpg" alt="167" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/168.jpg" alt="168" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/169.jpg" alt="169" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/170.jpg" alt="170" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/171.jpg" alt="171" /></p><h2 id="第6章-目标检测">第6章 目标检测</h2><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/172.jpg" alt="172" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/173.jpg" alt="173" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/174.jpg" alt="174" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/175.jpg" alt="175" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/176.jpg" alt="176" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/177.jpg" alt="177" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/178.jpg" alt="178" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/179.jpg" alt="179" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/180.jpg" alt="180" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/181.jpg" alt="181" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/182.jpg" alt="182" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/183.jpg" alt="183" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/184.jpg" alt="184" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/185.jpg" alt="185" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/186.jpg" alt="186" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/187.jpg" alt="187" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/188.jpg" alt="188" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/189.jpg" alt="189" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/190.jpg" alt="190" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/191.jpg" alt="191" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/192.jpg" alt="192" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/193.jpg" alt="193" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/194.jpg" alt="194" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/195.jpg" alt="195" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/196.jpg" alt="196" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/197.jpg" alt="197" /></p><h2 id="第7章-数据与模型可视化">第7章 数据与模型可视化</h2><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/198.jpg" alt="198" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/199.jpg" alt="199" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/200.jpg" alt="200" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/201.jpg" alt="201" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/202.jpg" alt="202" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/203.jpg" alt="203" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/204.jpg" alt="204" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/205.jpg" alt="205" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/206.jpg" alt="206" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/207.jpg" alt="207" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/208.jpg" alt="208" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/209.jpg" alt="209" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/210.jpg" alt="210" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/211.jpg" alt="211" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/212.jpg" alt="212" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/213.jpg" alt="213" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/214.jpg" alt="214" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/215.jpg" alt="215" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/216.jpg" alt="216" /></p><h2 id="第8章-模型压缩">第8章 模型压缩</h2><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/217.jpg" alt="217" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/218.jpg" alt="218" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/219.jpg" alt="219" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/220.png" alt="220" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/221.jpg" alt="221" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/222.jpg" alt="222" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/223.png" alt="223" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/224.jpg" alt="224" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/225.jpg" alt="225" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/226.jpg" alt="226" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/227.jpg" alt="227" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/228.jpg" alt="228" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/229.jpg" alt="229" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/230.jpg" alt="230" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/231.jpg" alt="231" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/232.jpg" alt="232" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/233.jpg" alt="233" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/234.jpg" alt="234" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/235.jpg" alt="235" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/236.jpg" alt="236" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/237.jpg" alt="237" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/238.jpg" alt="238" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/239.jpg" alt="239" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/240.jpg" alt="240" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/241.jpg" alt="241" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/242.jpg" alt="242" /></p><h2 id="第9章-损失函数">第9章 损失函数</h2><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/243.jpg" alt="243" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/244.jpg" alt="244" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/245.jpg" alt="245" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/246.jpg" alt="246" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/247.jpg" alt="247" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/248.jpg" alt="248" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/249.jpg" alt="249" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/250.jpg" alt="250" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/251.jpg" alt="251" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/252.jpg" alt="252" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/253.jpg" alt="253" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/254.jpg" alt="254" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/255.png" alt="255" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/256.png" alt="256" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/257.jpg" alt="257" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/258.jpg" alt="258" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/259.jpg" alt="259" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/260.jpg" alt="260" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/261.jpg" alt="261" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/262.jpg" alt="262" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/263.jpg" alt="263" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/264.jpg" alt="264" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/265.jpg" alt="265" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/266.jpg" alt="266" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/267.jpg" alt="267" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/268.jpg" alt="268" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/269.jpg" alt="269" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/270.jpg" alt="270" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/271.jpg" alt="271" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/272.jpg" alt="272" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/273.jpg" alt="273" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/274.png" alt="274" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/275.jpg" alt="275" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/276.jpg" alt="276" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/277.jpg" alt="277" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/278.jpg" alt="278" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/279.jpg" alt="279" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/280.jpg" alt="280" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/281.jpg" alt="281" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/282.jpg" alt="282" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/283.jpg" alt="283" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/284.jpg" alt="284" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/285.jpg" alt="285" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/286.jpg" alt="286" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/287.jpg" alt="287" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/288.jpg" alt="288" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/289.png" alt="289" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/290.png" alt="290" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/291.jpg" alt="291" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/292.jpg" alt="292" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/293.jpg" alt="293" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/294.jpg" alt="294" /></p><h2 id="第10章-模型部署与上线">第10章 模型部署与上线</h2><p><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/295.jpg" alt="295" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/296.jpg" alt="296" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/297.jpg" alt="297" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/298.jpg" alt="298" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/299.jpg" alt="299" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/300.jpg" alt="300" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/301.jpg" alt="301" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/302.jpg" alt="302" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/303.jpg" alt="303" /><br /><img src="https://2020.iosdevlog.com/2020/02/26/9787111624721/304.jpg" alt="304" /></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/26/9787111624721/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《深度学习之图像识别：核心技术与案例实战》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：深度学习之图像识别：核心技术与案例实战&lt;br /&gt;
作者：言有三&lt;br /&gt;
出版社：机械工业出版社&lt;br /&gt;
出版时间：2019-04&lt;br /&gt;
ISBN：9787111624721&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="DL" scheme="https://2020.iosdevlog.com/tags/DL/"/>
    
      <category term="CV" scheme="https://2020.iosdevlog.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>AI 及 NLP 基础</title>
    <link href="https://2020.iosdevlog.com/2020/02/25/nlp/"/>
    <id>https://2020.iosdevlog.com/2020/02/25/nlp/</id>
    <published>2020-02-25T10:30:44.000Z</published>
    <updated>2020-02-25T15:22:05.446Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/0.png" alt="" /><figcaption>AI</figcaption></figure><a id="more"></a><h3 id="ai-概览宣传片外的人工智能">AI 概览：宣传片外的人工智能</h3><p>内容概述</p><ul><li><p>人工智能是什么</p></li><li><p>人工智能的发展历史</p></li><li><p>人工智能的现状和展望</p></li></ul><p>人工智能之父？</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000105.jpg" alt="" /><figcaption>Fallece John McCathy</figcaption></figure><p>Fallece John McCathy</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000178.jpg" alt="" /><figcaption>达特茅斯参会者合影</figcaption></figure><p>达特茅斯参会者合影</p><p>人工智能 Artificial Intelligence</p><p>人工智能的复兴</p><ul><li><p>人工智能的复兴 ≈ 神经网络的复兴</p></li><li><p>神经网络的概念很早就已经出现，但是在 2000 年左右，由于 SVM 的出现，</p></li></ul><p>使得神经网络没落了一段时间。</p><ul><li>神经网络的再次兴起，主要源于 2013 年深度学习的出现。</li></ul><p>人工智能的本质是什么</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000102.jpg" alt="" /><figcaption>媒体眼中的人工智能</figcaption></figure><p>媒体眼中的人工智能</p><p>我们要讲到的人工智能</p><ul><li>预测性建模</li><li>优化问题</li></ul><p>预测性建模</p><ul><li><p>主要目的：在给定数据的基础上提升预测模型的准确率</p></li><li><p>绝大多数人工智能的应用在很大程度上均可以认为是预测性建模的范畴</p></li></ul><p>预测性建模 VS 统计学</p><ul><li><p>预测性建模：只关注模型的预测准确性</p></li><li><p>统计学模型：主要关注模型的可解释性</p></li></ul><p>优化问题</p><ul><li><p>优化问题是指如何根据已有的数学模型求出最优解的过程</p></li><li><p>近年来取得主要进展的优化问题主要在于增强学习领域</p></li></ul><p>增强学习</p><ul><li><p>解决的问题：如何在动态环境下做出正确的决定</p></li><li><p>主要应用：游戏 AI</p></li><li><p>其他应用：内容推荐、搜索排名、智能化定价</p></li></ul><h3 id="人工智能的另外一种分类">人工智能的另外一种分类</h3><ul><li><p>结构化数据</p></li><li><p>文本数据</p></li><li><p>图像、视频数据</p></li><li><p>语音数据</p></li></ul><p>被媒体夸大的人工智能</p><p>人工智能的真实发展</p><p>最小二乘法和牛顿法</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000100.jpg" alt="" /><figcaption>牛顿</figcaption></figure><p>牛顿</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000065.jpg" alt="" /><figcaption>高斯</figcaption></figure><p>高斯</p><h3 id="人工智能常常以不同面目出现">人工智能常常以不同面目出现</h3><p>机器智能</p><p>统计学</p><p>……</p><p>决策智能</p><p>运筹学</p><p>人工智能</p><p>深度学习</p><p>数据挖掘</p><p>模式识别</p><p>神经网络</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/0.png" alt="" /><figcaption>人工智能</figcaption></figure><h3 id="人工智能的现状">人工智能的现状</h3><p>学术</p><table><thead><tr class="header"><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>很多领域的准确性可以替代人类</td><td>可以替代人类准确性的只占很少部分</td></tr><tr class="even"><td>一些领域可以做到击败人类最好的决策者</td><td>这没有实际应用；并且要耗费很多的资源</td></tr><tr class="odd"><td>发展极为迅速</td><td>很多文章复现性很差</td></tr><tr class="even"><td>覆盖领域越来越多</td><td>很多领域都是哗众取宠</td></tr><tr class="odd"><td>所需要标注样本越来越少</td><td>标注仍然劳民伤财</td></tr><tr class="even"><td>开源代码越来越多</td><td>大部分人还是只会调包</td></tr><tr class="odd"><td>……</td><td>……</td></tr></tbody></table><p>业界</p><table><thead><tr class="header"><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>大量的公司涌现</td><td>实干企业较少</td></tr><tr class="even"><td>大量的热点出现</td><td>大热之后大扑街</td></tr><tr class="odd"><td>大量的新概念出现</td><td>大部分难以实现落地</td></tr><tr class="even"><td>大量的公司花大价钱投入研发</td><td>形式主义，研究难以实现落地</td></tr><tr class="odd"><td>大量的高薪就业机会</td><td>一旦生意失败，只能卷铺盖走人</td></tr><tr class="even"><td>大量的媒体创造热度</td><td>过于夸张的宣传，引起反感</td></tr><tr class="odd"><td>……</td><td>……</td></tr></tbody></table><h3 id="那我们该怎么做">那我们该怎么做</h3><ul><li>钻<ul><li>将一项任务做到极致</li><li>不要到处转换方向</li></ul></li><li>快<ul><li>快速学习新的领域</li><li>紧追热点，会读英文资料</li></ul></li><li>深<ul><li>夯实基础</li><li>数学和编程</li></ul></li><li>广<ul><li>广泛涉猎</li><li>抓住本质</li></ul></li></ul><h2 id="ai-项目流程">AI 项目流程</h2><p>内容概述</p><ul><li><p>如何判断是否要做一个 AI 项目</p></li><li><p>如何做前期的调研</p></li><li><p>如何做开发的计划</p></li><li><p>如何对结果进行验证</p></li><li><p>如何进行部署</p></li></ul><p>如何判断是否要做一个 AI 项目</p><ol type="1"><li><p>技术的成熟度</p></li><li><p>需求的可控程度</p></li><li><p>项目投入的周期和成本</p></li><li><p>项目最终的交付流程</p></li></ol><p>技术的成熟度</p><ol type="1"><li><p>底线：人工是否可以解决这个问题</p></li><li><p>Paper 中技术的复现性 VS 领先厂商当前的水平</p></li><li><p>初期通过小 Demo 测试准确率</p></li><li><p>团队的时间和能力</p></li><li><p>项目部署问题</p></li><li><p>保守估计项目的交付时间</p></li></ol><p>需求的可控程度</p><ul><li><p>销售导向 OR 技术导向</p></li><li><p>客户管理能力</p></li><li><p>团队整体的需求控制能力</p></li></ul><p>项目投入的周期和成本</p><ul><li><p>大多数时候，人们会低估项目投入的周期和成本。</p></li><li><p>项目周期和成本不可控的原因主要来源于需求的变更。</p></li><li><p>其他可能出现的问题：</p></li><li><p>标注的不可控性</p></li><li><p>模型效果调优所需要的时间</p></li><li><p>推断速度提升所需要的时间</p></li><li><p>环境部署所需要的时间</p></li><li><p>运行模型所需要的算力成本</p></li></ul><p>项目最终的交付流程</p><ul><li><p>明确项目目标</p></li><li><p>不要忽略交付流程中的额外投入</p></li><li><p>组织的项目交付的流水化能力：</p></li><li><p>是否有明确的交付流程</p></li><li><p>人员职责安排是否清晰</p></li><li><p>是否严格遵循时间规范</p></li><li><p>项目是否有烂尾的风险</p></li></ul><p>项目的一般流程</p><ul><li><p>前期调研和方案确定</p></li><li><p>数据标注和开发</p></li><li><p>效果调优（包括准确性和速度）</p></li><li><p>代码部署</p></li></ul><p>前期调研和方案确定</p><p>容易被忽略的问题：</p><ul><li><p>很多时候，学术结果难以复现。</p></li><li><p>很多方法在某些数据上可能会非常好用，但是在另一些数据上则会失效。</p></li><li><p>很多方法的成功取决于一些细节，而这些细节只有真正做过的人才会知道。</p></li><li><p>很多时候人们会过于关注方法的效果，而忽略了整体的运行实效。</p></li><li><p>在绝大多数的时候，人们都会低估整个项目的难度。</p></li></ul><p>数据标注</p><ul><li><p>前期一定要制定充分的标注规则</p></li><li><p>数据的采集一定要具有代表性</p></li><li><p>非常不建议采用自动标注的方式</p></li><li><p>先训练一个初步模型，然后只让相关人员进行校对，可以保证标注效率并减少标注成本</p></li></ul><p>算法开发</p><ul><li><p>千万不要采用规则的方式进行开发</p></li><li><p>初期就要引导客户使用和购买能够支持深度学习框架的硬件</p></li><li><p>算法开发的过程中，一定要有量化的指标并记录下来</p></li><li><p>开发的过程中，多分解问题</p></li><li><p>前端对接的时候一定要去引导何为“智能”</p></li></ul><p>效果优化</p><ul><li><p>初期要充分考虑到效果优化所需要的时间和成本</p></li><li><p>客户并不知道通过什么标准来评估一个系统的好坏</p></li><li><p>一定要从数据的角度出发进行优化</p></li><li><p>学会止损</p></li><li><p>出了准确性的优化，还要注重代码运算效率的优化</p></li><li><p>算法开发和效果优化常常是需要反复进行的工作</p></li></ul><p>算法部署</p><ul><li><p>如果客户的系统比较奇怪，或者难以满足一些要求，要提前让客户知晓这些风险。</p></li><li><p>即使再小的项目，我也强烈建议用微服务架构进行部署。</p></li><li><p>不要把算法部署在本地，尽量采用云端部署。</p></li></ul><h2 id="nlp-基本任务及研究方向">NLP 基本任务及研究方向</h2><p>内容概述</p><ul><li><p>基础性研究</p></li><li><p>专属 NLP 领域的研究</p></li><li><p>交叉领域的研究</p></li></ul><p>基础性研究：网络架构</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000101.png" alt="" /><figcaption>网络架构</figcaption></figure><p>基础性研究：优化理论</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000131.png" alt="" /><figcaption>优化理论</figcaption></figure><p>基础性研究：对抗训练</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000123.png" alt="" /><figcaption>对抗训练</figcaption></figure><p>基础性研究：数据增强</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000045.png" alt="" /><figcaption>数据增强</figcaption></figure><p>基础性研究：半监督学习</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000016.png" alt="" /><figcaption>半监督学习</figcaption></figure><p>基础性研究：域迁移</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000002.png" alt="" /><figcaption>域迁移</figcaption></figure><p>基础性研究：Meta Learning</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000078.png" alt="" /><figcaption>Meta Learning</figcaption></figure><p>基础性研究：Auto ML</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000112.png" alt="" /><figcaption>Auto ML</figcaption></figure><p>基础性研究：多任务学习</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000093.png" alt="" /><figcaption>多任务学习</figcaption></figure><p>基础性研究：集成学习</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000080.png" alt="" /><figcaption>集成学习</figcaption></figure><p>基础性研究：图网络</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000037.png" alt="" /><figcaption>图网络</figcaption></figure><p>基础性研究：知识图谱</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000176.jpg" alt="" /><figcaption>知识图谱</figcaption></figure><p>图片来源： <a href="https://lod-cloud.net" target="_blank" rel="noopener" class="uri">https://lod-cloud.net</a></p><p>基础性研究：多模态学习</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/1.png" alt="" /><figcaption>多模态学习</figcaption></figure><p>基础性研究：机器推理</p><p><img src="https://2020.iosdevlog.com/2020/02/25/nlp/1.png" alt="机器推理" /><br />000158</p><p>NLP 研究：预训练语言模型</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000050.png" alt="" /><figcaption>预训练语言模型</figcaption></figure><p>NLP 研究：文本分类</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000012.jpg" alt="" /><figcaption>文本分类</figcaption></figure><p>NLP 研究：序列标注</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000079.png" alt="" /><figcaption>序列标注</figcaption></figure><p>NLP 研究：关系提取</p><ul><li><p>Queen Elizabeth</p></li><li><p>Prince Charles</p></li><li><p>PER-PARENTS</p></li></ul><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/2.png" alt="" /><figcaption>关系提取</figcaption></figure><p>NLP 研究：Dependency Parsing</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000122.png" alt="" /><figcaption>Dependency Parsing</figcaption></figure><p>NLP 研究：Semantic Parsing</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000001.png" alt="" /><figcaption>Semantic Parsing</figcaption></figure><p>NLP 研究：Seq2Seq</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000117.png" alt="" /><figcaption>Seq2Seq</figcaption></figure><p>NLP 研究：文本生成</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000092.png" alt="" /><figcaption>文本生成</figcaption></figure><p>NLP 研究：文本推荐</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000159.png" alt="" /><figcaption>文本推荐</figcaption></figure><p>NLP 研究：翻译</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000064.png" alt="" /><figcaption>翻译</figcaption></figure><p>NLP 研究：指代消解</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000007.png" alt="" /><figcaption>指代消解</figcaption></figure><p>NLP 综合性研究</p><ul><li><p>智能对话机器人</p></li><li><p>文本校对</p></li><li><p>文本检索</p></li><li><p>开源情报系统</p></li><li><p>Smart BI</p></li></ul><h2 id="nlp-应用智能问答系统">NLP 应用：智能问答系统</h2><p>内容概述</p><ul><li><p>智能问答系统产品简介</p></li><li><p>如何构建智能问答产品</p></li><li><p>智能问答产品的一些挑战</p></li></ul><p>目的</p><ul><li><p>A Taste of Reality</p></li><li><p>很多系统是由多个组件组成的</p></li><li><p>很多系统是存在很多挑战的</p></li><li><p>很多系统落地是存在问题的</p></li><li><p><strong>但是，很多时候有些问题也是可以解决的…</strong></p></li></ul><p>一些智能问答产品</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000157.jpg" alt="" /><figcaption>Google Assistant</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000146.jpg" alt="" /><figcaption>amazon alexa</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000006.png" alt="" /><figcaption>Hey Cortana</figcaption></figure><p>如何构建一个智能问答系统</p><p>智能对话系统的挑战</p><ul><li><p>技术困难</p></li><li><p>投入巨大</p></li><li><p>落地困难</p></li></ul><p>如何把一个机器人问成弱智</p><ul><li><p>省略回复</p></li><li><p>知识推理</p></li><li><p>错别字</p></li><li><p>状态切换</p></li><li><p>延续话题</p></li></ul><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000130.jpg" alt="" /><figcaption>机器人问成弱智</figcaption></figure><p>对话机器人的实践</p><ul><li><p>我曾经生不如死地开发过一个对话机器人（类似于百度 UNIT ）</p></li><li><p>挑战：</p></li><li><p>技术复杂（次要）</p></li><li><p>客户关系（主要）</p></li></ul><p>总结</p><ul><li><p>目标分解</p></li><li><p>创新并非那么困难</p></li><li><p>综合考虑技术因素和人的因素</p></li></ul><h2 id="nlp-应用文本校对系统">NLP 应用：文本校对系统</h2><p>从 Grammarly 出发</p><p>理想很丰满，现实很骨感</p><ul><li><p>理论模型：Seq2Seq</p></li><li><p>实际模型：</p></li><li><p>？</p></li><li><p>？？</p></li><li><p>？？？</p></li></ul><p>为什么中文校对会比英文校对要难？</p><ul><li>本质：一旦错误，传统的模型会崩溃</li></ul><p>如何解决</p><ul><li><p>目标分解</p></li><li><p>逆向思维</p></li></ul><p>其他可以尝试的</p><ul><li><p>语法错误校对</p></li><li><p>常识校对</p></li></ul><h2 id="nlp-的学习方法如何在-ai-爆炸时代快速上手学习">NLP 的学习方法：如何在 AI 爆炸时代快速上手学习？</h2><p>AI 时代的学习</p><ul><li><p>为什么要学习</p></li><li><p>学习的误区</p></li><li><p>如何有效地进行学习</p></li></ul><p>为什么要学习</p><ul><li><p>迁移学习的出现使得技术不再是数据标注的衬托</p></li><li><p>AI 一直在迅速发展</p></li><li><p>AI 本身还不成熟，有大量的创新空间</p></li></ul><p>学习上的误区</p><ul><li><p>“大佬（同事）带带我”；</p></li><li><p>夯实基础，拿下西瓜书；</p></li><li><p>一切要从结果出发，要务实；</p></li><li><p>创新是给巨佬的，跟我没关系；</p></li><li><p>不要造轮子；</p></li><li><p>三个月内从零到 Kaggle Master；</p></li><li><p>Andrew Ng 说一切本质一定是过拟合和欠拟合；</p></li><li><p>看英文太费劲，国内有很多公众号不错；</p></li></ul><p>学习路线建议</p><ul><li><p>基础 = 数学 + 编程 +（英语）</p></li><li><p>积极寻找对 AI 有情怀的人</p></li><li><p>上来就是干</p></li><li><p>考虑其它维度</p></li><li><p>兼听则明，AI 届没有上帝</p></li><li><p>怀疑一切</p></li><li><p>人们将如此多的时间花在走捷径之上，以至于正常走远路的人反倒是首先到终点的</p></li><li><p>做深一点，扩展多点</p></li></ul><h2 id="深度学习框架简介如何选择合适的深度学习框架">深度学习框架简介：如何选择合适的深度学习框架？</h2><p>内容概述</p><ul><li><p>深度学习框架包括什么</p></li><li><p>选择深度学习框架的准则</p></li><li><p>TensorFlow 和 PyTorch 简介</p></li></ul><p>深度学习框架包括什么</p><ul><li><p>GPU 为基础的 Tensor 运算</p></li><li><p>构建网络后自动求解梯度的方法</p></li><li><p>模型训练体系</p></li><li><p>模型推断体系</p></li></ul><p>选择深度学习框架的准则</p><ul><li><p>生态圈</p></li><li><p>易用性（不要光看 Demo 来判断）</p></li><li><p>功能是否完整</p></li><li><p>API 是否稳定</p></li><li><p>效率</p></li></ul><h3 id="tensorflow">TensorFlow</h3><table><thead><tr class="header"><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>谷歌爸爸一撑腰，研究代码两丰收</td><td>API 不稳定</td></tr><tr class="even"><td>新版 TensorFlow API 较简洁</td><td>学习成本高</td></tr><tr class="odd"><td>天生和谷歌云兼容</td><td>开发成本高</td></tr><tr class="even"><td>有良好的推断支持</td><td></td></tr><tr class="odd"><td>功能十分强大</td><td></td></tr></tbody></table><p>PyTorch</p><table><thead><tr class="header"><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>上手容易</td><td>没有 Keras API 那样简洁</td></tr><tr class="even"><td>代码简洁</td><td>一些功能比较难以实现</td></tr><tr class="odd"><td>发展快速，现在已经支持 TPU</td><td></td></tr><tr class="even"><td>API 相对稳定</td><td></td></tr></tbody></table><h3 id="深度学习与硬件cpu-篇">深度学习与硬件：CPU 篇</h3><p>内容概述</p><ul><li><p>为何关注深度学习硬件</p></li><li><p>CPU 硬件基础</p></li><li><p>CPU 与深度学习</p></li></ul><p>为何关注硬件</p><ul><li><p>关注硬件不等于所有都要重新写</p></li><li><p>加速训练</p></li><li><p>避免部署出现问题</p></li></ul><p>CPU 基础架构（白板演示）</p><p>CPU 在训练时候的注意事项</p><ul><li><p>一般不用 CPU 训练深度学习模型。</p></li><li><p>很多 if…else 出现时，CPU 会比 GPU 快。</p></li><li><p>如果需要加速，可以通过 Cython 访问 C++，这在实际业务性应用时很有用。</p></li><li><p>对于大部分硬件（GPU，TPU，FPGA），CPU会负责数据的读写 -&gt; 在进行训练时，</p></li></ul><p>有时为了加速需要选择更多核的机器。</p><p>CPU 在部署时候的注意事项</p><ul><li><p>避免 Cache Miss</p></li><li><p>有时需要使用足够多的核来支持读写</p></li></ul><h3 id="深度学习与硬件gpu-篇">深度学习与硬件：GPU 篇</h3><p>内容概述</p><ul><li><p>GPU 产品举例</p></li><li><p>GPU 硬件特点</p></li><li><p>GPU 与深度学习</p></li></ul><p>GPU 的主要厂商</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000145.png" alt="" /><figcaption>Nvidia</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/amd.svg" alt="" /><figcaption>AMD</figcaption></figure><p>一些 Nvidia 的产品</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000034.png" alt="" /><figcaption>Nvidia V100</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000172.jpg" alt="" /><figcaption>Nvidia P1000</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000167.jpg" alt="" /><figcaption>Nvidia P1000</figcaption></figure><p>Nvidia P1000</p><p>GPU 硬件的其他特点</p><ul><li><p>显存独立于内存，内存和显存的读取可能会成为问题。</p></li><li><p>对于显存的处理，multi-stream processer 并不如 CPU 一样强大。</p></li><li><p>GPU 是非常复杂的处理器。</p></li></ul><p>GPU 训练注意事项</p><ul><li><p>GPU 训练效率可以被 DeepSpeed 显著提升。</p></li><li><p>很少出现 GPU 多线程训练。</p></li><li><p>GPU 训练有时可能会被一些其他因素影响，如CPU，GPU 之间沟通速度（多</p></li></ul><p>GPU或多节点）。</p><ul><li>传统来说，NLP 的训练一般来说不会耗尽 GPU的资源，但是深度迁移模型出现后，</li></ul><p>GPU 常常出现算力不足或显存资源不足的情况。</p><ul><li>GPU 可处理动态的网络。</li></ul><p>GPU 部署的注意事项</p><ul><li><p>GPU 部署的最大问题：显存污染。</p></li><li><p>GPU 部署经常被内存与显存之间的带宽影响。</p></li><li><p>部署时需要对参数做详细调整，最重要参数为 Batch Size。</p></li></ul><h3 id="深度学习与硬件tpu-篇">深度学习与硬件：TPU 篇</h3><p>内容概述</p><ul><li><p>TPU 概述</p></li><li><p>TPU 与深度学习</p></li><li><p>GCP 使用教程</p></li></ul><p>TPU</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000047.png" alt="" /><figcaption>TPU</figcaption></figure><p>TPU 集群</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/000103.png" alt="" /><figcaption>TPU 集群</figcaption></figure><p>TPU 的特点</p><ul><li><p>用于训练神经网络的 TPU 只能通过 GCP 获得</p></li><li><p>TPU 本质上来说是矩阵/向量相乘的机器，构造远比 GPU 简单，所以：</p></li><li><p>TPU 十分便宜</p></li><li><p>TPU 很容易预测其表现</p></li><li><p>TPU 很擅长基于 Transformer 架构的训练和预测</p></li><li><p>TPU 不能处理动态网络</p></li></ul><p>TPU 与深度学习</p><ul><li><p>原生 Tensorflow 对 TPU 支持最好，PyTorch 目前通过 XLA 的引进也部分支持 TPU。</p></li><li><p>TPU 的主要运算瓶颈在于 IO 支持。</p></li><li><p>建议采用 TPU V3 多线程的方式，这种方式性价比最高。</p></li></ul><p>AI 项目部署：基本原则</p><p>内容概述</p><ul><li><p>AI 项目部署的难点</p></li><li><p>AI 项目部署的目标</p></li><li><p>AI 项目部署的基本原则</p></li></ul><p>AI 项目部署的难点</p><ul><li><p>AI 项目整体结构复杂，模块繁多。</p></li><li><p>AI 很多时候需要大量的算力，需要使用 GPU，TPU 或者 FPGA。</p></li><li><p>深度学习框架依赖十分复杂，配置环境困难。</p></li></ul><p>AI 项目部署目标</p><ul><li><p>不要崩，不要崩，不要崩</p></li><li><p>保证不出大的问题</p></li><li><p>保证合适的效率</p></li><li><p>保证尽可能少的侵入性</p></li></ul><p>AI 项目部署基本原则</p><ul><li><p>采用微服务框架（方便、稳定）。</p></li><li><p>采用合适硬件，注意 CPU 选型和 GPU 选型。</p></li><li><p>以 Profiler 为导向进行优化。</p></li><li><p>推断服务应该用同一个框架和一个线程，TPU 除外。</p></li><li><p>部署应该是在项目初期就考虑的，要制定完善的项目计划，并注意和客户的沟通。</p></li></ul><p>AI 项目部署：框架</p><p>内容概述</p><ul><li><p>深度学习推断框架的任务</p></li><li><p>选择深度学习推断框架的主要根据</p></li><li><p>TF Serving 简介</p></li></ul><p>深度学习推断框架的任务</p><ul><li><p>读取模型，提供 REST 接口。</p></li><li><p>调用不同的硬件资源。</p></li><li><p>对推断过程做一定处理，其中最重要的是批处理。</p></li></ul><p>选择深度学习推断框架的主要根据</p><ul><li><p>生态圈</p></li><li><p>易用性和文档完整性</p></li><li><p>对不同硬件的支持程度</p></li><li><p>功能是否强大</p></li><li><p>推断速度</p></li></ul><p>TF Serving 简介</p><p>AI 项目部署：微服务简介</p><p>内容概述</p><ul><li><p>微服务基本介绍</p></li><li><p>为何选择微服务</p></li><li><p>微服务部署 AI 的一些基本原则</p></li></ul><p>微服务基本介绍</p><p>微服务是一个概念，而不是一个严谨的定义</p><p>微服务的主要原件</p><p>Docker</p><p>Kubernetes</p><p>Istio</p><p>为何选择微服务</p><ul><li><p>入侵性小</p></li><li><p>稳定性高</p></li><li><p>功能强大</p></li></ul><p>微服务部署 AI 的一些基本原则</p><ul><li><p>对于推断，一个节点只部署一个 Docker！（TPU 除外）</p></li><li><p>如果没时间，起码选择 Kubernetes 和 Docker，因为 Docker 很容易崩溃。</p></li><li><p>一些其他的考虑：</p></li><li><p>错误恢复</p></li><li><p>灰度上线</p></li><li><p>Kafka</p></li><li><p>Actor</p></li><li><p>其他功能</p></li></ul><p>内容来自《NLP 实战高手课》</p><figure><img src="https://2020.iosdevlog.com/2020/02/25/nlp/nlp.jpg" alt="" /><figcaption>《NLP 实战高手课》</figcaption></figure>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/25/nlp/0.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;AI&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="nlp" scheme="https://2020.iosdevlog.com/tags/nlp/"/>
    
      <category term="geektime" scheme="https://2020.iosdevlog.com/tags/geektime/"/>
    
  </entry>
  
  <entry>
    <title>Coursera 课程免费旁听与下载</title>
    <link href="https://2020.iosdevlog.com/2020/02/24/coursera/"/>
    <id>https://2020.iosdevlog.com/2020/02/24/coursera/</id>
    <published>2020-02-23T18:36:14.000Z</published>
    <updated>2020-02-24T13:58:22.812Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/24/coursera/4.png" alt="" /><figcaption>Coursera 课程</figcaption></figure><a id="more"></a><p>Coursera（/kɔːrsˈɛrə/）是由斯坦福大学的计算机科学教授吴恩达和达芙妮·科勒联合创建的一个营利性的教育科技公司。</p><p>Coursera 与多家大学合作，给大众带来一些在线免费课堂。在Knowledge@Wharton 座谈会上，Daphne Koller 在采访中说道，截至到 2012年11月，Coursera 上有来自 196 国家的超过 190万人。他们至少注册过一门课堂，尽管有数百万人注册过课堂，但完成率仅是 7-9%。</p><p>Coursera 成立于加州山景城，它的启动稍晚于由斯坦福大学教授 Sebastian Thrun 投资的盈利性在线教育网站 Udacity、但稍早于一个由麻省理工学院、哈佛大学和加州大学柏克莱分校所初创的非盈利性在线教育网站 edX。</p><p>更多信息请访问 <a href="https://zh.wikipedia.org/zh-cn/Coursera" target="_blank" rel="noopener">维基百科</a>。</p><h2 id="旁听-coursera">旁听 Coursera</h2><h3 id="打开-coursera-上的课程-tensorflow-data-and-deployment-专项课程">打开 Coursera 上的课程 <a href="ttps://www.coursera.org/specializations/tensorflow-data-and-deployment">TensorFlow: Data and Deployment 专项课程</a></h3><p><a href="https://www.coursera.org/specializations/tensorflow-data-and-deployment" target="_blank" rel="noopener" class="uri">https://www.coursera.org/specializations/tensorflow-data-and-deployment</a></p><figure><img src="https://2020.iosdevlog.com/2020/02/24/coursera/1.png" alt="" /><figcaption>课程</figcaption></figure><h3 id="点击里面的-课程-4-advanced-deployment-scenarios-with-tensorflow">点击里面的 <a href="https://www.coursera.org/learn/advanced-deployment-scenarios-tensorflow" target="_blank" rel="noopener">课程 4 Advanced Deployment Scenarios with TensorFlow</a></h3><p><a href="https://www.coursera.org/learn/advanced-deployment-scenarios-tensorflow" target="_blank" rel="noopener" class="uri">https://www.coursera.org/learn/advanced-deployment-scenarios-tensorflow</a></p><figure><img src="https://2020.iosdevlog.com/2020/02/24/coursera/2.png" alt="" /><figcaption>免费注册</figcaption></figure><h3 id="点击-免费注册">点击 <strong>免费注册</strong></h3><figure><img src="https://2020.iosdevlog.com/2020/02/24/coursera/3.png" alt="" /><figcaption>旁听</figcaption></figure><h3 id="点击-旁听">点击 <strong>旁听</strong></h3><figure><img src="https://2020.iosdevlog.com/2020/02/24/coursera/4.png" alt="" /><figcaption>完成</figcaption></figure><h2 id="下载视频和字幕">下载视频和字幕</h2><p>dl-coursera 0.1.2</p><p><a href="https://pypi.org/project/dl-coursera/" target="_blank" rel="noopener" class="uri">https://pypi.org/project/dl-coursera/</a></p><p><code>Chrome</code> 导出 <code>cookies.txt</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip3 install -U dl_coursera</span><br><span class="line">dl_coursera --version</span><br><span class="line">dl_coursera --cookies path/to/cookies.txt --slug advanced-deployment-scenarios-tensorflow --how <span class="built_in">builtin</span></span><br></pre></td></tr></table></figure><p><code>tree advanced-deployment-scenarios-tensorflow</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">advanced-deployment-scenarios-tensorflow&#x2F;</span><br><span class="line">├── 01@tensorflow-extended</span><br><span class="line">│   ├── 01@tf-serving-as-another-deployment-option-</span><br><span class="line">│   │   ├── 01@introduction-a-conversation-with-andrew-</span><br><span class="line">│   │   │   ├── 01@.mp4</span><br><span class="line">│   │   │   └── 01@.srt</span><br><span class="line">│   │   ├── 02@introduction</span><br><span class="line">│   │   │   ├── 01@.mp4</span><br><span class="line">│   │   │   └── 01@.srt</span><br><span class="line">│   │   ├── 03@downloading-the-coding-examples-and-exer</span><br><span class="line">│   │   │   ├── 01@downloading-the-coding-examples-and-exer.html</span><br><span class="line">│   │   │   ├── 1.png</span><br><span class="line">│   │   │   └── github_screenshot.png</span><br><span class="line">│   │   ├── 04@serving</span><br><span class="line">│   │   │   ├── 01@.mp4</span><br><span class="line">│   │   │   └── 01@.srt</span><br><span class="line">│   │   ├── 05@installing-tf-serving</span><br><span class="line">│   │   │   ├── 01@.mp4</span><br><span class="line">│   │   │   └── 01@.srt</span><br><span class="line">│   │   ├── 06@installation-link</span><br><span class="line">│   │   │   └── 01@installation-link.html</span><br><span class="line">│   │   └── 07@tensorflow-serving-summary</span><br><span class="line">│   │       ├── 01@.mp4</span><br></pre></td></tr></table></figure><h2 id="上传百度网盘">上传百度网盘</h2><p>BaiduPCS-Go</p><p><a href="https://github.com/iikira/BaiduPCS-Go" target="_blank" rel="noopener" class="uri">https://github.com/iikira/BaiduPCS-Go</a></p><p>后台上传</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohub /xxx/BaiduPCS-Go u XXX . &amp;</span><br></pre></td></tr></table></figure><p>查看进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep Baidu</span><br><span class="line">iosdevl+ 16986     1  0 14:46 ?        00:00:01 BaiduPCS-Go-v3.6.1-linux-amd64/BaiduPCS-Go u xx/ .</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/24/coursera/4.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;Coursera 课程&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="学习" scheme="https://2020.iosdevlog.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Coursera" scheme="https://2020.iosdevlog.com/tags/Coursera/"/>
    
      <category term="download" scheme="https://2020.iosdevlog.com/tags/download/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow-Data-and-Deployment</title>
    <link href="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/"/>
    <id>https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/</id>
    <published>2020-02-23T03:01:43.000Z</published>
    <updated>2020-02-23T12:14:47.699Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/1.png" alt="" /><figcaption>Layer</figcaption></figure><p>GitHub 源码：<a href="https://github.com/GameDevLog/TensorFlow-Data-and-Deployment-Specialization" target="_blank" rel="noopener" class="uri">https://github.com/GameDevLog/TensorFlow-Data-and-Deployment-Specialization</a></p><a id="more"></a><h2 id="使用tensorflow.js的基于浏览器的模型browser-based-models-with-tensorflow.js">1. 使用TensorFlow.js的基于浏览器的模型(Browser-based Models with TensorFlow.js)</h2><p>将机器学习模型带入现实世界不仅仅涉及建模。本专业知识将教您如何导航各种部署方案并更有效地使用数据来训练模型。在第一门课程中，您将使用TensorFlow.js在任何浏览器中训练和运行机器学习模型。您将学习在浏览器中处理数据的技术，最后将建立一个计算机视觉项目，该项目可以识别和分类来自网络摄像头的对象。该专业化基于我们的TensorFlow实践专业化。如果您不熟悉TensorFlow，我们建议您首先参加TensorFlow实践专业化课程。为了深入了解神经网络的工作原理，我们建议您参加“​​深度学习专业化”课程。</p><h3 id="building-the-model">Building the Model</h3><h3 id="first-html-page">First HTML Page</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span>First HTML Page.<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="tfjs-script">tfjs script</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -1,6 +1,7 @@</span></span><br><span class="line"> &lt;html&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;head&gt;&lt;/head&gt;</span><br><span class="line"><span class="addition">+&lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"&gt;&lt;/script&gt;</span></span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br><span class="line">     &lt;h1&gt;First HTML Page.&lt;/h1&gt;</span><br></pre></td></tr></table></figure><h3 id="model">model</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -1,7 +1,17 @@</span></span><br><span class="line"> &lt;html&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;head&gt;&lt;/head&gt;</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"> &lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"&gt;&lt;/script&gt;</span><br><span class="line"><span class="addition">+&lt;script lang="js"&gt;</span></span><br><span class="line"><span class="addition">+    const model = tf.sequential();</span></span><br><span class="line"><span class="addition">+    model.add(tf.layers.dense(&#123; units: 1, inputShape: [1] &#125;));</span></span><br><span class="line"><span class="addition">+    model.compile(&#123;</span></span><br><span class="line"><span class="addition">+        loss: 'meanSquaredError',</span></span><br><span class="line"><span class="addition">+        optimizer: 'sgd'</span></span><br><span class="line"><span class="addition">+    &#125;);</span></span><br><span class="line"><span class="addition">+    model.summary();</span></span><br><span class="line"><span class="addition">+&lt;/script&gt;</span></span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br><span class="line">     &lt;h1&gt;First HTML Page.&lt;/h1&gt;</span><br></pre></td></tr></table></figure><h3 id="data">data</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -11,6 +11,9 @@</span></span><br><span class="line">         optimizer: 'sgd'</span><br><span class="line">     &#125;);</span><br><span class="line">     model.summary();</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+    const xs = tf.tensor2d([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], [6, 1]);</span></span><br><span class="line"><span class="addition">+    const ys = tf.tensor2d([-3.0, -1.0, 2.0, 3.0, 5.0, 7.0], [6, 1]);</span></span><br><span class="line"> &lt;/script&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br></pre></td></tr></table></figure><h3 id="train">train</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -11,6 +11,13 @@</span></span><br><span class="line">         optimizer: 'sgd'</span><br><span class="line">     &#125;);</span><br><span class="line">     model.summary();</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+    const xs = tf.tensor2d([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], [6, 1]);</span></span><br><span class="line"><span class="addition">+    const ys = tf.tensor2d([-3.0, -1.0, 2.0, 3.0, 5.0, 7.0], [6, 1]);</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+    doTraining(model).then(() =&gt; &#123;</span></span><br><span class="line"><span class="addition">+        alert(model.predict(tf.tensor2d([10], [1, 1])));</span></span><br><span class="line"><span class="addition">+    &#125;);</span></span><br><span class="line"> &lt;/script&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br></pre></td></tr></table></figure><h3 id="dotraining">doTraining</h3><p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"> @@ -4,6 +4,23 @@</span><br><span class="line"> </span><br><span class="line"> &lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"&gt;&lt;/script&gt;</span><br><span class="line"> &lt;script lang="js"&gt;</span><br><span class="line"><span class="addition">+    async function doTraining(model) &#123;</span></span><br><span class="line"><span class="addition">+        const history =</span></span><br><span class="line"><span class="addition">+            await model.fit(xs, ys,</span></span><br><span class="line"><span class="addition">+                &#123;</span></span><br><span class="line"><span class="addition">+                    epochs: 500,</span></span><br><span class="line"><span class="addition">+                    callbacks: &#123;</span></span><br><span class="line"><span class="addition">+                        onEpochEnd: async (epoch, logs) =&gt; &#123;</span></span><br><span class="line"><span class="addition">+                            console.log("Epoch:"</span></span><br><span class="line"><span class="addition">+                                + epoch</span></span><br><span class="line"><span class="addition">+                                + " Loss:"</span></span><br><span class="line"><span class="addition">+                                + logs.loss);</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+                        &#125;</span></span><br><span class="line"><span class="addition">+                    &#125;</span></span><br><span class="line"><span class="addition">+                &#125;);</span></span><br><span class="line"><span class="addition">+    &#125;</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line">     const model = tf.sequential();</span><br><span class="line">     model.add(tf.layers.dense(&#123; units: 1, inputShape: [1] &#125;));</span><br><span class="line">     model.compile(&#123;</span><br></pre></td></tr></table></figure></p><h3 id="test">test</h3><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/2.png" alt="" /><figcaption>Safari</figcaption></figure><h3 id="iris">Iris</h3><p><a href="https://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" rel="noopener" class="uri">https://archive.ics.uci.edu/ml/datasets/Iris</a></p><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/3.png" alt="" /><figcaption>Iris</figcaption></figure><p><a href="https://commons.wikimedia.org/w/index.php?curid=46257808" target="_blank" rel="noopener" class="uri">https://commons.wikimedia.org/w/index.php?curid=46257808</a></p><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/Iris_dataset_scatterplot.svg" alt="" /><figcaption>Iris_dataset_scatterplot</figcaption></figure><h3 id="iris.csv">iris.csv</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sepal_length,sepal_width,petal_length,petal_width,species</span><br><span class="line">5.1,3.5,1.4,0.2,setosa</span><br><span class="line">4.9,3,1.4,0.2,setosa</span><br><span class="line">4.7,3.2,1.3,0.2,setosa</span><br><span class="line">4.6,3.1,1.5,0.2,setosa</span><br><span class="line">5,3.6,1.4,0.2,setosa</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="async">async</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -3,6 +3,8 @@</span></span><br><span class="line"> &lt;head&gt;&lt;/head&gt;</span><br><span class="line"> &lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"&gt;&lt;/script&gt;</span><br><span class="line"> &lt;script lang="js"&gt;</span><br><span class="line"><span class="addition">+    async function run() &#123;</span></span><br><span class="line"><span class="addition">+    &#125;</span></span><br><span class="line"> &lt;/script&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br></pre></td></tr></table></figure><p>### load iris.csv</p><p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> @@ -3,6 +3,16 @@</span><br><span class="line"> &lt;head&gt;&lt;/head&gt;</span><br><span class="line"> &lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"&gt;&lt;/script&gt;</span><br><span class="line"> &lt;script lang="js"&gt;</span><br><span class="line"><span class="addition">+    async function run() &#123;</span></span><br><span class="line"><span class="addition">+        const csvUrl = 'iris.csv';</span></span><br><span class="line"><span class="addition">+        const trainingData = tf.data.csv(csvUrl, &#123;</span></span><br><span class="line"><span class="addition">+            columnConfigs: &#123;</span></span><br><span class="line"><span class="addition">+                species: &#123;</span></span><br><span class="line"><span class="addition">+                    isLabel: true</span></span><br><span class="line"><span class="addition">+                &#125;</span></span><br><span class="line"><span class="addition">+            &#125;</span></span><br><span class="line"><span class="addition">+        &#125;);</span></span><br><span class="line"><span class="addition">+    &#125;</span></span><br><span class="line"> &lt;/script&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br></pre></td></tr></table></figure></p><h3 id="one-hot-encoder">One-Hot encoder</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -12,6 +12,18 @@</span></span><br><span class="line">                 &#125;</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;);</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        const numOfFeatures = (await trainingData.columnNames()).length - 1;</span></span><br><span class="line"><span class="addition">+        const numOfSamples = 150;</span></span><br><span class="line"><span class="addition">+        const convertedData =</span></span><br><span class="line"><span class="addition">+            trainingData.map((&#123; xs, ys &#125;) =&gt; &#123;</span></span><br><span class="line"><span class="addition">+                const labels = [</span></span><br><span class="line"><span class="addition">+                    ys.species == "setosa" ? 1 : 0,</span></span><br><span class="line"><span class="addition">+                    ys.species == "virginica" ? 1 : 0,</span></span><br><span class="line"><span class="addition">+                    ys.species == "versicolor" ? 1 : 0</span></span><br><span class="line"><span class="addition">+                ]</span></span><br><span class="line"><span class="addition">+                return &#123; xs: Object.values(xs), ys: Object.values(labels) &#125;;</span></span><br><span class="line"><span class="addition">+            &#125;).batch(10);</span></span><br><span class="line">     &#125;</span><br><span class="line"> &lt;/script&gt;</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/4.png" alt="" /><figcaption>One-Hot Encoder</figcaption></figure><h2 id="nn">NN</h2><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/5.png" alt="" /><figcaption>NN</figcaption></figure><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -24,6 +24,14 @@</span></span><br><span class="line">                 ]</span><br><span class="line">                 return &#123; xs: Object.values(xs), ys: Object.values(labels) &#125;;</span><br><span class="line">             &#125;).batch(10);</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        const model = tf.sequential();</span></span><br><span class="line"><span class="addition">+        model.add(tf.layers.dense(&#123;</span></span><br><span class="line"><span class="addition">+            inputShape: [numOfFeatures],</span></span><br><span class="line"><span class="addition">+            activation: "sigmoid", units: 5</span></span><br><span class="line"><span class="addition">+        &#125;))</span></span><br><span class="line"><span class="addition">+        model.add(tf.layers.dense(&#123; activation: "softmax", units: 3 &#125;));</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line">     &#125;</span><br><span class="line"> &lt;/script&gt;</span><br></pre></td></tr></table></figure><h3 id="compile">compile</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -32,6 +32,10 @@</span></span><br><span class="line">         &#125;))</span><br><span class="line">         model.add(tf.layers.dense(&#123; activation: "softmax", units: 3 &#125;));</span><br><span class="line"> </span><br><span class="line"><span class="addition">+        model.compile(&#123;</span></span><br><span class="line"><span class="addition">+            loss: "categoricalCrossentropy",</span></span><br><span class="line"><span class="addition">+            optimizer: tf.train.adam(0.06)</span></span><br><span class="line"><span class="addition">+        &#125;);</span></span><br><span class="line">     &#125;</span><br><span class="line"> &lt;/script&gt;</span><br></pre></td></tr></table></figure><h3 id="fit">fit</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -36,6 +36,15 @@</span></span><br><span class="line">             loss: "categoricalCrossentropy",</span><br><span class="line">             optimizer: tf.train.adam(0.06)</span><br><span class="line">         &#125;);</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        await model.fitDataset(convertedData, &#123;</span></span><br><span class="line"><span class="addition">+            epochs: 100,</span></span><br><span class="line"><span class="addition">+            callbacks: &#123;</span></span><br><span class="line"><span class="addition">+                onEpochEnd: async (epoch, logs) =&gt; &#123;</span></span><br><span class="line"><span class="addition">+                    console.log("Epoch: " + epoch + " Loss: " + logs.loss);</span></span><br><span class="line"><span class="addition">+                &#125;</span></span><br><span class="line"><span class="addition">+            &#125;</span></span><br><span class="line"><span class="addition">+        &#125;);</span></span><br><span class="line">     &#125;</span><br><span class="line"> &lt;/script&gt;</span><br></pre></td></tr></table></figure><p>### predict</p><p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -45,6 +45,10 @@</span></span><br><span class="line">                 &#125;</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;);</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        const testVal = tf.tensor2d([5.8, 2.7, 5.1, 1.9], [1, 4]);</span></span><br><span class="line"><span class="addition">+        const prediction = model.predict(testVal);</span></span><br><span class="line"><span class="addition">+        alert(prediction)</span></span><br><span class="line">     &#125;</span><br><span class="line"> &lt;/script&gt;</span><br></pre></td></tr></table></figure></p><h3 id="run">run</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -50,6 +50,8 @@</span></span><br><span class="line">         const prediction = model.predict(testVal);</span><br><span class="line">         alert(prediction)</span><br><span class="line">     &#125;</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+    run();</span></span><br><span class="line"> &lt;/script&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br></pre></td></tr></table></figure><p>HTTP Server</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python2 -m SimpleHTTPServer</span><br></pre></td></tr></table></figure></p><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/6.png" alt="" /><figcaption>run</figcaption></figure><p>第 2 种概率最高</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor</span><br><span class="line">     [[0.0000663, 0.8690438, 0.1308898],]</span><br></pre></td></tr></table></figure><h3 id="classname">className</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -46,9 +46,20 @@</span></span><br><span class="line">             &#125;</span><br><span class="line">         &#125;);</span><br><span class="line"> </span><br><span class="line"><span class="addition">+        // Setosa</span></span><br><span class="line"><span class="addition">+        // const testVal = tf.tensor2d([4.4, 2.9, 1.4, 0.2], [1, 4]);</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        // Versicolor</span></span><br><span class="line"><span class="addition">+        // const testVal = tf.tensor2d([6.4, 3.2, 4.5, 1.5], [1, 4]);</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        // Virginica</span></span><br><span class="line">         const testVal = tf.tensor2d([5.8, 2.7, 5.1, 1.9], [1, 4]);</span><br><span class="line"><span class="addition">+</span></span><br><span class="line">         const prediction = model.predict(testVal);</span><br><span class="line"><span class="deletion">-        alert(prediction)</span></span><br><span class="line"><span class="addition">+        const pIndex = tf.argMax(prediction, axis = 1).dataSync();</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        const classNames = ["Setosa", "Virginica", "Versicolor"];</span></span><br><span class="line"><span class="addition">+        alert(classNames[pIndex])</span></span><br><span class="line">     &#125;</span><br><span class="line"> </span><br><span class="line">     run();</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/7.png" alt="" /><figcaption>className</figcaption></figure><h2 id="使用tensorflow-lite的基于设备的模型device-based-models-with-tensorflow-lite">2. 使用TensorFlow Lite的基于设备的模型(Device-based Models with TensorFlow Lite)</h2><p>将机器学习模型带入现实世界不仅仅涉及建模。本专业知识将教您如何导航各种部署方案并更有效地使用数据来训练模型。第二门课程教您如何在移动应用程序中运行机器学习模型。您将学习如何为低功耗，电池供电的设备准备模型，然后在Android和iOS平台上执行模型。最后，您将探索如何在Raspberry Pi和微控制器上使用TensorFlow在嵌入式系统上进行部署。该专业化基于我们的TensorFlow实践专业化。如果您不熟悉TensorFlow，我们建议您首先参加TensorFlow实践专业化课程。为了深入了解神经网络的工作原理，</p><h2 id="使用tensorflow数据服务的数据管道data-pipelines-with-tensorflow-data-services">3. 使用TensorFlow数据服务的数据管道(Data Pipelines with TensorFlow Data Services)</h2><p>将机器学习模型带入现实世界不仅仅涉及建模。本专业知识将教您如何导航各种部署方案并更有效地使用数据来训练模型。在这第三门课程中，您将在TensorFlow中使用一套工具来更有效地利用数据和训练模型。您将学习如何仅用几行代码就可以利用内置数据集，如何使用API​​控制如何拆分数据以及如何处理所有类型的非结构化数据。该专业化基于我们的TensorFlow实践专业化。如果您不熟悉TensorFlow，我们建议您首先参加TensorFlow实践专业化课程。为了深入了解神经网络的工作原理，我们建议您参加“​​深度学习专业化”课程。</p><h2 id="使用tensorflow的高级部署方案advanced-deployment-scenarios-with-tensorflow">4. 使用TensorFlow的高级部署方案(Advanced Deployment Scenarios with TensorFlow)</h2><p>将机器学习模型带入现实世界不仅仅涉及建模。本专业知识将教您如何导航各种部署方案并更有效地使用数据来训练模型。在这最后的课程中，您将探索在部署模型时会遇到的四种不同情况。将向您介绍TensorFlow Serving，该技术可让您通过Web进行推理。您将继续使用TensorFlow Hub，该模型库可用于转移学习。然后，您将使用TensorBoard评估并了解模型的工作方式，并与他人共享模型元数据。最后，您将探索联合学习，以及如何在保持数据隐私的同时使用用户数据重新训练已部署的模型。该专业化基于我们的TensorFlow实践专业化。如果您不熟悉TensorFlow，我们建议您首先参加TensorFlow实践专业化课程。为了深入了解神经网络的工作原理，我们建议您参加“​​深度学习专业化”课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;Layer&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;GitHub 源码：&lt;a href=&quot;https://github.com/GameDevLog/TensorFlow-Data-and-Deployment-Specialization&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://github.com/GameDevLog/TensorFlow-Data-and-Deployment-Specialization&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="TensorFlow" scheme="https://2020.iosdevlog.com/tags/TensorFlow/"/>
    
      <category term="Android" scheme="https://2020.iosdevlog.com/tags/Android/"/>
    
      <category term="iOS" scheme="https://2020.iosdevlog.com/tags/iOS/"/>
    
      <category term="Deployment" scheme="https://2020.iosdevlog.com/tags/Deployment/"/>
    
      <category term="js" scheme="https://2020.iosdevlog.com/tags/js/"/>
    
      <category term="mobile" scheme="https://2020.iosdevlog.com/tags/mobile/"/>
    
  </entry>
  
  <entry>
    <title>《极简算法史：从数学到机器的故事》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/02/22/9787115500809/"/>
    <id>https://2020.iosdevlog.com/2020/02/22/9787115500809/</id>
    <published>2020-02-22T15:00:26.000Z</published>
    <updated>2020-02-22T17:44:16.115Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/1.jpg" alt="" /><figcaption>《极简算法史：从数学到机器的故事》</figcaption></figure><p>书名：极简算法史：从数学到机器的故事<br />作者：[法]吕克·德·布拉班迪尔<br />译者：任轶<br />出版社：人民邮电出版社<br />出版时间：2018-12<br />ISBN：9787115500809</p><p>一位工程师、一位数学家、一位逻辑学家和一位哲学家一起在苏格兰旅行。他们走在一条路上，栖息在悬岩上的一只黑山羊看着他们路过。</p><p>“你们看到了吗？”工程师说，“在苏格兰，山羊都是黑色的！”</p><p>数学家反驳道：“可能你想说的是：有些苏格兰山羊是黑色的。”</p><p>逻辑学家补充道：“先不要妄下结论。我们只能说：苏格兰至少有一只黑山羊！”</p><p>最后，哲学家总结道：“我们唯一能真正确定的是：在这个地方的这只山羊是黑色的！”</p><a id="more"></a><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/2.jpg" alt="" /><figcaption>序</figcaption></figure><p><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/3.jpg" alt="柏拉图" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/4.jpg" alt="亚里士多德" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/6.jpg" alt="阿尔·花拉子米" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/9.jpg" alt="笛卡儿" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/11.jpg" alt="伽利略" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/13.jpg" alt="布莱士·帕斯卡" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/15.jpg" alt="托马斯·贝叶斯" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/16.jpg" alt="莱布尼茨" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/19.jpg" alt="欧拉" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/23.jpg" alt="康德" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/24.jpg" alt="乔治·布尔" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/25.jpg" alt="库尔特·哥德尔" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/27.jpg" alt="伯特兰·罗素" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/28.jpg" alt="路德维希·维特根斯坦" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/29.jpg" alt="克劳德·香农" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/31.jpg" alt="诺伯特·维纳" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/32.jpg" alt="阿兰·图灵" /></p><h2 id="第一部分-莱布尼茨之梦">第一部分 莱布尼茨之梦</h2><p>三次“抽象运动”的硕果</p><ul><li>算术</li><li>几何</li><li>代数</li></ul><p>第四次抽象运动</p><ul><li>逻辑学<ul><li>演绎法<ul><li>这条街上的所有房子都很漂亮。</li><li>这座房子在这条街上。</li><li>这座房子很漂亮。</li></ul></li><li>归纳法<ul><li>这座房子在这条街上。</li><li>这座房子很漂亮。</li><li>在这条街上所有的房子都很漂亮。</li></ul></li><li>溯因法<ul><li>这座房子很漂亮。</li><li>在这条街上所有的房子都很漂亮。</li><li>这座房子在这条街上。</li></ul></li></ul></li></ul><blockquote><p>数学与语言无关，逻辑却并非如此。</p></blockquote><h3 id="哥德尔证明罗素是在浪费时间">哥德尔证明，罗素是在浪费时间</h3><ol type="1"><li>三角形内角和为180°；</li><li>正方形的内角和为270°。</li><li>1 是正确的。</li><li>2 是错误的。</li><li>在定理中无法对 5 加以证明。</li></ol><p>于是有两种可能的情况：</p><ol type="1"><li>要么可以证明 5，但是，由于语句说明的情况与此相反，因而定理不具有逻辑的严密性；</li><li>要么无法证明 5，因而语句为真，但这意味着，定理不具有完备性。</li></ol><p><strong>悖论</strong>：“所有克里特人都是骗子”的现代版——</p><blockquote><p>埃庇米尼得斯虽然这么宣布了，但他自己就是克里特人，如果他说的是真的，那么既然他也是克里特人，那说明他也是个骗子，他的话就不可信；而如果他说谎了，那么就印证了“所有克里特人都是骗子”这句话，那说明他所言为真……</p></blockquote><h2 id="第二部分-三座丰碑">第二部分 三座丰碑</h2><h3 id="贝叶斯">贝叶斯</h3><p>《关于如何在机会论的框架下解决问题》（Anessay towards solving a problem in thedoctrine of Chance</p><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/59.jpg" alt="" /><figcaption>罐子</figcaption></figure><p>每个罐子里装有40颗石子。1号罐子里装了30颗白色石子和10颗黑色石子，2号罐子里装有黑白石子各20颗。假设随机拿起一个罐子，并从这个罐子里随机取出一颗石子，且这颗石子是白色的，那么这颗白色石子来自1号罐子的概率是多少？</p><ol type="1"><li>已知被选中的石子是白色的，用获得白色石子的概率乘以选择1号罐子的概率；</li><li>已知白色石子来自1号罐子，用选择1号罐子的概率乘以获得白色石子的概率。</li></ol><p>A：1号罐子的假设<br />B：白色石子的假设</p><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/60.jpg" alt="" /><figcaption>相同的答案</figcaption></figure><p><span class="math display">\[p(\mathrm{B}). p(\mathrm{A} / 已知 \mathrm{B})=p(\mathrm{A}). p(\mathrm{B} / 已知 \mathrm{A})\]</span></p><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/61.jpg" alt="" /><figcaption>贝叶斯公式</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/62.jpg" alt="" /><figcaption>答案</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/63.jpg" alt="" /><figcaption>贝叶斯网络</figcaption></figure><h3 id="香农证明如何计算11">香农证明，如何计算1+1</h3><p>信息动力学，两个定理分别探讨的是 <strong>信息量</strong> 和信息的 <strong>质量</strong>。</p><ul><li>第一个定理涉及信息的压缩<ul><li>编码一条信息所需的最少符号数量是多少？</li></ul></li><li>第二个定理涉及信息的传输<ul><li>为了在终点处获取从起点处发出的准确信息，需要哪些必要条件？</li></ul></li></ul><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/65.jpg" alt="" /><figcaption>通信的环境进行建模</figcaption></figure><p>测量单位。就像卡路里可以量化热交换一样，香农提出的“比特”（bit，也叫位）的概念用于测量信息量。</p><p>比特是一个二进制数字，可以取0或1的值。</p><p>信息的测量应该满足一个苛刻的关系</p><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/66.jpg" alt="" /><figcaption>对数函数</figcaption></figure><p>其中，<span class="math inline">\(f\)</span> 只能是对数函数，因为根据定义，同一底数的两个正数的对数之和等于这两个数的积的对数。</p><p>香农的公式能让我们借助经验计算出摩尔斯电码的效率为85%！我们不得不佩服公式发明者出色的直觉。</p><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/68.jpg" alt="" /><figcaption>形式逻辑</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/69.jpg" alt="" /><figcaption>与或非门</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/70.jpg" alt="" /><figcaption>逻辑电路</figcaption></figure><p>香农数：国际象棋棋局的理论数目，结果是 <span class="math inline">\(10^{120}\)</span></p><h3 id="诺伯特维纳与控制论cybernetics">诺伯特·维纳与控制论（cybernetics）</h3><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/73.jpg" alt="" /><figcaption>控制</figcaption></figure><h2 id="第三部分-自动化理性批判">第三部分 自动化理性批判</h2><blockquote><p>科学是父亲教给儿子。<br />技术是儿子教给父亲。<br />——米歇尔·塞尔</p></blockquote><h3 id="算法algorithm">算法（algorithm）</h3><p>三种不确定性</p><ol type="1"><li>圣诞节是在什么时候？你不知道，我也不知道。</li><li>谁将在2022年当选法国总统？你不知道，我也不知道。</li><li>因为我们甚至不知道“我们不知道”。</li></ol><p>第三类不确定性涉及了“没人提出的问题”。在这种情况下，超级计算机也没有用了……因为没有什么可以计算的！</p><p>这类事件通常被称为 <strong>“黑天鹅”</strong>。</p><p>这一说法是用来纪念一位18世纪的英国探险家，这位探险家曾确信所有天鹅都是白色的，然而，他在澳大利亚逗留期间，惊讶地看到了一只黑天鹅——没有一个欧洲人曾对这类水禽的颜色提出过疑问。于是，黑天鹅成为第三类不确定性的象征。</p><h3 id="全球化管理的重要性">全球化管理的重要性</h3><ul><li>互联网并非公共空间。</li><li>互联网并非全球性的。</li><li>互联网并非环保。</li><li>互联网既不是虚拟的，也不是非物质的。</li><li>互联网并非透明。（大数据）</li><li>互联网并非中立。（算法）</li><li>互联网在其运作过程中并没有完全被理解。</li><li>互联网并非市场经济的保障。</li><li>互联网并非民主的保障。</li><li>互联网并非掌握真相。</li><li>互联网积累的信息正在变得无用。</li><li>互联网并非友善。（暴力）</li><li>互联网是脆弱的。（Bug）</li><li>互联网并非自动。（参数）</li><li>互联网并非自由。（surf，原意是冲浪）</li><li>互联网只有部分是可访问的。（暗网）</li><li>互联网并非优质教育的保障。</li><li>互联网并非公平。</li><li>互联网并非免费。</li><li>互联网是我们的工具，而我们也是互联网的工具。（测试）</li><li>互联网不好也不坏。</li><li>互联网，尤其对民主国家来说，是空前的挑战。</li></ul><h3 id="死亡电脑社">死亡电脑社</h3><p>互联网上的预言大师名为 <strong>奇点</strong>。</p><p>奇点指的是机器与人类彻底融合的时刻，这种情况注定会在某一天发生。</p><h3 id="人工智能许多问题之一">人工智能：许多问题之一</h3><p>智商（IQ）</p><ul><li>音乐智力，这种智力体现为对声音和节奏的感知度。它寻找音符的含义，并想象改编为其他乐曲的可能性。</li><li>运动智力，这种智力能释放身体各个部位的潜能。它组织了用来解决特定问题的最佳动作序列。</li><li>人际关系智力（或情感智力），这种智力能识别他人的感受和意图。它能感知到对于谈判、合作和互动等行为来说，什么是重要的因素。</li><li>视觉智力，这种智力可以从三个维度进行思考。它能让我们在建模之前、在空间内移动物体之前、在看到被要求思考的东西之前，就先进行想象。</li><li>语言智力，这种智力是利用语言反应的能力。如果有必要，这种智力甚至能够催生新的语言。</li></ul><h2 id="答案">答案</h2><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/89.jpg" alt="" /><figcaption>89</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/90.jpg" alt="" /><figcaption>90</figcaption></figure><p>从古希腊哲学到数学，从逻辑推理到“无所不能”的计算机。柏拉图、莱布尼茨、罗素、香农、图灵……一个个伟大的思想家试图从数学公式中证明推理的合理性。</p><p>他们是凭借天赋制胜，还是在鲁莽地大胆一搏？</p><p>如何将逻辑赋予数学意义？</p><p>如何从简单运算，走向复杂智慧？</p><p>一场人类探索数学、算法与逻辑思维，并最终走向人工智能的梦想之旅，展现了哲学家、逻辑学家与数学家独特的思维方式，探讨了算法与人工智能对科学和社会的巨大影响。</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/22/9787115500809/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《极简算法史：从数学到机器的故事》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：极简算法史：从数学到机器的故事&lt;br /&gt;
作者：[法]吕克·德·布拉班迪尔&lt;br /&gt;
译者：任轶&lt;br /&gt;
出版社：人民邮电出版社&lt;br /&gt;
出版时间：2018-12&lt;br /&gt;
ISBN：9787115500809&lt;/p&gt;
&lt;p&gt;一位工程师、一位数学家、一位逻辑学家和一位哲学家一起在苏格兰旅行。他们走在一条路上，栖息在悬岩上的一只黑山羊看着他们路过。&lt;/p&gt;
&lt;p&gt;“你们看到了吗？”工程师说，“在苏格兰，山羊都是黑色的！”&lt;/p&gt;
&lt;p&gt;数学家反驳道：“可能你想说的是：有些苏格兰山羊是黑色的。”&lt;/p&gt;
&lt;p&gt;逻辑学家补充道：“先不要妄下结论。我们只能说：苏格兰至少有一只黑山羊！”&lt;/p&gt;
&lt;p&gt;最后，哲学家总结道：“我们唯一能真正确定的是：在这个地方的这只山羊是黑色的！”&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="Math" scheme="https://2020.iosdevlog.com/tags/Math/"/>
    
      <category term="DL" scheme="https://2020.iosdevlog.com/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Concepts</title>
    <link href="https://2020.iosdevlog.com/2020/02/22/ds/"/>
    <id>https://2020.iosdevlog.com/2020/02/22/ds/</id>
    <published>2020-02-22T11:10:39.000Z</published>
    <updated>2020-02-22T11:19:03.548Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/22/ds/Process.png" alt="" /><figcaption>Process</figcaption></figure><a id="more"></a><h2 id="types">Types</h2><h3 id="regression">Regression</h3><ul><li>A supervised problem, the outputs are continuous rather than discrete.</li></ul><h3 id="classification">Classification</h3><ul><li>Inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one or more (multi-label classification) of these classes. This is typically tackled in a supervised way.</li></ul><h3 id="clustering">Clustering</h3><ul><li>A set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.</li></ul><h3 id="density-estimation">Density Estimation</h3><ul><li>Finds the distribution of inputs in some space.</li></ul><h3 id="dimensionality-reduction">Dimensionality Reduction</h3><ul><li>Simplifies inputs by mapping them into a lower-dimensional space.</li></ul><h2 id="kind">Kind</h2><h3 id="parametric">Parametric</h3><ul><li><p>Step 1: Making an assumption about the functional form or shape of our function (f), i.e.: f is linear, thus we will select a linear model.</p></li><li><p>Step 2: Selecting a procedure to fit or train our model. This means estimating the Beta parameters in the linear function. A common approach is the (ordinary) least squares, amongst others.</p></li></ul><h3 id="non-parametric">Non-Parametric</h3><ul><li>When we do not make assumptions about the form of our function (f). However, since these methods do not reduce the problem of estimating f to a small number of parameters, a large number of observations is required in order to obtain an accurate estimate for f. An example would be the thin-plate spline model.</li></ul><h2 id="categories">Categories</h2><h3 id="supervised">Supervised</h3><ul><li>The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs.</li></ul><h3 id="unsupervised">Unsupervised</h3><ul><li>No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).</li></ul><h3 id="reinforcement-learning">Reinforcement Learning</h3><ul><li>A computer program interacts with a dynamic environment in which it must perform a certain goal (such as <a href="https://en.wikipedia.org/wiki/Autonomous_car" target="_blank" rel="noopener">driving a vehicle</a> or playing a game against an opponent). The program is provided feedback in terms of rewards and punishments as it navigates its problem space.</li></ul><h2 id="approaches">Approaches</h2><h3 id="decision-tree-learning">Decision tree learning</h3><h3 id="association-rule-learning">Association rule learning</h3><h3 id="artificial-neural-networks">Artificial neural networks</h3><h3 id="deep-learning">Deep learning</h3><h3 id="inductive-logic-programming">Inductive logic programming</h3><h3 id="support-vector-machines">Support vector machines</h3><h3 id="clustering-1">Clustering</h3><h3 id="bayesian-networks">Bayesian networks</h3><h3 id="reinforcement-learning-1">Reinforcement learning</h3><h3 id="representation-learning">Representation learning</h3><h3 id="similarity-and-metric-learning">Similarity and metric learning</h3><h3 id="sparse-dictionary-learning">Sparse dictionary learning</h3><h3 id="genetic-algorithms">Genetic algorithms</h3><h3 id="rule-based-machine-learning">Rule-based machine learning</h3><h3 id="learning-classifier-systems">Learning classifier systems</h3><h2 id="taxonomy">Taxonomy</h2><h3 id="generative-methods">Generative Methods</h3><ul><li><p>Popular models</p><ul><li><p>Mixtures of Gaussians, Mixtures of experts, Hidden Markov Models (HMM)</p></li><li><p>Gaussians, Naïve Bayes, Mixtures of multinomials</p></li><li><p>Sigmoidal belief networks, Bayesian networks, Markov random fields</p></li></ul></li><li><p>Model class-conditional pdfs and prior probabilities. “Generative” since sampling can generate synthetic data points.</p></li></ul><h3 id="discriminative-methods">Discriminative Methods</h3><ul><li><p>Directly estimate posterior probabilities. No attempt to model underlying probability distributions. Focus computational resources on given task– better performance</p></li><li><p>Popular Models</p><ul><li><p>Logistic regression, SVMs</p></li><li><p>Traditional neural networks, Nearest neighbor</p></li><li><p>Conditional Random Fields (CRF)</p></li></ul></li></ul><h2 id="selection-criteria">Selection Criteria</h2><h3 id="prediction-accuracy-vs-model-interpretability">Prediction Accuracy vs Model Interpretability</h3><ul><li>There is an inherent tradeoff between Prediction Accuracy and Model Interpretability, that is to say that as the model get more flexible in the way the function (f) is selected, they get obscured, and are hard to interpret. Flexible methods are better for inference, and inflexible methods are preferable for prediction.</li></ul><h2 id="libraries">Libraries</h2><h3 id="python">Python</h3><ul><li><p>Numpy</p><ul><li>Adds support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays</li></ul></li><li><p>Pandas</p><ul><li>Offers data structures and operations for manipulating numerical tables and time series</li></ul></li><li><p>Scikit-Learn</p><ul><li>It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.</li></ul></li><li><p>Tensorflow</p><ul><li><p>Components<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/708342AF-41CC-4702-B41B-08DE83166234.png" /></p><ul><li><p>Does lazy evaluation. Need to build the graph, and then run it in a session.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/B19BDBEE-22D5-4A3E-8861-4790CDDE01E0.png" /></p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/FC3EF1F8-AC76-4D03-9E45-036D76C4E216.png" /></p></li><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/AA360355-4A67-41ED-8475-81391BD62DB3.png" /></p></li></ul></li></ul></li></ul></li><li><p>MXNet</p><ul><li>Is an modern open-source deep learning framework used to train, and deploy deep neural networks. MXNet library is portable and can scale to multiple GPUs and multiple machines. MXNet is supported by major Public Cloud providers including AWS and Azure. Amazon has chosen MXNet as its deep learning framework of choice at AWS.</li></ul></li><li><p>Keras</p><ul><li>Is an open source neural network library written in Python. It is capable of running on top of MXNet, Deeplearning4j, Tensorflow, CNTK or Theano. Designed to enable fast experimentation with deep neural networks, it focuses on being minimal, modular and extensible.</li></ul></li><li><p>Torch</p><ul><li>Torch is an open source machine learning library, a scientific computing framework, and a script language based on the Lua programming language. It provides a wide range of algorithms for deep machine learning, and uses the scripting language LuaJIT, and an underlying C implementation.</li></ul></li><li><p>Microsoft Cognitive Toolkit</p><ul><li>Previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.</li></ul></li></ul><h2 id="tuning">Tuning</h2><h3 id="cross-validation">Cross-validation</h3><ul><li><p>One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/8EAF745C-496E-45EA-B94F-FEC16E431FAD.png" /></li></ul></li><li><p>Methods</p><ul><li><p>Leave-p-out cross-validation</p></li><li><p>Leave-one-out cross-validation</p></li><li><p>k-fold cross-validation</p></li><li><p>Holdout method</p></li><li><p>Repeated random sub-sampling validation</p></li></ul></li></ul><h3 id="hyperparameters">Hyperparameters</h3><ul><li><p>Grid Search</p><ul><li>The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.</li></ul></li><li><p>Random Search</p><ul><li>Since grid searching is an exhaustive and therefore potentially expensive method, several alternatives have been proposed. In particular, a randomized search that simply samples parameter settings a fixed number of times has been found to be more effective in high-dimensional spaces than exhaustive search.</li></ul></li><li><p>Gradient-based optimization</p><ul><li>For specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks. Since then, these methods have been extended to other models such as support vector machines or logistic regression.</li></ul></li></ul><h3 id="early-stopping-regularization">Early Stopping (Regularization)</h3><ul><li>Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit, and stop the algorithm then.</li></ul><h3 id="overfitting">Overfitting</h3><ul><li>When a given method yields a small training MSE (or cost), but a large test MSE (or cost), we are said to be overfitting the data. This happens because our statistical learning procedure is trying too hard to find pattens in the data, that might be due to random chance, rather than a property of our function. In other words, the algorithms may be learning the training data too well. If model overfits, try removing some features, decreasing degrees of freedom, or adding more data.</li></ul><h3 id="underfitting">Underfitting</h3><ul><li>Opposite of Overfitting. Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. It occurs when the model or algorithm does not fit the data enough. Underfitting occurs if the model or algorithm shows low variance but high bias (to contrast the opposite, overfitting from high variance and low bias). It is often a result of an excessively simple model.</li></ul><h3 id="bootstrap">Bootstrap</h3><ul><li>Test that applies Random Sampling with Replacement of the available data, and assigns measures of accuracy (bias, variance, etc.) to sample estimates.</li></ul><h3 id="bagging">Bagging</h3><ul><li>An approach to ensemble learning that is based on bootstrapping. Shortly, given a training set, we produce multiple different training sets (called bootstrap samples), by sampling with replacement from the original dataset. Then, for each bootstrap sample, we build a model. The results in an ensemble of models, where each model votes with the equal weight. Typically, the goal of this procedure is to reduce the variance of the model of interest (e.g. decision trees).</li></ul><h2 id="performance-analysis">Performance Analysis</h2><h3 id="confusion-matrix">Confusion Matrix</h3><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/1F3239B7-A891-4326-831C-0F01A7ACFA00.png" /></li></ul><h3 id="accuracy">Accuracy</h3><ul><li>Fraction of correct predictions, not reliable as skewed when the data set is unbalanced (that is, when the number of samples in different classes vary greatly)</li></ul><h3 id="f1-score">f1 score</h3><ul><li><p>Precision</p><ul><li>Out of all the examples the classifier labeled as positive, what fraction were correct?<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/7B2111F9-6BF0-43B2-B130-C24CAAC39365.png" /></li></ul></li><li><p>Recall</p><ul><li>Out of all the positive examples there were, what fraction did the classifier pick up?<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/77C64150-2B24-41B5-8F70-AD154A20EC66.png" /></li></ul></li><li><p>Harmonic Mean of Precision and Recall: (2 * p * r / (p + r))</p></li></ul><h3 id="roc-curve---receiver-operating-characteristics">ROC Curve - Receiver Operating Characteristics</h3><ul><li>True Positive Rate (Recall / Sensitivity) vs False Positive Rate (1-Specificity)<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/33911326-3EBD-4DF9-9875-B75F287E30D0.png" /></li></ul><h3 id="bias-variance-tradeoff">Bias-Variance Tradeoff</h3><ul><li><p>Bias refers to the amount of error that is introduced by approximating a real-life problem, which may be extremely complicated, by a simple model. If Bias is high, and/or if the algorithm performs poorly even on your training data, try adding more features, or a more flexible model.</p></li><li><p>Variance is the amount our model’s prediction would change when using a different training data set. High: Remove features, or obtain more data.</p></li></ul><h3 id="goodness-of-fit-r2">Goodness of Fit = R^2</h3><ul><li>1.0 - sum_of_squared_errors / total_sum_of_squares(y)</li></ul><h3 id="mean-squared-error-mse">Mean Squared Error (MSE)</h3><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/06091448-C605-4DEA-9650-D71A77C710C8.png" /></p><ul><li>The mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors or deviations—that is, the difference between the estimator and what is estimated.</li></ul></li></ul><h3 id="error-rate">Error Rate</h3><ul><li><p>The proportion of mistakes made if we apply out estimate model function the the training observations in a classification setting.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/6D6D7323-715D-42F8-9B71-39BE839F2C4B.png" /></li></ul></li></ul><h2 id="motivation">Motivation</h2><h3 id="prediction">Prediction</h3><ul><li>When we are interested mainly in the predicted variable as a result of the inputs, but not on the each way of the inputs affect the prediction. In a real estate example, Prediction would answer the question of: Is my house over or under valued? Non-linear models are very good at these sort of predictions, but not great for inference because the models are much less interpretable.</li></ul><h3 id="inference">Inference</h3><ul><li>When we are interested in the way each one of the inputs affect the prediction. In a real estate example, Inference would answer the question of: How much would my house cost if it had a view of the sea? Linear models are more suited for inference because the models themselves are easier to understand than their non-linear counterparts.</li></ul><h1 id="machine-learning-process">Machine Learning Process</h1><h2 id="data">Data</h2><h3 id="find">Find</h3><h3 id="collect">Collect</h3><h3 id="explore">Explore</h3><h3 id="clean-features">Clean Features</h3><h3 id="impute-features">Impute Features</h3><h3 id="engineer-features">Engineer Features</h3><h3 id="select-features">Select Features</h3><h3 id="encode-features">Encode Features</h3><h3 id="build-datasets">Build Datasets</h3><ul><li>Machine Learning is math. In specific, performing Linear Algebra on Matrices. Our data values must be numeric.</li></ul><h2 id="model">Model</h2><h3 id="select-algorithm-based-on-question-and-data-available">Select Algorithm based on question and data available</h3><h2 id="cost-function">Cost Function</h2><h3 id="the-cost-function-will-provide-a-measure-of-how-far-my-algorithm-and-its-parameters-are-from-accurately-representing-my-training-data.">The cost function will provide a measure of how far my algorithm and its parameters are from accurately representing my training data.</h3><h3 id="sometimes-referred-to-as-cost-or-loss-function-when-the-goal-is-to-minimise-it-or-objective-function-when-the-goal-is-to-maximise-it.">Sometimes referred to as Cost or Loss function when the goal is to minimise it, or Objective function when the goal is to maximise it.</h3><h2 id="optimization">Optimization</h2><h3 id="having-selected-a-cost-function-we-need-a-method-to-minimise-the-cost-function-or-maximise-the-objective-function.-typically-this-is-done-by-gradient-descent-or-stochastic-gradient-descent.">Having selected a cost function, we need a method to minimise the Cost function, or maximise the Objective function. Typically this is done by Gradient Descent or Stochastic Gradient Descent.</h3><h2 id="tuning-1">Tuning</h2><h3 id="different-algorithms-have-different-hyperparameters-which-will-affect-the-algorithms-performance.-there-are-multiple-methods-for-hyperparameter-tuning-such-as-grid-and-random-search.">Different Algorithms have different Hyperparameters, which will affect the algorithms performance. There are multiple methods for Hyperparameter Tuning, such as Grid and Random search.</h3><h2 id="results-and-benchmarking">Results and Benchmarking</h2><h3 id="analyse-the-performance-of-each-algorithms-and-discuss-results.">Analyse the performance of each algorithms and discuss results.</h3><h3 id="are-the-results-good-enough-for-production">Are the results good enough for production?</h3><h3 id="is-the-ml-algorithm-training-and-inference-completing-in-a-reasonable-timeframe">Is the ML algorithm training and inference completing in a reasonable timeframe?</h3><h2 id="scaling">Scaling</h2><h3 id="how-does-my-algorithm-scale-for-both-training-and-inference">How does my algorithm scale for both training and inference?</h3><h2 id="deployment-and-operationalisation">Deployment and Operationalisation</h2><h3 id="how-can-feature-manipulation-be-done-for-training-and-inference-in-real-time">How can feature manipulation be done for training and inference in real-time?</h3><h3 id="how-to-make-sure-that-the-algorithm-is-retrained-periodically-and-deployed-into-production">How to make sure that the algorithm is retrained periodically and deployed into production?</h3><h3 id="how-will-the-ml-algorithms-be-integrated-with-other-systems">How will the ML algorithms be integrated with other systems?</h3><h2 id="infrastructure">Infrastructure</h2><h3 id="can-the-infrastructure-running-the-machine-learning-process-scale">Can the infrastructure running the machine learning process scale?</h3><h3 id="how-is-access-to-the-ml-algorithm-provided-rest-api-sdk">How is access to the ML algorithm provided? REST API? SDK?</h3><h3 id="is-the-infrastructure-appropriate-for-the-algorithm-we-are-running-cpus-or-gpus">Is the infrastructure appropriate for the algorithm we are running? CPU's or GPU's?</h3><h2 id="direction">Direction</h2><h3 id="saas---pre-built-machine-learning-models">SaaS - Pre-built Machine Learning models</h3><ul><li><p>Google Cloud</p><ul><li><p>Vision API</p></li><li><p>Speech API</p></li><li><p>Jobs API</p></li><li><p>Video Intelligence API</p></li><li><p>Language API</p></li><li><p>Translation API</p></li></ul></li><li><p>AWS</p><ul><li><p>Rekognition</p></li><li><p>Lex</p></li><li><p>Polly</p></li></ul></li><li><p>… many others</p></li></ul><h3 id="data-science-and-applied-machine-learning">Data Science and Applied Machine Learning</h3><ul><li><p>Google Cloud</p><ul><li>ML Engine</li></ul></li><li><p>AWS</p><ul><li>Amazon Machine Learning</li></ul></li><li><p>Tools: Jupiter / Datalab / Zeppelin</p></li><li><p>… many others</p></li></ul><h3 id="machine-learning-research">Machine Learning Research</h3><ul><li><p>Tensorflow</p></li><li><p>MXNet</p></li><li><p>Torch</p></li><li><p>… many others</p></li></ul><h2 id="question">Question</h2><h3 id="is-this-a-or-b">Is this A or B?</h3><ul><li>Classification</li></ul><h3 id="how-much-or-how-many-of-these">How much, or how many of these?</h3><ul><li>Regression</li></ul><h3 id="is-this-anomalous">Is this anomalous?</h3><ul><li>Anomaly Detection</li></ul><h3 id="how-can-these-elements-be-grouped">How can these elements be grouped?</h3><ul><li>Clustering</li></ul><h3 id="what-should-i-do-now">What should I do now?</h3><ul><li>Reinforcement Learning</li></ul><h1 id="machine-learning-mathematics">Machine Learning Mathematics</h1><h2 id="costlossmin-objectivemax-functions">Cost/Loss(Min) Objective(Max) Functions</h2><h3 id="intuition">Intuition</h3><ul><li><p>The cost function will tell us how right the predictions of our model and weight matrix are, and the choice of cost function will drive how much we care about how wrong each prediction is. For instance, a hinge loss will assume the difference between each incorrect prediction value is linear. Were we to square the hinge loss and use that as our cost function, we would be telling the system that being very wrong gets exponentially worse as we get away from the right prediction. Cross Entropy would offer a probabilistic approach.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/AAAF1D3A-8184-4843-A1E3-AC53E43315FC.png" /></li></ul></li></ul><h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3><ul><li><p>Many cost functions are the result of applying Maximum Likelihood. For instance, the Least Squares cost function can be obtained via Maximum Likelihood. Cross-Entropy is another example.</p></li><li><p>The likelihood of a parameter value (or vector of parameter values), θ, given outcomes x, is equal to the probability (density) assumed for those observed outcomes given those parameter values, that is</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/A37BBE5F-E77F-411E-BA91-5A2D475063D0.png" /></li></ul></li><li><p>The natural logarithm of the likelihood function, called the log-likelihood, is more convenient to work with. Because the logarithm is a monotonically increasing function, the logarithm of a function achieves its maximum value at the same points as the function itself, and hence the log-likelihood can be used in place of the likelihood in maximum likelihood estimation and related techniques.</p></li><li><p>In general, for a fixed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the <a href="https://en.wikipedia.org/wiki/Likelihood_function" target="_blank" rel="noopener">likelihood function</a>. Intuitively, this maximizes the "agreement" of the selected model with the observed data, and for discrete random variables it indeed maximizes the probability of the observed data under the resulting distribution. Maximum-likelihood estimation gives a unified approach to estimation, which is <a href="https://en.wikipedia.org/wiki/Well_defined" target="_blank" rel="noopener">well-defined</a> in the case of the <a href="https://en.wikipedia.org/wiki/Normal_distribution" target="_blank" rel="noopener">normal distribution</a> and many other problems.</p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/396DE3E5-85C2-487D-9324-FB83FCC7F8FD.png" /></p></li><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/690903C2-BC9F-400F-877E-7B625F2A20BD.png" /></p></li><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/B8AA9D14-192E-4C93-A166-A429A293990C.png" /></p></li><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/2EE71F38-3B82-48E7-A8B8-44FC8B2A5805.png" /></p></li><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/B7F5DF41-7571-412D-81AD-4B683CC1812B.png" /></p></li></ul></li></ul><h3 id="cross-entropy">Cross-Entropy</h3><ul><li><p>Cross entropy can be used to define the loss function in machine learning and optimization. The true probability pi is the true label, and the given distribution qi is the predicted value of the current model.</p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/9E271C6A-6FBB-41FC-960B-4AA394E6EEA5.png" /></p></li><li><p>Cross-entropy error function and logistic regression<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/0397888B-C44B-4178-B1D5-F7FB2ED1F3EC.png" /></p></li></ul></li></ul><h3 id="logistic">Logistic</h3><ul><li><p>The logistic loss function is defined as:</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/8942D559-DDAA-4EE2-AD81-3D6DE9C1540C.png" /></li></ul></li></ul><h3 id="quadratic">Quadratic</h3><ul><li><p>The use of a quadratic loss function is common, for example when using least squares techniques. It is often more mathematically tractable than other loss functions because of the properties of variances, as well as being symmetric: an error above the target causes the same loss as the same magnitude of error below the target. If the target is t, then a quadratic loss function is:</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C2D73C61-1BAC-4146-B59C-2C6D75CAFCF6.png" /></li></ul></li></ul><h3 id="loss">0-1 Loss</h3><ul><li><ul><li><p>In <a href="https://en.wikipedia.org/wiki/Statistics" target="_blank" rel="noopener">statistics</a> and <a href="https://en.wikipedia.org/wiki/Decision_theory" target="_blank" rel="noopener">decision theory</a>, a frequently used loss function is the 0-1 loss function</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/4597CE60-7571-403E-98C5-B3087805A418.png" /></li></ul></li></ul></li></ul><h3 id="hinge-loss">Hinge Loss</h3><ul><li><p>The hinge loss is a loss function used for training classifiers. For an intended output t = ±1 and a classifier score y, the hinge loss of the prediction y is defined as:</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B96B2BB2-CF51-4989-8BE7-1A825082D2D6.png" /></li></ul></li></ul><h3 id="exponential">Exponential</h3><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/329F57B1-378D-47B7-B536-0AD0F626A708.png" /></li></ul><h3 id="hellinger-distance">Hellinger Distance</h3><ul><li><p>It is used to quantify the similarity between two probability distributions. It is a type of f-divergence.</p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/A9DC5C23-CACF-429F-83B2-F432DA3B3F1A.png" /></p></li><li><p>To define the Hellinger distance in terms of <a href="https://en.wikipedia.org/wiki/Measure_theory" target="_blank" rel="noopener">measure theory</a>, let P and Q denote two <a href="https://en.wikipedia.org/wiki/Probability_measure" target="_blank" rel="noopener">probability measures</a> that are <a href="https://en.wikipedia.org/wiki/Absolute_continuity" target="_blank" rel="noopener">absolutely continuous</a> with respect to a third probability measure λ. The square of the Hellinger distance between P and Q is defined as the quantity</p></li></ul></li></ul><h3 id="kullback-leibler-divengence">Kullback-Leibler Divengence</h3><ul><li><p>Is a measure of how one probability distribution diverges from a second expected probability distribution. Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference.</p><ul><li><p>Discrete<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/FC311DB8-C4BD-4D96-A9B0-786CF4091DE7.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/19746786-7A83-4A0A-B8C2-38F7E7189DFE.png" /></li></ul></li></ul></li></ul><h3 id="itakurasaito-distance">Itakura–Saito distance</h3><ul><li><p>is a measure of the difference between an original spectrum P(ω) and an approximation<br />P^(ω) of that spectrum. Although it is not a perceptual measure, it is intended to reflect perceptual (dis)similarity.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/55037B97-44EC-4173-9D7E-2755F6648E9C.png" /></li></ul></li></ul><h3 id="httpsstats.stackexchange.comquestions154879a-list-of-cost-functions-used-in-neural-networks-alongside-applications">https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications</h3><h3 id="httpsen.wikipedia.orgwikiloss_functions_for_classification">https://en.wikipedia.org/wiki/Loss_functions_for_classification</h3><h2 id="probability">Probability</h2><h3 id="concepts">Concepts</h3><ul><li><p>Frequentist vs Bayesian Probability</p><ul><li><p>Frequentist</p><ul><li>Basic notion of probability: # Results / # Attempts</li></ul></li><li><p>Bayesian</p><ul><li>The probability is not a number, but a distribution itself.</li></ul></li><li><p>http://www.behind-the-enemy-lines.com/2008/01/are-you-bayesian-or-frequentist-or.html</p></li></ul></li><li><p>Random Variable</p><ul><li><p>In <a href="https://en.wikipedia.org/wiki/Probability_and_statistics" target="_blank" rel="noopener">probability and statistics</a>, a random variable, random quantity, aleatory variable or stochastic variable is a <a href="https://en.wikipedia.org/wiki/Variable_(mathematics)" target="_blank" rel="noopener">variable</a> whose value is subject to variations due to chance (i.e. <a href="https://en.wikipedia.org/wiki/Randomness" target="_blank" rel="noopener">randomness</a>, in a mathematical sense). A random variable can take on a set of possible different values (similarly to other mathematical variables), each with an associated probability, in contrast to other mathematical variables.</p><ul><li><p>Expectation (Expected Value) of a Random Variable<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/2956137E-E540-471C-A34B-66BBE0507483.png" /></p><ul><li>Same, for continuous variables<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/54388388-46D3-4E97-A228-23669D4E1E88.png" /></li></ul></li></ul></li></ul></li><li><p>Independence</p><ul><li><p>Two <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)" target="_blank" rel="noopener">events</a> are independent, statistically independent, or stochastically independent if the occurrence of one does not affect the probability of the other.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/FE866A04-B684-4395-92EB-E73593004643.png" /></li></ul></li></ul></li><li><p>Conditionality</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/E07D7B09-0747-48C9-ADC6-87AA26B8D3BF.png" /></li></ul></li><li><p>Bayes Theorem (rule, law)</p><ul><li><p>Simple Form<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/870B8130-69C4-4DBD-9BCF-D8D96D3C7D77.png" /></p><ul><li>With Law of Total probability<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/E90A57EE-85B2-4601-BF52-1F71E557A13F.png" /></li></ul></li></ul></li><li><p>Marginalisation</p><ul><li><p>The marginal distribution of a <a href="https://en.wikipedia.org/wiki/Subset" target="_blank" rel="noopener">subset</a> of a collection of <a href="https://en.wikipedia.org/wiki/Random_variable" target="_blank" rel="noopener">random variables</a> is the <a href="https://en.wikipedia.org/wiki/Probability_distribution" target="_blank" rel="noopener">probability distribution</a> of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables.</p><ul><li><p>Continuous<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/4D1F3F76-3341-47A5-96C1-9097E5C87A38.png" /></p><ul><li><p>Discrete<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/860CED1C-EA17-44D3-A99E-B696A87A92BF.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/7E9DE18D-616C-45BB-A0C2-02C7E469F034.png" /></li></ul></li></ul></li></ul></li></ul></li><li><p>Law of Total Probability</p><ul><li><p>Is a fundamental rule relating <a href="https://en.wikipedia.org/wiki/Marginal_probability" target="_blank" rel="noopener">marginal probabilities</a> to <a href="https://en.wikipedia.org/wiki/Conditional_probabilities" target="_blank" rel="noopener">conditional probabilities</a>. It expresses the total probability of an outcome which can be realized via several distinct events - hence the name.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5F52537E-8AF1-42FA-8CB4-2B2549F3FDA7.png" /></li></ul></li></ul></li><li><p>Chain Rule</p><ul><li>Permits the calculation of any member of the <a href="https://en.wikipedia.org/wiki/Joint_distribution" target="_blank" rel="noopener">joint distribution</a> of a set of <a href="https://en.wikipedia.org/wiki/Random_variables" target="_blank" rel="noopener">random variables</a> using only <a href="https://en.wikipedia.org/wiki/Conditional_probabilities" target="_blank" rel="noopener">conditional probabilities</a>.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/4739C552-28D0-4FAB-80B1-10DC7BBE1014.png" /></li></ul></li><li><p>Bayesian Inference</p><ul><li>Bayesian inference derives the <a href="https://en.m.wikipedia.org/wiki/Posterior_probability" target="_blank" rel="noopener">posterior probability</a> as a <a href="https://en.m.wikipedia.org/wiki/Consequence_relation" target="_blank" rel="noopener">consequence</a> of two <a href="https://en.m.wikipedia.org/wiki/Antecedent_(logic)" target="_blank" rel="noopener">antecedents</a>, a <a href="https://en.m.wikipedia.org/wiki/Prior_probability" target="_blank" rel="noopener">prior probability</a> and a "<a href="https://en.m.wikipedia.org/wiki/Likelihood_function" target="_blank" rel="noopener">likelihood function</a>" derived from a <a href="https://en.m.wikipedia.org/wiki/Statistical_model" target="_blank" rel="noopener">statistical model</a> for the observed data. Bayesian inference computes the posterior probability according to <a href="https://en.m.wikipedia.org/wiki/Bayes%27_theorem" target="_blank" rel="noopener">Bayes' theorem</a>. It can be applied iteratively so to update the confidence on out hypothesis.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/5EFA0B04-EB55-4414-8DFF-69C42072527A.png" /></li></ul></li></ul><h2 id="distributions">Distributions</h2><h3 id="definition">Definition</h3><ul><li>Is a table or an equation that links each outcome of a statistical experiment with the probability of occurence. When Continuous, is is described by the Probability Density Function</li></ul><h3 id="types-density-function">Types (Density Function)</h3><ul><li><p>Normal (Gaussian)<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/6BAD212C-E812-4D94-887E-B8FC28594153.png" /></p><ul><li><p>Poisson<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/B3457C07-7876-49F4-9FDA-5516DECF2E65.png" /></p><ul><li>Uniform<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/CFF452C6-1F11-465E-BDE8-EA3B3D55251D.png" /></li></ul></li></ul></li><li><p>Bernoulli<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/8F02DF2E-0723-4EDA-83E1-04DEBB0A222F.png" /></p><ul><li><p>Gamma<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/F2C3B775-C53A-4AA9-A398-120FBDFF6EF3.png" /></p><ul><li>Binomial<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/FF9BC15A-3117-4FA4-B59D-9EBD6E46F3A6.png" /></li></ul></li></ul></li></ul><h3 id="cumulative-distribution-function-cdf">Cumulative Distribution Function (CDF)</h3><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D6AD4AEC-161F-4A81-A996-50C936E3752B.png" /></li></ul><h2 id="information-theory">Information Theory</h2><h3 id="entropy">Entropy</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/E0EC20FC-0906-4B7C-A75F-27338A35CB48.png" /></p><ul><li><p>Entropy is a measure of unpredictability of information content.</p><ul><li>To evaluate a language model, we should measure how much surprise it gives us for real sequences in that language. For each real word encountered, the language model will give a probability p. And we use -log(p) to quantify the surprise. And we average the total surprise over a long enough sequence. So, in case of a 1000-letter sequence with 500 A and 500 B, the surprise given by the 1/3-2/3 model will be:<br />[-500<em>log(1/3) - 500</em>log(2/3)]/1000 = 1/2 * Log(9/2)<br />While the correct 1/2-1/2 model will give:<br />[-500<em>log(1/2) - 500</em>log(1/2)]/1000 = 1/2 * Log(8/2)<br />So, we can see, the 1/3, 2/3 model gives more surprise, which indicates it is worse than the correct model.<br />Only when the sequence is long enough, the average effect will mimic the expectation over the 1/2-1/2 distribution. If the sequence is short, it won't give a convincing result.</li></ul></li></ul><h3 id="cross-entropy-1">Cross Entropy</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/BCFC979D-F514-45F2-8543-8AFC4539CD08.png" /></p><ul><li>Cross entropy between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an "unnatural" probability distribution q, rather than the "true" distribution p.</li></ul><h3 id="joint-entropy">Joint Entropy</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/250ECB8A-F48D-4814-B542-2C3FC45B2127.png" /></p><h3 id="conditional-entropy">Conditional Entropy</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/97F11D13-2F43-499D-9E53-5CE038479F74.png" /></p><h3 id="mutual-information">Mutual Information</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/B0FFBB3D-70E6-4C13-AFEC-4E1E4F133926.png" /></p><h3 id="kullback-leibler-divergence">Kullback-Leibler Divergence</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F669FC47-9F7C-45BB-9311-148E23624F68.png" /></p><h2 id="density-estimation-1">Density Estimation</h2><h3 id="mostly-non-parametric.-parametric-makes-assumptions-on-my-datarandom-variables-for-instance-that-they-are-normally-distributed.-non-parametric-does-not.">Mostly Non-Parametric. Parametric makes assumptions on my data/random-variables, for instance, that they are normally distributed. Non-parametric does not.</h3><h3 id="the-methods-are-generally-intended-for-description-rather-than-formal-inference">The methods are generally intended for description rather than formal inference</h3><h3 id="methods">Methods</h3><ul><li><p>Kernel Density Estimation<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/7886A650-E942-414E-B9C1-FF657BFA8738.png" /></p><ul><li><p>non-negative</p></li><li><p>it’s a type of PDF that it is symmetric</p></li><li><p>real-valued</p></li><li><p>symmetric</p></li><li><p>integral over function is equal to 1</p></li><li><p>non-parametric</p></li><li><p>calculates kernel distributions for every sample point, and then adds all the distributions</p></li><li><p>Uniform, Triangle, Quartic, Triweight, Gaussian, Cosine, others...</p></li></ul></li><li><p>Cubic Spline</p><ul><li>A cubic spline is a function created from cubic polynomials on each between-knot interval by pasting them together twice continuously differentiable at the knots.</li></ul></li></ul><h2 id="regularization">Regularization</h2><h3 id="l1-norm">L1 norm</h3><ul><li><p>Manhattan Distance</p><ul><li><p>L1-norm is also known as least absolute deviations (LAD), least absolute errors (LAE). It is basically minimizing the sum of the absolute differences (S) between the target value and the estimated values.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C8581E8F-5C6B-424E-839D-9A01C3BEAE53.png" /></li></ul></li><li><p>Intuitively, the L1 norm prefers a weight matrix which contains the larger number of zeros.</p></li></ul></li></ul><h3 id="l2-norm">L2 norm</h3><ul><li><p>Euclidean Distance</p><ul><li><p>L2-norm is also known as least squares. It is basically minimizing the sum of the square of the differences (S) between the target value and the estimated values:</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/7839524D-1CE8-4634-A5D3-8B493ADBF0CE.png" /></li></ul></li><li><p>Intuitively, the L2 norm prefers a weight matrix where the norm is distributed across all weight matrix entries.</p></li></ul></li></ul><h3 id="early-stopping">Early Stopping</h3><ul><li>Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit, and stop the algorithm then.</li></ul><h3 id="dropout">Dropout</h3><ul><li>Is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term "dropout" refers to dropping out units (both hidden and visible) in a neural network</li></ul><h3 id="sparse-regularizer-on-columns">Sparse regularizer on columns</h3><ul><li><p>This regularizer defines an L2 norm on each column and an L1 norm over all columns. It can be solved by proximal methods.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/DF7A3A69-3945-4222-86DA-EF91B8FF6740.png" /></li></ul></li></ul><h3 id="nuclear-norm-regularization">Nuclear norm regularization</h3><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5D348DC1-B376-4FCA-9FA5-F2C2E4697D84.png" /></li></ul><h3 id="mean-constrained-regularization">Mean-constrained regularization</h3><ul><li><p>This regularizer constrains the functions learned for each task to be similar to the overall average of the functions across all tasks. This is useful for expressing prior information that each task is expected to share similarities with each other task. An example is predicting blood iron levels measured at different times of the day, where each task represents a different person.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/AED450B9-7864-4B51-8554-0B9A8CBB92BE.png" /></li></ul></li></ul><h3 id="clustered-mean-constrained-regularization">Clustered mean-constrained regularization</h3><ul><li><p>This regularizer is similar to the mean-constrained regularizer, but instead enforces similarity between tasks within the same cluster. This can capture more complex prior information. This technique has been used to predict Netflix recommendations.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/4427EABD-9457-44F5-AE53-47D654CA898C.png" /></li></ul></li></ul><h3 id="graph-based-similarity">Graph-based similarity</h3><ul><li><p>More general than above, similarity between tasks can be defined by a function. The regularizer encourages the model to learn similar functions for similar tasks.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B26E5AA5-905F-49BB-A492-F0D34378ACC2.png" /></li></ul></li></ul><h2 id="optimization-1">Optimization</h2><h3 id="gradient-descent">Gradient Descent</h3><ul><li>Is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.</li></ul><h3 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h3><ul><li><p>Gradient descent uses total gradient over all examples per update, SGD updates after only 1 or few examples:</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/187B983B-A9BE-4B1C-8ACF-ACB40B0E5725.png" /></li></ul></li></ul><h3 id="mini-batch-stochastic-gradient-descent-sgd">Mini-batch Stochastic Gradient Descent (SGD)</h3><ul><li>Gradient descent uses total gradient over all examples per update, SGD updates after only 1 example</li></ul><h3 id="momentum">Momentum</h3><ul><li>Idea: Add a fraction v of previous update to current one. When the gradient keeps pointing in the same direction, this will<br />increase the size of the steps taken towards the minimum.</li></ul><h3 id="adagrad">Adagrad</h3><ul><li>Adaptive learning rates for each parameter</li></ul><h2 id="statistics">Statistics</h2><h3 id="measures-of-central-tendency">Measures of Central Tendency</h3><ul><li><p>Mean</p></li><li><p>Median</p><ul><li>Value in the middle or an ordered list, or average of two in middle.</li></ul></li><li><p>Mode</p><ul><li>Most Frequent Value</li></ul></li><li><p>Quantile</p><ul><li>Division of probability distributions based on contiguous intervals with equal probabilities. In short: Dividing observations numbers in a sample list equally.</li></ul></li></ul><h3 id="dispersion">Dispersion</h3><ul><li><p>Range</p></li><li><p>Medium Absolute Deviation (MAD)</p><ul><li>The average of the absolute value of the deviation of each value from the mean</li></ul></li><li><p>Inter-quartile Range (IQR)</p><ul><li>Three quartiles divide the data in approximately four equally divided parts</li></ul></li><li><p>Variance</p><ul><li><p>Definition</p><ul><li>The average of the squared differences from the Mean. Formally, is the expectation of the squared deviation of a random variable from its mean, and it informally measures how far a set of (random) numbers are spread out from their mean.</li></ul></li><li><p>Types</p><ul><li><p>Continuous<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/19DB13EA-7024-46CD-9404-64E8EA2850A4.png" /></p><ul><li>Discrete<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/C416473F-2CEC-4F12-8E9D-B0D7D1C686A6.png" /></li></ul></li></ul></li></ul></li><li><p>Standard Deviation</p><ul><li><p>sqrt(variance)<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/15CD2CF7-B982-4C9C-B69F-33D20657E7A2.png" /></p><ul><li><p>z-score/value/factor</p><ul><li>The signed number of <a href="https://en.wikipedia.org/wiki/Standard_deviation" target="_blank" rel="noopener">standard deviations</a> an observation or <a href="https://en.wikipedia.org/wiki/Data" target="_blank" rel="noopener">datum</a> is above the <a href="https://en.wikipedia.org/wiki/Mean" target="_blank" rel="noopener">mean</a>.</li></ul></li></ul></li></ul></li></ul><h3 id="relationship">Relationship</h3><ul><li><p>Covariance</p><ul><li><p>dot(de_mean(x), de_mean(y)) / (n - 1)<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/0C8FD4CE-AC05-4975-ABDD-B54BD11F8312.png" /></p><ul><li>A measure of how much two random variables change together. http://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean</li></ul></li></ul></li><li><p>Correlation</p><ul><li><p>Pearson</p><ul><li>Benchmarks linear relationship, most appropriate for measurements taken from an interval scale, is a measure of the linear dependence between two variables<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/65EE6640-3536-4A5D-8935-020FDEA6FCDB.png" /></li></ul></li><li><p>Spearman</p><ul><li>Benchmarks monotonic relationship (whether linear or not), Spearman's coefficient is appropriate for both continuous and discrete variables, including ordinal variables.</li></ul></li><li><p>Kendall</p><ul><li><p>Is a <a href="https://en.wikipedia.org/wiki/Statistic" target="_blank" rel="noopener">statistic</a> used to measure the <a href="https://en.wikipedia.org/wiki/Ordinal_association" target="_blank" rel="noopener">ordinal association</a> between two measured quantities.</p></li><li><p>Contrary to the <a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" target="_blank" rel="noopener">Spearman correlation</a>, the Kendall correlation is not affected by how far from each other ranks are but only by whether the ranks between observations are equal or not, and is thus only appropriate for <a href="https://en.wikipedia.org/wiki/Discrete_variable" target="_blank" rel="noopener">discrete variables</a> but not defined for <a href="https://en.wikipedia.org/wiki/Continuous_variable" target="_blank" rel="noopener">continuous variables</a>.</p></li></ul></li><li><p>Summary: Pearson’s r for two normally distributed variables // Spearman’s rho for ratio data, ordinal data, etc (rank-order correlation) // Kendall’s tau for ordinal variables</p></li></ul></li><li><p>Co-occurrence</p><ul><li>The results are presented in a matrix format, where the cross tabulation of two fields is a cell value. The cell value represents the percentage of times that the two fields exist in the same events.</li></ul></li></ul><h3 id="techniques">Techniques</h3><ul><li><p>Null Hypothesis</p><ul><li>Is a general statement or default position that there is no relationship between two measured phenomena, or no association among groups. The null hypothesis is generally assumed to be true until evidence indicates otherwise.</li></ul></li><li><p>p-value</p><ul><li><p>Five heads in a row Example</p><ul><li><p>This demonstrates that specifying a direction (on a symmetric test statistic) halves the p-value (increases the significance) and can mean the difference between data being considered significant or not.</p></li><li><p>Suppose a researcher flips a coin five times in a row and assumes a null hypothesis that the coin is fair. The test statistic of "total number of heads" can be one-tailed or two-tailed: a one-tailed test corresponds to seeing if the coin is biased towards heads, but a two-tailed test corresponds to seeing if the coin is biased either way. The researcher flips the coin five times and observes heads each time (HHHHH), yielding a test statistic of 5. In a one-tailed test, this is the upper extreme of all possible outcomes, and yields a p-value of (1/2)5 = 1/32 ≈ 0.03. If the researcher assumed a significance level of 0.05, this result would be deemed significant and the hypothesis that the coin is fair would be rejected. In a two-tailed test, a test statistic of zero heads (TTTTT) is just as extreme and thus the data of HHHHH would yield a p-value of 2×(1/2)5 = 1/16 ≈ 0.06, which is not significant at the 0.05 level.</p></li></ul></li><li><p>In this method, as part of experimental design, before performing the experiment, one first chooses a model (the null hypothesis) and a threshold value for p, called the significance level of the test, traditionally 5% or 1% and denoted as α. If the p-value is less than the chosen significance level (α), that suggests that the observed data is sufficiently inconsistent with the null hypothesis that the null hypothesis may be rejected. However, that does not prove that the tested hypothesis is true. For typical analysis, using the standard α = 0.05 cutoff, the null hypothesis is rejected when p &lt; .05 and not rejected when p &gt; .05. The p-value does not, in itself, support reasoning about the probabilities of hypotheses but is only a tool for deciding whether to reject the null hypothesis.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/031EBE1C-F064-4DB0-9DE7-CABBA8D17668.png" /></li></ul></li></ul></li><li><p>p-hacking</p><ul><li>The process of data mining involves automatically testing huge numbers of hypotheses about a single <a href="https://en.wikipedia.org/wiki/Data_set" target="_blank" rel="noopener">data set</a> by exhaustively searching for combinations of variables that might show a correlation. Conventional tests of <a href="https://en.wikipedia.org/wiki/Statistical_significance" target="_blank" rel="noopener">statistical significance</a> are based on the probability that an observation arose by chance, and necessarily accept some risk of mistaken test results, called the <a href="https://en.wikipedia.org/wiki/Statistical_significance" target="_blank" rel="noopener">significance</a>.</li></ul></li></ul><h3 id="central-limit-theorem">Central Limit Theorem</h3><ul><li><p>States that a random variable defined as the average of a large number of independent and identically distributed random variables is itself approximately normally distributed.</p><ul><li>http://blog.vctr.me/posts/central-limit-theorem.html</li></ul></li></ul><h3 id="experiments-and-tests">Experiments and Tests</h3><ul><li><p>Flow Chart of Commonly Used Stat Tests<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/0DDFCAD3-A0C9-4978-A0F1-B47872AC82B2.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/45D4572F-11EE-4280-A978-384D0E1D872A.png" /></li></ul></li><li><p>Research Question</p><ul><li><p>Research question (Q):</p><ul><li>Asks whether the independent variable has an effect: “If there is a change in the independent variable, will there also be a change in the dependent variable?”</li></ul></li><li><p>Null hypothesis (Ho):</p><ul><li>The assumption that there is no effect: “There is no change in the dependent variable when the independent variable changes.”</li></ul></li></ul></li><li><p>Types of variables</p><ul><li><p>Dependent variable is the measure of interest</p></li><li><p>Independent variable is manipulated to observe the effect on dependent variable</p></li><li><p>Controlled variables are materials, measurements and methods that don’t change</p></li></ul></li><li><p>Experiment design</p><ul><li><p>Between subjects: Each subject sees one and only one condition</p></li><li><p>Within subjects: Subjects see more than one or all conditions</p></li></ul></li><li><p>Testing reliability with p-values</p><ul><li><p>Most tests calculate a p-value measuring observation extremity</p></li><li><p>Compare to significance level threshold α</p></li><li><p>α is the probability of rejecting H0 given that it is true</p></li><li><p>Commonly use α of 5% or 1%</p></li></ul></li></ul><h2 id="linear-algebra">Linear Algebra</h2><h3 id="matrices">Matrices</h3><ul><li><p>Almost all Machine Learning algorithms use Matrix algebra in one way or another. This is a broad subject, too large to be included here in it’s full length. Here’s a start: https://en.wikipedia.org/wiki/Matrix_(mathematics)</p><ul><li><p>Basic Operations: Addition, Multiplication, Transposition</p></li><li><p>Transformations</p></li><li><p>Trace, Rank, Determinante, Inverse</p></li></ul></li></ul><h3 id="eigenvectors-and-eigenvalues">Eigenvectors and Eigenvalues</h3><ul><li><p>In <a href="https://en.wikipedia.org/wiki/Linear_algebra" target="_blank" rel="noopener">linear algebra</a>, an eigenvector or characteristic vector of a <a href="https://en.wikipedia.org/wiki/Linear_map" target="_blank" rel="noopener">linear transformation</a> T from a <a href="https://en.wikipedia.org/wiki/Vector_space" target="_blank" rel="noopener">vector space</a> V over a <a href="https://en.wikipedia.org/wiki/Field_(mathematics)" target="_blank" rel="noopener">field</a> F into itself is a non-zero <a href="https://en.wikipedia.org/wiki/Vector_space" target="_blank" rel="noopener">vector</a> that does not change its direction when that linear transformation is applied to it.</p><ul><li>http://setosa.io/ev/eigenvectors-and-eigenvalues/<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/877F41BA-7063-48C3-AA8C-26173DDA8DE4.png" /></li></ul></li></ul><h3 id="derivatives-chain-rule">Derivatives Chain Rule</h3><ul><li><p>Rule<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/A416F2D4-5106-41F7-8E5B-40B5E73EA434.png" /></p><ul><li>Leibniz Notation<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/D2FA6B8A-FA31-4015-87B8-5FDCC70A63F4.png" /></li></ul></li></ul><h3 id="jacobian-matrix">Jacobian Matrix</h3><ul><li><p>The <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)" target="_blank" rel="noopener">matrix</a> of all first-order <a href="https://en.wikipedia.org/wiki/Partial_derivative" target="_blank" rel="noopener">partial derivatives</a> of a <a href="https://en.wikipedia.org/wiki/Vector-valued_function" target="_blank" rel="noopener">vector-valued function</a>. When the matrix is a <a href="https://en.wikipedia.org/wiki/Square_matrix" target="_blank" rel="noopener">square matrix</a>, both the matrix and its <a href="https://en.wikipedia.org/wiki/Determinant" target="_blank" rel="noopener">determinant</a> are referred to as the Jacobian in literature</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/4E2AA87B-6A9A-42C7-8C3E-C2B25E198D30.png" /></li></ul></li></ul><h3 id="gradient">Gradient</h3><ul><li><p>The gradient is a multi-variable generalization of the derivative. The gradient is a vector-valued function, as opposed to a derivative, which is scalar-valued.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/695F6632-0CA9-4AE7-9058-6F7E6DC780B4.png" /></li></ul></li></ul><h3 id="tensors">Tensors</h3><ul><li><p>For Machine Learning purposes, a Tensor can be described as a Multidimentional Matrix Matrix. Depending on the dimensions, the Tensor can be a Scalar, a Vector, a Matrix, or a Multidimentional Matrix.</p><ul><li><p>When measuring the forces applied to an infinitesimal cube, one can store the force values in a multidimensional matrix.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/1F2AE40B-6E5E-4501-AEB1-FFCAF145B311.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/7E0FE9AC-605D-4FB4-8790-DE3B05336F1F.png" /></li></ul></li></ul></li></ul><h3 id="curse-of-dimensionality">Curse of Dimensionality</h3><ul><li>When the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality.</li></ul><h1 id="machine-learning-data-processing">Machine Learning Data Processing</h1><h2 id="feature-selection">Feature Selection</h2><h3 id="correlation">Correlation</h3><ul><li><p>Features should be uncorrelated with each other and highly correlated to the feature we’re trying to predict.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/87F7C9A2-8634-4E57-85E2-9B3C4B9D33F8.png" /></p><ul><li><p>Covariance</p><ul><li>A measure of how much two random variables change together. Math: dot(de_mean(x), de_mean(y)) / (n - 1)<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/0C8FD4CE-AC05-4975-ABDD-B54BD11F8312.png" /></li></ul></li></ul></li></ul><h3 id="dimensionality-reduction-1">Dimensionality Reduction</h3><ul><li><p>Principal Component Analysis (PCA)</p><ul><li><p>Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.</p><ul><li>Plot the variance per feature and select the features with the largest variance.</li></ul></li></ul></li><li><p>Singular Value Decomposition (SVD)</p><ul><li><p>SVD is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix (for example, a symmetric matrix with positive eigenvalues) to any m×n matrix via an extension of the polar decomposition. It has many useful applications in signal processing and statistics.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/53B97C2C-8826-477D-BC26-41CABEC4A42E.png" /></li></ul></li></ul></li></ul><h3 id="importance">Importance</h3><ul><li><p>Filter Methods</p><ul><li><p>Filter type methods select features based only on general metrics like the correlation with the variable to predict. Filter methods suppress the least interesting variables. The other variables will be part of a classification or a regression model used to classify or to predict data. These methods are particularly effective in computation time and robust to overfitting.</p><ul><li><p>Correlation</p></li><li><p>Linear Discriminant Analysis</p></li><li><p>ANOVA: Analysis of Variance</p></li><li><p>Chi-Square</p></li></ul></li></ul></li><li><p>Wrapper Methods</p><ul><li><p>Wrapper methods evaluate subsets of variables which allows, unlike filter approaches, to detect the possible interactions between variables. The two main disadvantages of these methods are : The increasing overfitting risk when the number of observations is insufficient. AND. The significant computation time when the number of variables is large.</p><ul><li><p>Forward Selection</p></li><li><p>Backward Elimination</p></li><li><p>Recursive Feature Ellimination</p></li><li><p>Genetic Algorithms</p></li></ul></li></ul></li><li><p>Embedded Methods</p><ul><li><p>Embedded methods try to combine the advantages of both previous methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously.</p><ul><li><p>Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.</p></li><li><p>Ridge regression performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients.</p></li></ul></li></ul></li></ul><h2 id="feature-encoding">Feature Encoding</h2><h3 id="machine-learning-algorithms-perform-linear-algebra-on-matrices-which-means-all-features-must-be-numeric.-encoding-helps-us-do-this.">Machine Learning algorithms perform Linear Algebra on Matrices, which means all features must be numeric. Encoding helps us do this.</h3><h3 id="label-encoding">Label Encoding</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/397D4FCF-04A7-4D68-927F-D6414FA747FE.png" /></p><ul><li><p>One Hot Encoding<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/8F75FD12-E83A-4DE5-94ED-CDDD231E3E14.png" /></p><ul><li>In One Hot Encoding, make sure the encodings are done in a way that all features are linearly independent.</li></ul></li></ul><h2 id="feature-normalisation-or-scaling">Feature Normalisation or Scaling</h2><h3 id="section"></h3><ul><li><p>Since the range of values of raw data varies widely, in some <a href="https://en.wikipedia.org/wiki/Machine_learning" target="_blank" rel="noopener">machine learning</a> algorithms, objective functions will not work properly without <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)" target="_blank" rel="noopener">normalization</a>. Another reason why feature scaling is applied is that <a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener">gradient descent</a> converges much faster with feature scaling than without it.</p></li><li><p>Methods</p><ul><li><p>Rescaling</p><ul><li><p>The simplest method is rescaling the range of features to scale the range in [0, 1] or [−1, 1].</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/044BC9F5-9D15-4A64-9529-63403036B623.png" /></li></ul></li></ul></li><li><p>Standardization</p><ul><li><p>Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C228CF35-C057-4DF4-BEC2-6A7CB7D1C92F.png" /></li></ul></li></ul></li><li><p>Scaling to unit length</p><ul><li><p>To scale the components of a feature vector such that the complete vector has length one.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/1452F430-91D5-4E1A-8021-3BA21B90AD83.png" /></li></ul></li></ul></li></ul></li></ul><h2 id="dataset-construction">Dataset Construction</h2><h3 id="training-dataset">Training Dataset</h3><ul><li><p>A set of examples used for learning</p><ul><li><ul><li>To fit the parameters of the classifier in the Multilayer Perceptron, for instance, we would use the training set to find the “optimal” weights when using back-progapation.</li></ul></li></ul></li></ul><h3 id="test-dataset">Test Dataset</h3><ul><li><p>A set of examples used only to assess the performance of a fully-trained classifier</p><ul><li>In the Multilayer Perceptron case, we would use the test to estimate the error rate after we have chosen the final model (MLP size and actual weights) After assessing the final model on the test set, YOU MUST NOT tune the model any further.</li></ul></li></ul><h3 id="validation-dataset">Validation Dataset</h3><ul><li><p>A set of examples used to tune the parameters of a classifier</p><ul><li>In the Multilayer Perceptron case, we would use the validation set to find the “optimal” number of hidden units or determine a stopping point for the back-propagation algorithm</li></ul></li></ul><h3 id="cross-validation-1">Cross Validation</h3><ul><li>One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds.</li></ul><h2 id="feature-engineering">Feature Engineering</h2><h3 id="decompose">Decompose</h3><ul><li>Converting 2014-09-20T20:45:40Z into categorical attributes like hour_of_the_day, part_of_day, etc.</li></ul><h3 id="discretization">Discretization</h3><ul><li><p>Continuous Features</p><ul><li>Typically data is discretized into partitions of K equal lengths/width (equal intervals) or K% of the total data (equal frequencies).</li></ul></li><li><p>Categorical Features</p><ul><li>Values for categorical features may be combined, particularly when there’s few samples for some categories.</li></ul></li></ul><h3 id="reframe-numerical-quantities">Reframe Numerical Quantities</h3><ul><li>Changing from grams to kg, and losing detail might be both wanted and efficient for calculation</li></ul><h3 id="crossing">Crossing</h3><ul><li>Creating new features as a combination of existing features. Could be multiplying numerical features, or combining categorical variables. This is a great way to add domain expertise knowledge to the dataset.</li></ul><h2 id="feature-imputation">Feature Imputation</h2><h3 id="hot-deck">Hot-Deck</h3><ul><li>The technique then finds the first missing value and uses the cell value immediately prior to the data that are missing to impute the missing value.</li></ul><h3 id="cold-deck">Cold-Deck</h3><ul><li>Selects donors from another dataset to complete missing data.</li></ul><h3 id="mean-substitution">Mean-substitution</h3><ul><li>Another imputation technique involves replacing any missing value with the mean of that variable for all other cases, which has the benefit of not changing the sample mean for that variable.</li></ul><h3 id="regression-1">Regression</h3><ul><li>A regression model is estimated to predict observed values of a variable based on other variables, and that model is then used to impute values in cases where that variable is missing</li></ul><h3 id="some-libraries...">Some Libraries...</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/091E0B05-1B93-4959-B8C7-7E135A336EF7.png" /></p><h2 id="feature-cleaning">Feature Cleaning</h2><h3 id="missing-values">Missing values</h3><ul><li>One may choose to either omit elements from a dataset that contain missing values or to impute a value</li></ul><h3 id="special-values">Special values</h3><ul><li>Numeric variables are endowed with several formalized special values including ±Inf, NA and NaN. Calculations involving special values often result in special values, and need to be handled/cleaned</li></ul><h3 id="outliers">Outliers</h3><ul><li>They should be detected, but not necessarily removed. Their inclusion in the analysis is a statistical decision.</li></ul><h3 id="obvious-inconsistencies">Obvious inconsistencies</h3><ul><li>A person's age cannot be negative, a man cannot be pregnant and an under-aged person cannot possess a drivers license.</li></ul><h2 id="data-exploration">Data Exploration</h2><h3 id="variable-identification">Variable Identification</h3><ul><li>Identify Predictor (Input) and Target (output) variables. Next, identify the data type and category of the variables.</li></ul><h3 id="univariate-analysis">Univariate Analysis</h3><ul><li><p>Continuous Features</p><ul><li>Mean, Median, Mode, Min, Max, Range, Quartile, IQR, Variance, Standard Deviation, Skewness, Histogram, Box Plot</li></ul></li><li><p>Categorical Features</p><ul><li>Frequency, Histogram</li></ul></li></ul><h3 id="bi-variate-analysis">Bi-variate Analysis</h3><ul><li><p>Finds out the relationship between two variables.</p></li><li><p>Scatter Plot</p></li><li><p>Correlation Plot - Heatmap</p></li><li><ul><li><p>Two-way table</p><ul><li>We can start analyzing the relationship by creating a two-way table of count and count%.</li></ul></li><li><p>Stacked Column Chart</p></li><li><p>Chi-Square Test</p><ul><li>This test is used to derive the statistical significance of relationship between the variables.</li></ul></li><li><p>Z-Test/ T-Test</p></li><li><p>ANOVA</p></li></ul></li></ul><h2 id="data-types">Data Types</h2><h3 id="nominal---is-for-mutual-exclusive-but-not-ordered-categories.">Nominal - is for mutual exclusive, but not ordered, categories.</h3><h3 id="ordinal---is-one-where-the-order-matters-but-not-the-difference-between-values.">Ordinal - is one where the order matters but not the difference between values.</h3><h3 id="ratio---has-all-the-properties-of-an-interval-variable-and-also-has-a-clear-definition-of-0.0.">Ratio - has all the properties of an interval variable, and also has a clear definition of 0.0.</h3><h3 id="interval---is-a-measurement-where-the-difference-between-two-values-is-meaningful.">Interval - is a measurement where the difference between two values is meaningful.</h3><h3 id="section-1"></h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F45BC780-34A4-4478-8204-361A57CAC7A4.png" /></p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F3BE7BDA-CC8D-44E9-9586-3C4EA3B875B2.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/385CB279-89CD-4AD6-82E7-D80BAA9F2826.png" /></li></ul></li></ul><h1 id="machine-learning-models">Machine Learning Models</h1><h2 id="regression-2">Regression</h2><h3 id="linear-regression">Linear Regression</h3><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B3C1808C-9C79-4E45-9AC6-A5685A5D8407.png" /></li></ul><h3 id="generalised-linear-models-glms">Generalised Linear Models (GLMs)</h3><ul><li><p>Is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.</p></li><li><p>Link Function</p><ul><li><p>Identity</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C5A62BA8-B2BB-4325-A5CC-ADBF498E685A.png" /></li></ul></li><li><p>Inverse</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/72E0D7A0-9E26-49E9-A540-18A69873BE47.png" /></li></ul></li><li><p>Logit</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/3B5287CE-BF34-4688-844F-57875F627F8A.png" /></li></ul></li></ul></li><li><p>Cost Function is found via Maximum Likelihood Estimation</p></li></ul><h3 id="locally-estimated-scatterplot-smoothing-loess">Locally Estimated Scatterplot Smoothing (LOESS)</h3><h3 id="ridge-regression">Ridge Regression</h3><h3 id="least-absolute-shrinkage-and-selection-operator-lasso">Least Absolute Shrinkage and Selection Operator (LASSO)</h3><h3 id="logistic-regression">Logistic Regression</h3><ul><li><p>Logistic Function<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/249C3E38-0B9A-4493-982C-CC26B7614B12.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D75A0917-3A7D-441E-B18F-A51B087761D3.png" /></li></ul></li></ul><h2 id="bayesian">Bayesian</h2><h3 id="naive-bayes">Naive Bayes</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/A1AD6FA2-19D3-4C06-A092-420D007E8E3E.png" /></p><ul><li>Naive Bayes Classifier. We neglect the denominator as we calculate for every class and pick the max of the numerator<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/1810007C-AE05-41BF-A70B-6FDA309289BC.png" /></li></ul><h3 id="multinomial-naive-bayes">Multinomial Naive Bayes</h3><h3 id="bayesian-belief-network-bbn">Bayesian Belief Network (BBN)</h3><h2 id="dimensionality-reduction-2">Dimensionality Reduction</h2><h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3><h3 id="partial-least-squares-regression-plsr">Partial Least Squares Regression (PLSR)</h3><h3 id="principal-component-regression-pcr">Principal Component Regression (PCR)</h3><h3 id="partial-least-squares-discriminant-analysis">Partial Least Squares Discriminant Analysis</h3><h3 id="quadratic-discriminant-analysis-qda">Quadratic Discriminant Analysis (QDA)</h3><h3 id="linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)</h3><h2 id="instance-based">Instance Based</h2><h3 id="k-nearest-neighbour-knn">k-nearest Neighbour (kNN)</h3><h3 id="learning-vector-quantization-lvq">Learning Vector Quantization (LVQ)</h3><h3 id="self-organising-map-som">Self-Organising Map (SOM)</h3><h3 id="locally-weighted-learning-lwl">Locally Weighted Learning (LWL)</h3><h2 id="decision-tree">Decision Tree</h2><h3 id="random-forest">Random Forest</h3><h3 id="classification-and-regression-tree-cart">Classification and Regression Tree (CART)</h3><h3 id="gradient-boosting-machines-gbm">Gradient Boosting Machines (GBM)</h3><h3 id="conditional-decision-trees">Conditional Decision Trees</h3><h3 id="gradient-boosted-regression-trees-gbrt">Gradient Boosted Regression Trees (GBRT)</h3><h2 id="clustering-2">Clustering</h2><h3 id="algorithms">Algorithms</h3><ul><li><p>Hierarchical Clustering</p><ul><li><p>Linkage</p><ul><li><p>complete</p></li><li><p>single</p></li><li><p>average</p></li><li><p>centroid</p></li></ul></li><li><p>Dissimilarity Measure</p><ul><li><p>Euclidean</p><ul><li>Euclidean distance or Euclidean metric is the "ordinary" straight-line distance between two points in Euclidean space.</li></ul></li><li><p>Manhattan</p><ul><li>The distance between two points measured along axes at right angles.</li></ul></li></ul></li></ul></li><li><p>k-Means</p><ul><li>How many clusters do we select?</li></ul></li><li><p>k-Medians</p></li><li><p>Fuzzy C-Means</p></li><li><p>Self-Organising Maps (SOM)</p></li><li><p>Expectation Maximization</p></li><li><p>DBSCAN</p></li></ul><h3 id="validation">Validation</h3><ul><li><p>Data Structure Metrics</p><ul><li><p>Dunn Index</p></li><li><p>Connectivity</p></li><li><p>Silhouette Width</p></li></ul></li><li><p>Stability Metrics</p><ul><li><p>Non-overlap APN</p></li><li><p>Average Distance AD</p></li><li><p>Figure of Merit FOM</p></li><li><p>Average Distance Between Means ADM</p></li></ul></li></ul><h2 id="neural-networks">Neural Networks</h2><h3 id="unit-neurons">Unit (Neurons)</h3><ul><li><p>A unit often refers to the activation function in a layer by which the inputs are transformed via a nonlinear activation function (for example by the logistic sigmoid function). Usually, a unit has several incoming connections and several outgoing connections.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5697CF3A-4832-4849-98B9-3132500C6576.png" /></li></ul></li></ul><h3 id="input-layer">Input Layer</h3><ul><li>Comprised of multiple Real-Valued inputs. Each input must be linearly independent from each other.</li></ul><h3 id="hidden-layers">Hidden Layers</h3><ul><li><p>Layers other than the input and output layers. A layer is the highest-level building block in deep learning. A layer is a container that usually receives weighted input, transforms it with a set of mostly non-linear functions and then passes these values as output to the next layer.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/EB87C752-40AC-40E4-89BF-E85F3AA6AE97.png" /></li></ul></li></ul><h3 id="batch-normalization">Batch Normalization</h3><ul><li><p>Using mini-batches of examples, as opposed to one example at a time, is helpful in several ways. First, the gradient of the loss over a mini-batch is an estimate of the gradient over the training set, whose quality improves as the batch size increases. Second, computation over a batch can be much more efficient than m computations for individual examples, due to the parallelism afforded by the modern computing platforms.</p><ul><li>With SGD, the training proceeds in steps, and at each step we consider a mini- batch x1...m of size m. The mini-batch is used to approx- imate the gradient of the loss function with respect to the parameters.</li></ul></li></ul><h3 id="learning-rate">Learning Rate</h3><ul><li><p>Neural networks are often trained by gradient descent on the weights. This means at each iteration we use backpropagation to calculate the derivative of the loss function with respect to each weight and subtract it from that weight.</p><ul><li>However, if you actually try that, the weights will change far too much each iteration, which will make them “overcorrect” and the loss will actually increase/diverge. So in practice, people usually multiply each derivative by a small value called the “learning rate” before they subtract it from its corresponding weight.</li></ul></li><li><p>Tricks</p><ul><li><p>Simplest recipe: keep it fixed and use the same for all parameters.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/0FCF647C-94C1-42AA-B26D-BB5F5DB14248.png" /></li></ul></li><li><p>Better results by allowing learning rates to decrease Options:</p><ul><li><p>Reduce by 0.5 when validation error stops improving</p></li><li><p>Reduction by O(1/t) because of theoretical convergence guarantees, with hyper-parameters ε0 and τ and t is iteration numbers.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5100966F-881F-45C0-9D48-807D86657D02.png" /></li></ul></li><li><p>Better yet: No hand-set learning of rates by using AdaGrad</p></li></ul></li></ul></li></ul><h3 id="weight-initialization">Weight Initialization</h3><ul><li><p>All Zero Initialization</p><ul><li><p>In the ideal situation, with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative. A reasonable-sounding idea then might be to set all the initial weights to zero, which you expect to be the “best guess” in expectation.</p><ul><li>But, this turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during back-propagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.</li></ul></li></ul></li><li><p>Initialization with Small Random Numbers</p><ul><li><p>Thus, you still want the weights to be very close to zero, but not identically zero. In this way, you can random these neurons to small numbers which are very close to zero, and it is treated as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network.</p><ul><li>The implementation for weights might simply drawing values from a normal distribution with zero mean, and unit standard deviation. It is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.</li></ul></li></ul></li><li><p>Calibrating the Variances</p><ul><li><p>One problem with the above suggestion is that the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs. It turns out that you can normalize the variance of each neuron's output to 1 by scaling its weight vector by the square root of its fan-in (i.e., its number of inputs)</p><ul><li>This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence. The detailed derivations can be found from Page. 18 to 23 of the slides. Please note that, in the derivations, it does not consider the influence of ReLU neurons.</li></ul></li></ul></li></ul><h3 id="backpropagation">Backpropagation</h3><ul><li><p>Is a method used in artificial neural networks to calculate the error contribution of each neuron after a batch of data. It calculates the gradient of the loss function. It is commonly used in the gradient descent optimization algorithm. It is also called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers.</p><ul><li><p>Neural Network taking 4 dimension vector representation of words.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/76B7C54A-E48B-4D1B-A1CF-602F165D7C09.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/DB988698-5B05-4D61-8514-5DE7B085C286.png" /></li></ul></li></ul></li><li><p>In this method, we reuse partial derivatives computed for higher layers in lower layers, for efficiency.</p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/3DAD67C3-1FCB-475E-96B7-E2D2793A9FB6.png" /></p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F6753EA8-E4DB-4409-978F-AB2BCC323032.png" /></p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/4C67AB0A-2FB0-4B03-B9FA-E36203B3E37C.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/F340BAA3-345D-4D1E-9A6A-761D6BCF4E1F.png" /></li></ul></li></ul></li></ul></li></ul></li></ul><h3 id="activation-functions">Activation Functions</h3><ul><li><p>Defines the output of that node given an input or set of inputs.</p></li><li><p>Types</p><ul><li><p>ReLU</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B6CD5EBF-451C-4E1F-AA90-DFB9FD2165FE.png" /></li></ul></li><li><p>Sigmoid / Logistic</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D54BBB93-EE9A-433D-99ED-7D4C82B94AD2.png" /></li></ul></li><li><p>Binary</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C2E4A20B-CE21-414D-82F6-D179E30C7572.png" /></li></ul></li><li><p>Tanh</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/FCECCABF-A68B-421E-8498-EDF2D313371B.png" /></li></ul></li><li><p>Softplus</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C4BDC911-1EDC-44D1-AEFA-FDC64360BA6D.png" /></li></ul></li><li><p>Softmax</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D42822B8-CE14-4B16-A24D-8151614F601D.png" /></li></ul></li><li><p>Maxout</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/9E4B97B9-A6DC-4F0D-8D8A-7030819525B1.png" /></li></ul></li><li><p>Leaky ReLU, PReLU, RReLU, ELU, SELU, and others.</p></li></ul></li></ul><p>参考：<a href="https://github.com/dformoso/machine-learning-mindmap" target="_blank" rel="noopener" class="uri">https://github.com/dformoso/machine-learning-mindmap</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/22/ds/Process.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;Process&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="ML" scheme="https://2020.iosdevlog.com/tags/ML/"/>
    
      <category term="DS" scheme="https://2020.iosdevlog.com/tags/DS/"/>
    
  </entry>
  
</feed>
