<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Game 2020</title>
  
  <subtitle>https://2020.iosdevlog.com</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://2020.iosdevlog.com/"/>
  <updated>2020-03-21T15:52:24.999Z</updated>
  <id>https://2020.iosdevlog.com/</id>
  
  <author>
    <name>iOSDevLog</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>最有效的 Python 项目源码阅读方式</title>
    <link href="https://2020.iosdevlog.com/2020/03/21/pylint/"/>
    <id>https://2020.iosdevlog.com/2020/03/21/pylint/</id>
    <published>2020-03-21T14:54:29.000Z</published>
    <updated>2020-03-21T15:52:24.999Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><img src="https://2020.iosdevlog.com/2020/03/21/pylint/pylint.png" /></p><p>Pylint - Star your Python code!</p><!-- more --><p>众所周知，目前（2020年）人工智能项目大部分代码是使用 <code>Python</code> 语言编写的。</p><p>在发 <code>Paper</code> 的学术界有 <code>Pytorch</code>；在工业界部署 <code>First</code> 的工业界，这两个框架已经占据了深度学习的中心地位。</p><p>在机器学习领域有包含几乎所有常用机器学习算法的开源库：<code>sklean</code>。</p><p>如果遇到问题，找开源代码是参考是最简捷的方案。</p><p>目前大部分的人工智能相关项目的源码都是合适 <code>Python</code> 编写的，如何快速阅读这些源码呢？</p><p>这里推荐一套最有效的阅读 <code>Python</code> 源码的方式。</p><ol type="1"><li>下载相关源码：推荐去 <code>GitHub</code> 上下载，也可以用 <code>Chrome</code> 插件看</li><li>查看 <code>README.md</code> 和相关说明文档</li><li>参考 <code>Tutorials</code> 将代码跑起来</li><li>利用 <a href="https://www.pylint.org" target="_blank" rel="noopener">Pyreverse 包含在 Pypylint</a> 生成项目框架图</li><li>找到需要参考的代码，修改</li></ol><p>下面介绍一下如何生成 <code>manim</code> 项目框架图</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pylint</span></span><br><span class="line">pip install pylint</span><br><span class="line"><span class="comment"># touch ....../__init__.py</span></span><br><span class="line">pyreverse -A -o png -p manimlib manim/manimlib</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/21/pylint/pylint.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Pylint - Star your Python code!&lt;/p&gt;
&lt;!-- m
      
    
    </summary>
    
    
      <category term="Python" scheme="https://2020.iosdevlog.com/categories/Python/"/>
    
    
      <category term="Code" scheme="https://2020.iosdevlog.com/tags/Code/"/>
    
  </entry>
  
  <entry>
    <title>《统计学习方法》第 14 章 聚类方法 KMeans</title>
    <link href="https://2020.iosdevlog.com/2020/03/20/kmeans/"/>
    <id>https://2020.iosdevlog.com/2020/03/20/kmeans/</id>
    <published>2020-03-20T15:48:06.000Z</published>
    <updated>2020-03-20T15:52:10.458Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://2020.iosdevlog.com/2020/03/20/kmeans/kmeans.gif" /></p><a id="more"></a><h2 id="k-均值聚类">k-均值聚类</h2><p>n 个样本分到 k 个不同的类或簇，每个样本到其所属类的中心的距离最小。</p><p>每个样本只能属于一个类，所有 <strong>k-均值聚类</strong> 是 <strong>硬聚类</strong>。</p><h3 id="模型">模型</h3><ul><li>k &lt; n</li></ul><p><span class="math display">\[G_{i} \cap G_{j} = \varnothing, \bigcup_{i=1}^{k}G_{i} = X\]</span></p><!--more--><h3 id="策略">策略</h3><ul><li>距离： 欧式距离</li><li>损失函数：样本与所属类的中心的距离总保</li><li>NP 困难问题</li></ul><h3 id="算法">算法</h3><p>目标函数极小化</p><ol type="1"><li>初始化，随机取 $ k $ 个样本做中心</li><li>对样本进行聚类，计算样本到类中心距离，每个样本指派到与其最近的中心的类</li><li>计算新的类中心。对聚类结果计算样本的均值，做为新的类中心</li><li>如果迭代收敛或符合停止条件，输出。否则，令 $ t = t + 1 $ ，返回 2</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    self._setup_input(X)</span><br><span class="line">    n_samples, _ = X.shape</span><br><span class="line">    <span class="comment"># 从 X 中随机获取 k 个元素做中心</span></span><br><span class="line">    self._centers = np.array(</span><br><span class="line">        random.sample(list(np.unique(X, axis=<span class="number">0</span>)), self.k))</span><br><span class="line"></span><br><span class="line">    old_clusters = <span class="literal">None</span></span><br><span class="line">    n_iters = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        new_clusters = [self._min_k(x) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> new_clusters == old_clusters:</span><br><span class="line">            print(<span class="string">"Training finished after &#123;n_iters&#125; iterations!"</span>.format(n_iters=n_iters))</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        old_clusters = new_clusters</span><br><span class="line">        n_iters += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> cluster_i <span class="keyword">in</span> range(self.k):</span><br><span class="line">            <span class="comment"># 计算新的中心</span></span><br><span class="line">            points_idx = np.where(np.array(new_clusters) == cluster_i)</span><br><span class="line">            xi = X[points_idx]</span><br><span class="line">            self._centers[cluster_i] = xi.mean(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        self._centers_list.append(np.copy(self._centers))</span><br></pre></td></tr></table></figure><p>源码：<a href="https://github.com/iOSDevLog/slmethod" target="_blank" rel="noopener">https://github.com/iOSDevLog/slmethod</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/20/kmeans/kmeans.gif&quot; /&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学习方法" scheme="https://2020.iosdevlog.com/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="ML" scheme="https://2020.iosdevlog.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>《统计学习方法》导论 anim</title>
    <link href="https://2020.iosdevlog.com/2020/03/19/slm/"/>
    <id>https://2020.iosdevlog.com/2020/03/19/slm/</id>
    <published>2020-03-19T15:35:11.000Z</published>
    <updated>2020-03-19T15:40:27.739Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://2020.iosdevlog.com/2020/03/19/slm/Model.png" /></p><a id="more"></a><p><img src="https://2020.iosdevlog.com/2020/03/19/slm/Title.png" /><br /><img src="https://2020.iosdevlog.com/2020/03/19/slm/SupervisedStep.png" /><br /><img src="https://2020.iosdevlog.com/2020/03/19/slm/SupervisedLearning.png" /><br /><img src="https://2020.iosdevlog.com/2020/03/19/slm/Statisc.png" /><br /><img src="https://2020.iosdevlog.com/2020/03/19/slm/End.png" /></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/19/slm/Model.png&quot; /&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="manim" scheme="https://2020.iosdevlog.com/tags/manim/"/>
    
      <category term="统计学习方法" scheme="https://2020.iosdevlog.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>AI开发日志 代码资源等</title>
    <link href="https://2020.iosdevlog.com/2020/03/18/AIDevLog/"/>
    <id>https://2020.iosdevlog.com/2020/03/18/AIDevLog/</id>
    <published>2020-03-18T15:44:51.000Z</published>
    <updated>2020-03-18T16:30:41.907Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/18/AIDevLog/Issues.png" alt="" /><figcaption>Issues</figcaption></figure><a id="more"></a><h2 id="云社区">云<span class="math inline">\(^{+}\)</span>社区</h2><p>昨天收到一个来自 云<span class="math inline">\(^{+}\)</span>社区 的小礼物：一个公仔，一张腾讯视频 VIP 月卡。</p><p>我记得是从简书抓取的文章，我 <a href="https://www.jianshu.com/u/1577b98c62f4" target="_blank" rel="noopener">简书</a> 也有很长时间没有更新。</p><figure><img src="https://2020.iosdevlog.com/2020/03/18/AIDevLog/Tencent.png" alt="" /><figcaption>Tencent</figcaption></figure><h2 id="github-ios">GitHub iOS</h2><p>今天（2020-03-18）收到 GitHub 邮件，说 iOS 版正式发布了。</p><p>Thanks for signing up for the GitHub for mobile beta. The Android and iOS versions are now out of beta and generally available! Head over to Google Play or the App Store to start using the releases.</p><p><a href="https://github.us11.list-manage.com/track/click?u=9d7ced8c4bbd6c2f238673f0f&amp;id=8c49f60050&amp;e=ab5005febc" target="_blank" rel="noopener">Download for Android</a></p><p><a href="https://github.us11.list-manage.com/track/click?u=9d7ced8c4bbd6c2f238673f0f&amp;id=a5ee567519&amp;e=ab5005febc" target="_blank" rel="noopener">Download for iOS</a></p><h3 id="with-github-for-mobile-you-can">With GitHub for mobile, you can: </h3><ul><li><strong>Organize tasks in a swipe:</strong> Get your inbox to zero in no time—swipe to finish a task or save the notification to return to it later.</li><li><strong>Give feedback and respond to issues:</strong> Respond to comments while you’re on the go.</li><li><strong>Review and merge pull requests:</strong> Merge and mark pull requests to breeze through your workflow, wherever you are.</li></ul><p><a href="https://github.us11.list-manage.com/track/click?u=9d7ced8c4bbd6c2f238673f0f&amp;id=69c429f973&amp;e=ab5005febc" target="_blank" rel="noopener">Learn more about GitHub for mobile</a></p><figure><img src="https://2020.iosdevlog.com/2020/03/18/AIDevLog/GitHub.png" alt="" /><figcaption>GitHub</figcaption></figure><p>下载体验后，<code>Issues</code> 里面发现 <a href="https://github.com/iOSDevLog/AIDevLog/" target="_blank" rel="noopener">AIDevLog</a> 里面我从 <a href="https://github.com/iOSDevLog/AIDevLog/projects" target="_blank" rel="noopener">Project</a> 生成的 <a href="https://github.com/iOSDevLog/AIDevLog/issues/10" target="_blank" rel="noopener">Issues</a> 已经是 10 个月前。</p><figure><img src="https://2020.iosdevlog.com/2020/03/18/AIDevLog/Projects.png" alt="" /><figcaption>Issues</figcaption></figure><h2 id="消失-10-个月的-aidevlog">消失 10 个月的 AIDevLog</h2><figure><img src="https://2020.iosdevlog.com/2020/03/18/AIDevLog/AIDevLog.png" alt="" /><figcaption>AIDevLog</figcaption></figure><ol type="1"><li>预备知识<ol type="1"><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/Python%20基础/README.md" target="_blank" rel="noopener">Python 基础</a></li><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/数学基础/README.md" target="_blank" rel="noopener">数学基础</a></li></ol></li><li>机器学习<ol type="1"><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/统计学习方法/README.md" target="_blank" rel="noopener">统计学习方法</a></li><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/西瓜书/README.md" target="_blank" rel="noopener">统计学习（西瓜书）</a></li><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/机器学习实战/README.md" target="_blank" rel="noopener">机器学习实战</a></li></ol></li><li>深度学习<ol type="1"><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/花书/README.md" target="_blank" rel="noopener">深度学习（花书）</a></li><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/Tensorflow/README.md" target="_blank" rel="noopener">Tensorflow</a></li><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/PyTorch/README.md" target="_blank" rel="noopener">PyTorch</a></li><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/计算机视觉/README.md" target="_blank" rel="noopener">CS231n: 计算机视觉</a></li><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/自然语言处理/README.md" target="_blank" rel="noopener">CS224n: 自然语言处理</a></li></ol></li><li>强化学习<ol type="1"><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/简易%20AlphaGo/README.md" target="_blank" rel="noopener">简易 AlphaGo</a></li></ol></li><li>竞赛<ol type="1"><li><a href="https://github.com/iOSDevLog/AIDevLog/blob/master/Kaggle/README.md" target="_blank" rel="noopener">Kaggle</a></li></ol></li></ol><h3 id="进度">进度</h3><p><a href="https://github.com/iOSDevLog/AIDevLog/projects?query=is%3Aopen+sort%3Aname-asc" target="_blank" rel="noopener">https://github.com/iOSDevLog/AIDevLog/projects</a></p><h3 id="联系方式">联系方式</h3><p>网站: <a href="https://2019.iosdevlog.com/" target="_blank" rel="noopener">http://2019.iosdevlog.com/</a></p><p>GitHub: <a href="https://github.com/iOSDevLog/AIDevLog" target="_blank" rel="noopener">https://github.com/iOSDevLog/AIDevLog</a></p><blockquote><p>不积跬步，无以至千里；不积小流，无以成江海</p></blockquote><p>——荀子《劝学篇》</p><p>计划做的再好，没有去实施也终究是计划。</p><p>明日开始在公众号里面更新。</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/18/AIDevLog/Issues.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;Issues&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="game" scheme="https://2020.iosdevlog.com/categories/game/"/>
    
    
      <category term="Godot" scheme="https://2020.iosdevlog.com/tags/Godot/"/>
    
  </entry>
  
  <entry>
    <title>微积分的本质（Essence of calculus)</title>
    <link href="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/"/>
    <id>https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/</id>
    <published>2020-03-17T07:30:34.000Z</published>
    <updated>2020-03-17T15:44:45.779Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/1.png" alt="" /><figcaption>Essence of calculus</figcaption></figure><a id="more"></a><h2 id="概论">01. 概论</h2><p>圆的面积</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Circle.png" alt="" /><figcaption>Circle</figcaption></figure><h2 id="导数的悖论">02. 导数的悖论</h2><p>微积分之父</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Fathers_of_Calculus.png" alt="" /><figcaption>Fathers_of_Calculus</figcaption></figure><p>汽车</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/dt.png" alt="" /><figcaption>dt</figcaption></figure><h2 id="用几何来求导">03. 用几何来求导</h2><p>应用</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Applications.png" alt="" /><figcaption>Applications</figcaption></figure><p><span class="math inline">\(x^{2}\)</span></p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/2.png" alt="" /><figcaption>2</figcaption></figure><p><span class="math inline">\(x^{3}\)</span></p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/3.png" alt="" /><figcaption>3</figcaption></figure><p><span class="math inline">\(x^{n}\)</span></p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/n.png" alt="" /><figcaption>n</figcaption></figure><p><span class="math inline">\(\sin(x)\)</span></p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Sin.png" alt="" /><figcaption>Sin</figcaption></figure><p><span class="math inline">\(\frac{1}{x})\)</span></p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/x.png" /></p><h2 id="直观理解链式法则和乘积法则">04. 直观理解链式法则和乘积法则</h2><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Chain.png" alt="" /><figcaption>Chain</figcaption></figure><p>组合函数 求导</p><ul><li>加 Sum</li><li>乘 Product</li><li>复合 Composition</li></ul><p>加法法则</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Sum.png" alt="" /><figcaption>Sum</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Sum_rule.png" alt="" /><figcaption>Sum rule</figcaption></figure><p>乘积法则</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Product.png" alt="" /><figcaption>Product</figcaption></figure><p>复合：链式法则</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Composition.png" alt="" /><figcaption>Composition</figcaption></figure><h2 id="指数函数求导">05. 指数函数求导</h2><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/e.png" alt="" /><figcaption>e</figcaption></figure><p>2的指数 -&gt; e表示</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/ln2.png" alt="" /><figcaption>ln2</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/ln.png" alt="" /><figcaption>ln</figcaption></figure><h2 id="隐函数求导是怎么回事">06. 隐函数求导是怎么回事</h2><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/hide.png" alt="" /><figcaption>hide</figcaption></figure><p>变化落在函数曲线上</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/1_x.png" alt="" /><figcaption>1_x</figcaption></figure><h2 id="极限">07. 极限</h2><p>目标</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Goal.png" alt="" /><figcaption>Goal</figcaption></figure><p>导数定义</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Derivative.png" alt="" /><figcaption>Derivative</figcaption></figure><p><span class="math inline">\(dx\)</span> -&gt; <span class="math inline">\(h\)</span></p><ol type="1"><li>明确表示 <span class="math inline">\(h\)</span> 是一个普通的数，与无穷小无关</li><li><span class="math inline">\(dx\)</span> 就是无穷小量</li></ol><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Limit.png" alt="" /><figcaption>Limit</figcaption></figure><p><span class="math inline">\(\frac{0}{0}\)</span></p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/0_0.png" alt="" /><figcaption>0_0</figcaption></figure><h2 id="积分与微积分的基本定理">08. 积分与微积分的基本定理</h2><p><code>积分</code></p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Calculus.png" alt="" /><figcaption>Calculus</figcaption></figure><p><code>积分：求导的逆运算</code></p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Inverse.png" alt="" /><figcaption>Inverse</figcaption></figure><p><code>积分</code>：小量累积</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Integrates.png" alt="" /><figcaption>Integrates</figcaption></figure><p>微积分基本定理</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Theorem.png" alt="" /><figcaption>Theorem</figcaption></figure><p>原函数 &lt;==&gt; 导数</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Origin.png" alt="" /><figcaption>Origin</figcaption></figure><h2 id="面积与斜率有什么关系">09. 面积与斜率有什么关系</h2><p>平均值</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Average.png" alt="" /><figcaption>Average</figcaption></figure><h2 id="脚注-高阶导数">09脚注-高阶导数</h2><p>二阶导数</p><ul><li>导数的导数</li><li>变化量的变化量</li></ul><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Second_derivatives.png" alt="" /><figcaption>Second_derivatives</figcaption></figure><p>高阶导数</p><figure><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Higher_orders.png" alt="" /><figcaption>Higher_orders</figcaption></figure><h2 id="泰勒级数">10. 泰勒级数</h2><p>泰勒级数是利用函数某单个点的导数来近似这个点附近函数的值</p><p>用多项式 <code>近似</code> 其它函数</p><ul><li>好计算</li><li>好求导</li><li>好积分</li></ul><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/cos1.png" /></p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/cos2.png" /></p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/cos3.png" /></p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/cos4.png" /></p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/cos5.png" /></p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/cos6.png" /></p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/cos7.png" /></p><ol type="1"><li><span class="math inline">\(c_{0}\)</span>：多项式在 x=0 处与 cos(0) 相等</li><li><span class="math inline">\(c_{1}\)</span>：两者导数一致</li><li><span class="math inline">\(c_{2}\)</span>：两者二阶导数一致</li></ol><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/not_affect.png" /></p><p>控制</p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Control.png" /></p><p>泰勒多项式</p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Taylor.png" /></p><p>泰勒公式</p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Taylor_formula.png" /></p><p><span class="math inline">\(e^{x}\)</span> 泰勒多项式</p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Taylor_e.png" /></p><p>几何表示</p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Taylor_geometry.png" /></p><p>泰勒级数</p><ul><li>泰勒多项：有限多项</li><li>泰勒级数：无阶多项</li></ul><p>收敛</p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Converge.png" /></p><p>发散</p><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Diverges.png" /></p><h2 id="你在微积分课上学不到的知识">11. 你在微积分课上学不到的知识</h2><p><img src="https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/Knowledge.png" /></p><p>在这里观看完整的“微积分的本质”播放列表：&lt;hhttp://3b1b.co/calculus&gt;</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/17/Essence-of-calculus/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;Essence of calculus&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="math" scheme="https://2020.iosdevlog.com/categories/math/"/>
    
    
      <category term="manim" scheme="https://2020.iosdevlog.com/tags/manim/"/>
    
      <category term="3b1b" scheme="https://2020.iosdevlog.com/tags/3b1b/"/>
    
      <category term="calculus" scheme="https://2020.iosdevlog.com/tags/calculus/"/>
    
  </entry>
  
  <entry>
    <title>线性代数的本质（Essence of linear algebra）</title>
    <link href="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/"/>
    <id>https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/</id>
    <published>2020-03-16T14:24:25.000Z</published>
    <updated>2020-03-17T08:31:41.862Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/1.png" /></p><a id="more"></a><h2 id="向量">向量</h2><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Vector.png" /></p><p>空间的一组基的严格定义是这样的:</p><blockquote><p>张成该空间的一个线性无关向量的集合</p></blockquote><ul><li>单个向量：箭头表示</li><li>多个向量：点表示</li></ul><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Vector3.png" /></p><h2 id="变换">变换</h2><p><span class="math display">\[变换 == 函数\]</span></p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Linear_transformation.png" /></p><p><span class="math display">\[y = f(x)\]</span></p><ul><li>IN: 向量(x)</li><li>OUT： 向量(y)</li></ul><h3 id="为什么叫-变换">为什么叫 <code>变换</code></h3><blockquote><p>因为使用“变换”是在暗示以特定方式来可视化这一输入-输出关系</p></blockquote><p>运动 =&gt; 向量的函数</p><h3 id="线性变换">线性变换</h3><ol type="1"><li>直线在变换后仍然保持为直线,不能有所弯曲</li><li>原点必须保持固定</li></ol><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/base.png" /></p><p>基向量</p><figure><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Matrix_multiplication.png" alt="" /><figcaption>直观</figcaption></figure><p>矩阵乘法</p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Composition.png" /></p><p>复合变换 -&gt; 复合函数</p><h2 id="行列式">行列式</h2><blockquote><p>线性变换改变面积的比例。</p></blockquote><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Determinant.png" /></p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Det0.png" /></p><ul><li>行列式为 <span class="math inline">\(0\)</span> =&gt; 空间压缩到更小的维度</li><li>当空间定向改变的情况发生时,行列式为 <code>负</code></li></ul><h3 id="计算行列式">计算行列式</h3><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Det1.png" /></p><ul><li><span class="math inline">\(a\)</span>： <span class="math inline">\(x\)</span> 轴伸缩比例</li><li><span class="math inline">\(b\)</span>： 对角方向 拉伸/压缩 比例</li><li><span class="math inline">\(c\)</span>： 对角方向 拉伸/压缩 比例</li><li><span class="math inline">\(d\)</span>： <span class="math inline">\(y\)</span> 轴伸缩比例</li></ul><h3 id="二阶行列式">二阶行列式</h3><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Det2.png" /></p><h3 id="三阶行列式">三阶行列式</h3><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Det3.png" /></p><p>平行六面体的体积</p><h2 id="矩阵">矩阵</h2><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Matrix.png" /></p><p><span class="math inline">\(A\)</span> 代表变换矩阵</p><h3 id="逆矩阵">逆矩阵</h3><p>恒等变换：单位矩阵</p><p><span class="math display">\[A^{-1}A = I\]</span></p><p>逆乘以 <span class="math inline">\(A\)</span></p><ul><li><code>列空间</code>：矩阵的列所张成的空间</li><li><code>秩</code>：列空间的维数（变换后空间的维数）</li></ul><h3 id="矩阵乘法">矩阵乘法</h3><p>变换矩阵</p><ul><li><code>行</code>：变换后的独立坐标</li><li><code>列</code>：变换前的基向量</li><li><code>零空间/核</code>：变换后落在 <code>原点</code> 的向量的集合</li></ul><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/2to3.png" /></p><p>2 列 -&gt; 3 行<br />=&gt;<br />2 维 -&gt; 3 维</p><h2 id="点积数量积标量积">点积（数量积/标量积）</h2><blockquote><p>一种接受两个等长的数字序列（通常是坐标向量）、返回单个数字的代数运算</p></blockquote><ul><li>内积：两个笛卡尔坐标向量的点积常称为内积</li></ul><p>代数定义</p><p><span class="math display">\[\vec{a} \cdot \vec{b} = \sum_{i=1}^n a_ib_i = a_1b_1 + a_2b_2 + \cdots + a_nb_n\]</span></p><p><span class="math display">\[\vec{a} \cdot \vec{b} = \vec{a}\vec{b}^T\]</span></p><p>几何定义</p><p><span class="math display">\[\vec{a} \cdot \vec{b} = |\vec{a}| \, |\vec{b}| \cos \theta \;\]</span></p><ul><li><span class="math inline">\(|\vec{a}|\)</span>：模长</li><li><span class="math inline">\(\theta\)</span>：夹角</li></ul><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Dot_product_1.png" /></p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Dot_product_2.png" /></p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Dot_product.png" /></p><h2 id="叉积">叉积</h2><p>面积</p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Cross_product.png" /></p><p>求行列式</p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Det.png" /></p><p>方向（右手定则）</p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Cross_product_direction.png" /></p><h2 id="特征向量-和-特征值-eigenvectors-and-eigenvalues">特征向量 和 特征值 Eigenvectors and eigenvalues</h2><ul><li>特征向量：线性变换[a]之后，得到的新向量仍然与原来的向量保持在同一条直线上</li></ul><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Eigenvectors.png" /></p><ul><li>特征值：特征向量的长度在该线性变换下 <strong>缩放</strong> 的比例</li></ul><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Eigenvalue.png" /></p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Eigenvectors_formula.png" /></p><ul><li>特征基：一组基向量（也是特征向量）构成的集合</li></ul><h2 id="抽象向量空间">抽象向量空间</h2><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Coordinate.png" /></p><p><code>函数</code> =&gt; 向量特性</p><figure><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Function_addition.png" alt="" /><figcaption>Addtion</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Function_product.png" alt="" /><figcaption>Scaling</figcaption></figure><p>向量</p><figure><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Vector_Operation.png" alt="" /><figcaption>Vector_Operation</figcaption></figure><p>函数</p><figure><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Function_Operation.png" alt="" /><figcaption>Function_Operation</figcaption></figure><p>导数：函灵敏的线性变换</p><figure><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Derivative.png" alt="" /><figcaption>Derivative</figcaption></figure><p>线性</p><blockquote><p>线性变换保持向量 <em>加法</em> 运算和 <em>数乘</em> 运算</p></blockquote><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Linear.png" /></p><p>求导是线性的</p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Derivative_addition.png" /></p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Derivative_multiplication.png" /></p><p>基函数</p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Base_function.png" /></p><p>矩阵求导</p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Matrix_derivative.png" /></p><p>线性代数 vs 函数</p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Linear_function.png" /></p><p>向量空间</p><p>类似向量的事物</p><ul><li>箭头 （物理）</li><li>一组数 （计算机）</li><li>函数</li><li>...</li></ul><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Vector_Spaces.png" /></p><p>公理</p><p><img src="https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/Axioms.png" /></p><p>在这里观看完整的“线性代数的本质”播放列表：<a href="https://goo.gl/R1kBdb" target="_blank" rel="noopener" class="uri">https://goo.gl/R1kBdb</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/16/Essence-of-linear-algebra/1.png&quot; /&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="math" scheme="https://2020.iosdevlog.com/categories/math/"/>
    
    
      <category term="manim" scheme="https://2020.iosdevlog.com/tags/manim/"/>
    
      <category term="linear" scheme="https://2020.iosdevlog.com/tags/linear/"/>
    
      <category term="3b1b" scheme="https://2020.iosdevlog.com/tags/3b1b/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法之旅</title>
    <link href="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/"/>
    <id>https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/</id>
    <published>2020-03-15T13:49:49.000Z</published>
    <updated>2020-03-15T15:20:59.941Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/ensemble.png" /></p><p>最合适的线条组合的一个很好的例子。弱预测成员为灰色，组合预测为红色。</p><p>绘图来自Wikipedia，在公共领域获得许可。</p><a id="more"></a><p>在本文中，我们将浏览<strong><em>最流行的机器学习算法</em></strong>。</p><p>考察该领域的主要算法对了解可用的方法很有用。</p><p>有太多算法，当抛出算法名称时会感到不知所措，并且您应该只知道它们是什么以及它们适合什么位置。</p><p>我想给您提供两种方式来考虑和分类您在该领域可能遇到的算法。</p><ul><li>首先是按照<strong>学习风格</strong>对算法进行分组。</li><li>第二个是按形式或功能上的<strong>相似性</strong>将算法分组（例如将相似的动物分组在一起）。</li></ul><p>两种方法都是有用的，但是我们将着重于通过相似性进行算法分组，并浏览各种不同的算法类型。</p><p>阅读这篇文章后，您将对用于监督学习的最受欢迎的机器学习算法以及它们之间的关系有更好的了解。</p><p><a href="https://machinelearningmastery.com/master-machine-learning-algorithms/" target="_blank" rel="noopener">在我的新书中</a>（包括22个excel教程和示例），了解机器学习算法如何工作，包括kNN，决策树，朴素贝叶斯，SVM，集成等。</p><p>让我们开始吧。</p><h2 id="按学习风格分组的算法">按学习风格分组的算法</h2><p>算法可以根据其与经验或环境的交互作用或我们要调用输入数据的方式，以不同的方式对问题建模。</p><p>首先要考虑算法可以采用的学习方式，这在机器学习和人工智能教科书中很普遍。</p><p>一个算法只能有几种主要的学习方式或学习模型，我们将在此通过一些适合他们的算法和问题类型的示例进行介绍。</p><p>这种分类法或组织机器学习算法的方法很有用，因为它迫使您考虑输入数据的角色和模型准备过程，并选择最适合您问题的模型，以获得最佳结果。</p><h3 id="让我们看一下机器学习算法中的三种不同的学习风格">让我们看一下机器学习算法中的三种不同的学习风格：</h3><h4 id="监督学习"><strong>1.监督学习</strong></h4><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Supervised-Learning-Algorithms.png" /></p><p>输入数据称为训练数据，并且一次具有已知标签或结果，例如垃圾邮件/非垃圾邮件或股票价格。</p><p>通过训练过程来准备模型，其中需要进行预测，并在这些预测错误时进行校正。训练过程将继续进行，直到模型在训练数据上达到所需的准确性水平为止。</p><p>示例问题是 <strong>分类</strong> 和 <strong>回归</strong>。</p><p>示例算法包括：<strong>逻辑回归</strong> 和 <strong>反向传播神经网络</strong>。</p><h4 id="无监督学习"><strong>2.无监督学习</strong></h4><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Unsupervised-Learning-Algorithms.png" /></p><p>输入数据未标记，结果未知。</p><p>通过推导输入数据中存在的结构来准备模型。这可能是提取一般规则。可以通过数学过程来系统地减少冗余，也可以通过相似性组织数据。</p><p>示例问题包括 <strong>聚类</strong>，<strong>降维</strong> 和 <strong>关联规则学习</strong>。</p><p>示例算法包括：<strong>Apriori算法</strong> 和 <strong>K-Means</strong>。</p><h4 id="半监督学习"><strong>3.半监督学习</strong></h4><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Semi-supervised-Learning-Algorithms.png" /></p><p>输入数据是带标签和未带标签的示例的混合。</p><p>存在一个期望的预测问题，但是模型必须学习用于组织数据以及进行预测的结构。</p><p>示例问题是 <strong>分类</strong> 和 <strong>回归</strong>。</p><p>示例算法是对其他灵活方法的扩展，这些方法对如何建模未标记数据进行了假设。</p><h3 id="机器学习算法概述">机器学习算法概述</h3><p>处理数据以对业务决策进行建模时，最典型的情况是您使用有监督和无监督的学习方法。</p><p>目前最热门的话题是 <strong>图像分类</strong> 等领域的 <code>半监督</code> 学习方法，在这些领域中，大型数据集的示例很少。</p><h2 id="相似度分组算法">相似度分组算法</h2><p>算法通常在功能（工作方式）方面按相似性分组。例如，基于树的方法和基于神经网络的方法。</p><p>我认为这是对算法进行分组的最有用的方法，也是我们将在此处使用的方法。</p><p>这是一种有用的分组方法，但并不完美。仍然有一些算法可以很容易地适合多个类别，例如“学习矢量量化”既是神经网络启发性方法又是基于实例的方法。也有描述问题的同名类别和算法类别，例如回归和聚类。</p><p>我们可以通过两次列出算法或选择主观上 <em>最</em> 适合的组来处理这些情况。我喜欢后一种方法，即不重复算法以保持简单。</p><p>在本节中，我们列出了许多流行的机器学习算法，这些算法按照我们认为最直观的方式进行了分组。该列表在组或算法中都不是详尽无遗的，但我认为它是具有代表性的，对您了解土地状况将很有用。</p><blockquote><p><strong>请注意</strong>：用于分类和回归的算法有很大的偏见，这是您将遇到的两个最普遍的监督式机器学习问题。</p></blockquote><p>如果您知道某个算法或一组未列出的算法，请在注释中添加并与我们分享。让我们潜入。</p><h3 id="回归算法">回归算法</h3><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Regression-Algorithms.png" /></p><p>回归模型涉及对变量之间的关系进行建模，这些变量之间的关系使用模型进行的预测中的误差度量进行了迭代完善。</p><p>回归方法是统计工作的主力军，已被选入统计机器学习中。这可能会造成混淆，因为我们可以使用回归来指代问题的类别和算法的类别。确实，回归是一个过程。</p><p>最受欢迎的回归算法是：</p><ul><li>普通最小二乘回归（OLSR）</li><li>线性回归</li><li>逻辑回归</li><li>逐步回归</li><li>多元自适应回归样条（MARS）</li><li>局部估计的散点图平滑（LOESS）</li></ul><h3 id="基于实例的算法">基于实例的算法</h3><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Instance-based-Algorithms.png" /></p><p>基于实例的学习模型是一个决策问题，其中包含训练数据的实例或示例，这些实例或示例被认为对该模型很重要或需要。</p><p>这样的方法通常建立示例数据的数据库，并使用相似性度量将新数据与数据库进行比较，以便找到最佳匹配并做出预测。因此，基于实例的方法也称为获胜者通吃方法和基于内存的学习。重点放在存储实例的表示以及实例之间使用的相似性度量上。</p><p>最受欢迎的基于实例的算法是：</p><ul><li>k最近邻居（kNN）</li><li>学习矢量量化（LVQ）</li><li>自组织图（SOM）</li><li>本地加权学习（LWL）</li><li>支持向量机（SVM）</li></ul><h3 id="正则化算法">正则化算法</h3><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Regularization-Algorithms.png" /></p><p>对另一种方法（通常是回归方法）的扩展，该方法根据模型的复杂性对模型进行惩罚，而倾向于更易于泛化的简单模型。</p><p>我在这里单独列出了正则化算法，因为它们是对其他方法的流行，功能强大且通常简单的修改。</p><p>最受欢迎的正则化算法是：</p><ul><li>岭回归</li><li>最小绝对收缩和选择算子（LASSO）</li><li>弹性网</li><li>最小角度回归（LARS）</li></ul><h3 id="决策树算法">决策树算法</h3><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Decision-Tree-Algorithms.png" /></p><p>决策树方法构建了一个基于数据中属性的实际值制定的决策模型。</p><p>决策派生到树结构中，直到为给定记录做出预测决策为止。对决策树进行有关分类和回归问题的数据训练。决策树通常快速，准确，并且在机器学习中大受欢迎。</p><p>最受欢迎的决策树算法是：</p><ul><li>分类和回归树（CART）</li><li>迭代二叉树 3 代（ID3）</li><li>C4.5和C5.0（功能强大的方法的不同版本）</li><li>卡方自动互动检测（CHAID）</li><li>决策树桩</li><li>M5</li><li>条件决策树</li></ul><h3 id="贝叶斯算法">贝叶斯算法</h3><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Bayesian-Algorithms.png" /></p><p>贝叶斯方法是将贝叶斯定理明确应用于分类和回归等问题的方法。</p><p>最受欢迎的贝叶斯算法是：</p><ul><li>朴素贝叶斯</li><li>高斯朴素贝叶斯</li><li>多项式朴素贝叶斯</li><li>平均一依赖估计量（AODE）</li><li>贝叶斯信仰网络（BBN）</li><li>贝叶斯网络（BN）</li></ul><h3 id="聚类算法">聚类算法</h3><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Clustering-Algorithms.png" /></p><p>像回归一样，聚类描述问题的类别和方法的类别。</p><p>聚类方法通常通过建模方法（例如基于质心和层次的方法）进行组织。所有方法都涉及使用数据中的固有结构来最好地将数据组织成具有最大共性的组。</p><p>最受欢迎的聚类算法是：</p><ul><li>k均值 k-Means</li><li>k中位数 k-Medians</li><li>期望最大化（EM）</li><li>层次聚类</li></ul><h3 id="关联规则学习算法">关联规则学习算法</h3><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Assoication-Rule-Learning-Algorithms.png" /></p><p>关联规则学习方法提取的规则可以最好地解释数据中变量之间观察到的关系。</p><p>这些规则可以在组织可以利用的大型多维数据集中发现重要的商业上有用的关联。</p><p>最受欢迎的关联规则学习算法是：</p><ul><li>Apriori 算法</li><li>Eclat 算法</li></ul><h3 id="人工神经网络算法">人工神经网络算法</h3><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Artificial-Neural-Network-Algorithms.png" /></p><p>人工神经网络是受生物神经网络的结构和/或功能启发的模型。</p><p>它们是一类模式匹配，通常用于回归和分类问题，但实际上是一个巨大的子领域，由数百种算法和各种问题类型的变体组成。</p><p>请注意，由于该领域的迅速发展和普及，我将深度学习与神经网络分开了。在这里，我们关注更经典的方法。</p><p>最受欢迎的人工神经网络算法是：</p><ul><li>感知器</li><li>多层感知器（MLP）</li><li>反向传播</li><li>随机梯度下降</li><li>霍普菲尔德网络</li><li>径向基函数网络（RBFN）</li></ul><h3 id="深度学习算法">深度学习算法</h3><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Deep-Learning-Algorithms.png" /></p><p><a href="https://machinelearningmastery.com/what-is-deep-learning/" target="_blank" rel="noopener">深度学习</a>方法是对利用大量廉价计算的人工神经网络的一种现代更新。</p><p>他们关注的是构建更大，更复杂的神经网络，并且如上所述，许多方法都涉及标记的模拟数据（例如图像，文本）的超大型数据集。音频和视频。</p><p>最受欢迎的深度学习算法是：</p><ul><li>卷积神经网络（CNN）</li><li>递归神经网络（RNN）</li><li>长短期记忆网络（LSTM）</li><li>堆叠式自动编码器</li><li>深玻尔兹曼机（DBM）</li><li>深度信仰网络（DBN）</li></ul><h3 id="降维算法">降维算法</h3><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Dimensional-Reduction-Algorithms.png" /></p><p>像聚类方法一样，降维会寻找和利用数据中的固有结构，但是在这种情况下，将以无监督的方式或顺序使用较少的信息来汇总或描述数据。</p><p>这对于可视化尺寸数据或简化可以在监督学习方法中使用的数据很有用。这些方法中的许多方法都可以用于分类和回归。</p><ul><li>主成分分析（PCA）</li><li>主成分回归（PCR）</li><li>偏最小二乘回归（PLSR）</li><li>萨蒙地图</li><li>多维缩放（MDS）</li><li>投影追踪</li><li>线性判别分析（LDA）</li><li>混合判别分析（MDA）</li><li>二次判别分析（QDA）</li><li>弹性判别分析（FDA）</li></ul><h3 id="集成学习算法">集成学习算法</h3><p><img src="https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/Ensemble-Algorithms.png" /></p><p>集合方法是由多个较弱的模型组成的模型，这些模型经过独立训练，其预测以某种方式组合在一起以进行总体预测。</p><p>对于要组合哪些类型的弱学习者以及如何将它们组合在一起，需要付出很多努力。这是一类非常强大的技术，因此非常受欢迎。</p><ul><li>Boosting</li><li>Bootstrapped Aggregation (Bagging)</li><li>AdaBoost</li><li>加权平均值（混合）</li><li>堆叠泛化（堆叠）Stacked Generalization (Stacking)</li><li>梯度提升机（GBM）Gradient Boosting Machines (GBM)</li><li>梯度增强回归树（GBRT）Gradient Boosting Machines (GBM)</li><li>随机森林</li></ul><h3 id="其他机器学习算法">其他机器学习算法</h3><p>许多算法没有涵盖。</p><p>在机器学习过程中，我没有涉及特殊任务的算法，例如：</p><ul><li>特征选择算法</li><li>算法精度评估</li><li>绩效指标</li><li>优化算法</li></ul><p>我也没有涵盖来自机器学习专业子领域的算法，例如：</p><ul><li>计算智能（进化算法等）</li><li>计算机视觉（CV）</li><li>自然语言处理（NLP）</li><li>推荐系统</li><li>强化学习</li><li>图形模型</li><li>更多…</li></ul><p>这些可能会在以后的帖子中出现。</p><h2 id="机器学习算法的进一步阅读">机器学习算法的进一步阅读</h2><p>这次机器学习算法之旅旨在为您提供概述，以及有关如何将算法相互关联的一些想法。</p><p>我已经收集了一些资源，供您继续阅读算法。如果您有特定问题，请发表评论。</p><h3 id="机器学习算法的其他列表">机器学习算法的其他列表</h3><p>如果您有兴趣，还有很多其他的算法列表。以下是一些精选示例。</p><ul><li><a href="http://en.wikipedia.org/wiki/List_of_machine_learning_algorithms" target="_blank" rel="noopener">机器学习算法列表</a>：在Wikipedia上。尽管内容广泛，但我认为此列表或算法的组织并没有特别有用。</li><li><a href="http://en.wikipedia.org/wiki/Category:Machine_learning_algorithms" target="_blank" rel="noopener">机器学习算法类别</a>：也在Wikipedia上，比上面的Wikipedia很棒的列表稍微有用。它按字母顺序组织算法。</li><li><a href="http://cran.r-project.org/web/views/MachineLearning.html" target="_blank" rel="noopener">CRAN任务视图：机器学习和统计学习</a>：R中每个机器学习软件包所支持的所有软件包和所有算法的列表。它使您对现有内容以及人们日常使用的分析有扎实的感觉。</li><li><a href="http://www.cs.uvm.edu/~icdm/algorithms/index.shtml" target="_blank" rel="noopener">数据挖掘中的十大算法</a>：已<a href="http://link.springer.com/article/10.1007/s10115-007-0114-2" target="_blank" rel="noopener">发表的文章</a>，现在是一<a href="http://www.amazon.com/dp/1420089641?tag=inspiredalgor-20" target="_blank" rel="noopener">本有关最流行的数据挖掘算法的书</a>（会员链接）。另一种扎根但不太压倒性的方法可以让您深入学习。</li></ul><h3 id="如何学习机器学习算法">如何学习机器学习算法</h3><p>算法是机器学习的重要组成部分。这是我热衷的话题，并在此博客上写了很多。以下是一些可能会吸引您进一步阅读的精选帖子。</p><ul><li><a href="http://machinelearningmastery.com/how-to-learn-a-machine-learning-algorithm/" target="_blank" rel="noopener">如何学习任何机器学习算法</a>：一种可以使用“算法描述模板”来学习和理解任何机器学习算法的系统方法（我用这种方法写了<a href="http://cleveralgorithms.com/nature-inspired/index.html" target="_blank" rel="noopener">第一本书</a>）。</li><li><a href="http://machinelearningmastery.com/create-lists-of-machine-learning-algorithms/" target="_blank" rel="noopener">如何创建机器学习算法的目标列表</a>：如何创建自己的机器学习算法的系统列表，以开始处理下一个机器学习问题。</li><li><a href="http://machinelearningmastery.com/how-to-research-a-machine-learning-algorithm/" target="_blank" rel="noopener">如何研究机器学习算法</a>：您可以用来研究机器学习算法的系统方法（与上面列出的模板方法协同工作非常有效）。</li><li><a href="http://machinelearningmastery.com/how-to-investigate-machine-learning-algorithm-behavior/" target="_blank" rel="noopener">如何研究机器学习算法的行为</a>：可以通过对行为进行很小的研究并对其进行研究，从而了解机器学习算法如何工作的方法。研究不仅针对学者！</li><li><a href="http://machinelearningmastery.com/how-to-implement-a-machine-learning-algorithm/" target="_blank" rel="noopener">如何实施机器学习算法</a>：从头开始实施机器学习算法的过程，技巧和窍门。</li></ul><h3 id="如何运行机器学习算法">如何运行机器学习算法</h3><p>有时，您只想深入研究代码。以下是一些链接，您可以使用这些链接来运行机器学习算法，使用标准库对其进行编码或从头开始实施它们。</p><ul><li><a href="http://machinelearningmastery.com/how-to-get-started-with-machine-learning-algorithms-in-r/" target="_blank" rel="noopener">如何开始使用R中的机器学习算法</a>：链接到此站点上的大量代码示例，这些示例演示了R中的机器学习算法。</li><li><a href="http://machinelearningmastery.com/get-your-hands-dirty-with-scikit-learn-now/" target="_blank" rel="noopener">scikit-learn中的机器学习算法食谱</a>：一系列Python代码示例，展示了如何使用scikit-learn创建预测模型。</li><li><a href="http://machinelearningmastery.com/how-to-run-your-first-classifier-in-weka/" target="_blank" rel="noopener">如何在Weka中运行您的第一个分类器如何在Weka</a>：中运行第一个分类器的教程（<strong>无需任何代码！</strong>）。</li></ul><p>原文：<a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" target="_blank" rel="noopener">A Tour of Machine Learning Algorithms</a></p><p>作者：<a href="https://machinelearningmastery.com/author/jasonb/" target="_blank" rel="noopener" title="Posts by Jason Brownlee">Jason Brownlee</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/15/A-Tour-of-Machine-Learning-Algorithms/ensemble.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;最合适的线条组合的一个很好的例子。弱预测成员为灰色，组合预测为红色。&lt;/p&gt;
&lt;p&gt;绘图来自Wikipedia，在公共领域获得许可。&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="ml" scheme="https://2020.iosdevlog.com/tags/ml/"/>
    
      <category term="algorithm" scheme="https://2020.iosdevlog.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法实现的简洁示例：MLAlgorithms</title>
    <link href="https://2020.iosdevlog.com/2020/03/15/MLAlgorithms/"/>
    <id>https://2020.iosdevlog.com/2020/03/15/MLAlgorithms/</id>
    <published>2020-03-15T13:03:07.000Z</published>
    <updated>2020-03-15T13:42:39.639Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://2020.iosdevlog.com/2020/03/15/MLAlgorithms/1.jpg" /></p><p>Minimal and clean examples of machine learning algorithms implementations</p><p>介绍一个常用的机器学习算法的最小和简洁实现的集合：机器学习算法库 <a href="https://github.com/rushter/MLAlgorithms" target="_blank" rel="noopener" class="uri">https://github.com/rushter/MLAlgorithms</a>。</p><a id="more"></a><h2 id="为什么"><a href="https://github.com/rushter/MLAlgorithms#why" target="_blank" rel="noopener">为什么</a>？</h2><p>该项目面向想要学习ml算法的内部知识或从头开始实现它们的人们。</p><p>与优化的库相比，该代码更易于遵循和使用。</p><p>所有算法均使用 <code>numpy</code>，<code>scipy</code> 和 <code>autograd</code> 在 <code>Python</code> 中实现。</p><h2 id="实现"><a href="https://github.com/rushter/MLAlgorithms#implemented" target="_blank" rel="noopener">实现</a> ：</h2><ul><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/neuralnet" target="_blank" rel="noopener">深度学习（MLP，CNN，RNN，LSTM）</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/linear_models.py" target="_blank" rel="noopener">线性回归，逻辑回归</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/ensemble/random_forest.py" target="_blank" rel="noopener">随机森林</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/svm" target="_blank" rel="noopener">支持向量机（SVM），带有内核（线性，多项式，RBF）</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/kmeans.py" target="_blank" rel="noopener">K均值</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/gaussian_mixture.py" target="_blank" rel="noopener">高斯混合模型</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/knn.py" target="_blank" rel="noopener">K近邻</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/naive_bayes.py" target="_blank" rel="noopener">朴素贝叶斯</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/pca.py" target="_blank" rel="noopener">主成分分析（PCA）</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/fm.py" target="_blank" rel="noopener">分解机</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/rbm.py" target="_blank" rel="noopener">受限玻尔兹曼机（RBM）</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/tsne.py" target="_blank" rel="noopener">t分布随机邻居嵌入（t-SNE）</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/ensemble/gbm.py" target="_blank" rel="noopener">梯度增强树（也称为GBDT，GBRT，GBM，XGBoost）</a></li><li><a href="https://github.com/rushter/MLAlgorithms/blob/master/mla/rl" target="_blank" rel="noopener">强化学习（深度Q学习）</a></li></ul><h2 id="安装"><a href="https://github.com/rushter/MLAlgorithms#installation" target="_blank" rel="noopener">安装</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;rushter&#x2F;MLAlgorithms</span><br><span class="line"> cd MLAlgorithms </span><br><span class="line">pip install scipy numpy </span><br><span class="line">python setup.py开发</span><br></pre></td></tr></table></figure><h2 id="如何在不安装的情况下运行示例"><a href="https://github.com/rushter/MLAlgorithms#how-to-run-examples-without-installation" target="_blank" rel="noopener">如何在不安装的情况下运行示例</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd MLAlgorithms </span><br><span class="line">python -m examples.linear_models</span><br></pre></td></tr></table></figure><h2 id="如何在docker中运行示例"><a href="https://github.com/rushter/MLAlgorithms#how-to-run-examples-within-docker" target="_blank" rel="noopener">如何在Docker中运行示例</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd MLAlgorithms docker </span><br><span class="line">build -t mlalgorithms 。</span><br><span class="line">docker run --rm -it mlalgorithms bash </span><br><span class="line">python -m examples.linear_models</span><br></pre></td></tr></table></figure><h2 id="贡献"><a href="https://github.com/rushter/MLAlgorithms#contributing" target="_blank" rel="noopener">贡献</a></h2><p>永远欢迎您的贡献！<br />随时改进现有代码，文档或实施新算法。<br />如果您的更改足够大，请提出一个建议。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/15/MLAlgorithms/1.jpg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Minimal and clean examples of machine learning algorithms implementations&lt;/p&gt;
&lt;p&gt;介绍一个常用的机器学习算法的最小和简洁实现的集合：机器学习算法库 &lt;a href=&quot;https://github.com/rushter/MLAlgorithms&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://github.com/rushter/MLAlgorithms&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="nn" scheme="https://2020.iosdevlog.com/tags/nn/"/>
    
      <category term="dl" scheme="https://2020.iosdevlog.com/tags/dl/"/>
    
      <category term="ml" scheme="https://2020.iosdevlog.com/tags/ml/"/>
    
      <category term="algorithm" scheme="https://2020.iosdevlog.com/tags/algorithm/"/>
    
      <category term="python" scheme="https://2020.iosdevlog.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>仿抖音小游戏《潜水艇》-FlappyBird</title>
    <link href="https://2020.iosdevlog.com/2020/03/14/FalppyBird/"/>
    <id>https://2020.iosdevlog.com/2020/03/14/FalppyBird/</id>
    <published>2020-03-14T13:01:39.000Z</published>
    <updated>2020-03-14T13:28:44.786Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://raw.githubusercontent.com/iOSDevLog/FlappySwift/master/FlappySwift.png" alt="" /><figcaption>FlappySwift</figcaption></figure><a id="more"></a><h2 id="flappyswift">FlappySwift</h2><p>An implementation of Flappy Bird and Face Detection in Swift for iOS 11.</p><p><a href="https://www.bilibili.com/video/av96310774/" target="_blank" rel="noopener">B站视频</a></p><h2 id="face-detection">Face Detection</h2><p><a href="https://www.raywenderlich.com/1163620-face-detection-tutorial-using-the-vision-framework-for-ios" target="_blank" rel="noopener">Face Detection Tutorial Using the Vision Framework for iOS</a></p><h2 id="flappybird">FlappyBird</h2><p><a href="https://github.com/fullstackio/FlappySwift" target="_blank" rel="noopener">FlappySwift</a></p><h2 id="github">GitHub</h2><p><a href="https://github.com/iOSDevLog/FlappySwift" target="_blank" rel="noopener" class="uri">https://github.com/iOSDevLog/FlappySwift</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/iOSDevLog/FlappySwift/master/FlappySwift.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;FlappySwift&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="game" scheme="https://2020.iosdevlog.com/categories/game/"/>
    
    
      <category term="iOS" scheme="https://2020.iosdevlog.com/tags/iOS/"/>
    
      <category term="Swift" scheme="https://2020.iosdevlog.com/tags/Swift/"/>
    
      <category term="cv" scheme="https://2020.iosdevlog.com/tags/cv/"/>
    
  </entry>
  
  <entry>
    <title>tf2 CheekSheet</title>
    <link href="https://2020.iosdevlog.com/2020/03/13/tf2/"/>
    <id>https://2020.iosdevlog.com/2020/03/13/tf2/</id>
    <published>2020-03-13T15:34:37.000Z</published>
    <updated>2020-03-14T13:29:46.585Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://2020.iosdevlog.com/2020/03/13/tf2/0.jpg" /></p><p><a href="https://www.aicheatsheets.com/" target="_blank" rel="noopener" class="uri">https://www.aicheatsheets.com/</a></p><a id="more"></a><p><img src="https://2020.iosdevlog.com/2020/03/13/tf2/1.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/13/tf2/2.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/13/tf2/3.jpg" /></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/13/tf2/0.jpg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.aicheatsheets.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://www.aicheatsheets.com/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="tf2" scheme="https://2020.iosdevlog.com/tags/tf2/"/>
    
      <category term="cheeksheet" scheme="https://2020.iosdevlog.com/tags/cheeksheet/"/>
    
  </entry>
  
  <entry>
    <title>《香农传》</title>
    <link href="https://2020.iosdevlog.com/2020/03/12/Shannon/"/>
    <id>https://2020.iosdevlog.com/2020/03/12/Shannon/</id>
    <published>2020-03-12T09:15:04.000Z</published>
    <updated>2020-03-12T15:30:10.685Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/1.jpg" alt="" /><figcaption>《香农传》</figcaption></figure><p>书名：香农传<br />作者：[美]吉米·索尼，[美]罗伯·古德曼<br />译者：杨晔<br />出版社：中信出版集团<br />出版时间：2019-02<br />ISBN：9787508694801</p><a id="more"></a><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/20.jpg" /></p><p>克劳德·香农的父亲老克劳德·艾尔伍德·香农，1862年出生于美国新泽西州，他曾做过家具推销员、丧葬承办人和遗嘱检验法官。克劳德的母亲玛贝尔·沃尔夫是一位德国移民的女儿，她曾做过教师和校长。1909年，他们的婚礼公告成为盖洛德的头条新闻。这验证了这座城有多么小，也验证了香农夫妇在这一社群里的活跃角色</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/3.jpg" /></p><p>到了克劳德·香农在密歇根大学拍下注册照片的时候，他已经成为一名娴熟的发明家。他的发明包括简易升降机、后院小推车和通过带刺铁丝网传递加密消息的电报系统</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/4.jpg" /></p><p>香农似乎遗传了他的爷爷戴维·香农的才华，他骄傲地持有美国第407130号专利，对洗衣机进行了一系列改进。这个男孩继承了爷爷在机械方面的天赋，对于他来说，家里出了这样一名拥有专利的发明家是一件值得炫耀的事情</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/5.jpg" /></p><p>在香农入学之前，密歇根大学工程学院经历了巨大的发展。在学院的一次公开展览上，学生们“震惊了访客，他们使用20000转/分钟的纸张切割木材，通过液态气体冷冻鲜花，还展示了只用两根细线支撑的瓶子，水流从中缓缓流出——这是罕有人能够解决的难题”。密歇根大学的工程建筑可以满足重工业的要求，正如这间工厂……</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/6.jpg" /></p><p>……以及这个船舶池，学生们在这里测试模型船的流体动力</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/7.jpg" /></p><p>1934年春，克劳德·香农于17岁的时候，在《美国数学月刊》第191页上发表了第一篇学术作品。香农解出了一道数学难题。他阅读这类期刊的行为本身，揭示了他对学术事务非同寻常的关注，而他的解题方法被选中也表明他不是一个普通的人才</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/8.jpg" /></p><p>麻省理工学院的校园是香农初次作为工程师成名的地方，它的设计是建筑师们相互妥协的产物，建筑上方的圆顶秉承了“效率的原则，避免师生做无用功，它相当于最好的工业作品”。它半是庙宇，半是工厂</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/9.jpg" /></p><p>在麻省理工学院，香农加入了一个小组。这个小组旨在使得微分分析仪成为通用机械计算机，助力解决计算电力传输、电话网络等工程难题或宇宙射线和亚原子微粒等高等物理难题。这个项目追随了威廉·汤姆森的步伐，他是一名留着独特胡子的物理学家，被尊称为凯尔文爵士。他在1876年建造了早期的机械计算机</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/10.jpg" /></p><p>在麻省理工学院，香农在业余时间选修了飞行课。这门课的执教教授强烈建议麻省理工学院的校长禁止他继续上课，他认为香农是一个不可多得的人才，不应为可能发生的飞机事故而冒险。校长则拒绝了他的建议：“我怀疑以他智力超群为由，禁止这位年轻人参加飞行课或者武断地剥夺他的机会是明智的。”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/11.jpg" /></p><p>微分分析仪是如房间般大小的“大脑”，为了解决问题会日夜不停地运转。“这个由轴、齿轮、线路、滚轮组成的家伙十分可怕，但它确实有效。”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/12.jpg" /></p><p>在绝大多数情况下，范内瓦·布什都是20世纪中期美国最有权力的科学家。他在麻省理工学院主持微分分析仪，为总统提供咨询服务，在“二战”期间领导美国的科学家。《科利尔》杂志将他称为“会决定战争胜利或失败的男人”；《时代周刊》将他称作“物理将军”。而且，这些成就中尤其有这么一条：他成为克劳德·香农的第一位导师，也是对他影响最大的导师</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/13.jpg" /></p><p>1939年夏，香农来到了冷泉港，抵达了美国最顶尖的基因实验室，这也是美国最大的科学羞耻之一——优生学记录室。它收藏了大量基因数据，基于此，香农完成了他的理论基因学论文</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/14.jpg" /></p><p>香农拿到奖学金前往负有盛名的普林斯顿高等研究院，但这次经历并不令人愉快。这里见证了他第一段婚姻的失败，以及他对愈演愈烈的第二次世界大战的恐惧；在这里他还与阿尔伯特·爱因斯坦有过几次接触</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/15.jpg" /></p><p>这里是1936年的贝尔实验室综合楼，照片上的视角是从曼哈顿西村的华盛顿街看过去的景象。香农的一名同事回忆道：“大家在贝尔实验室做得非常好，他们做的事在其他人看来根本不可能。”当贝尔实验室的办公场所还在下曼哈顿区（靠近当今的高线公园）的时候，香农签下了这里的全职工作</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/16.jpg" /></p><p>克劳德·香农和他的同事戴维·哈格尔巴格一同在贝尔实验室工作。另一名同事回忆起那段时光：“在这里，我可以任意获取全世界电气工程领域的信息。我所需要的就是拿起电话或者去询问某人，然后便能得到答案。”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/17.jpg" /></p><p>桑顿·弗赖伊成立了实验室的数学组，并将香农派到这里。他有一次说，“数学家们都是怪人，这是事实，所以遇到任何足够奇怪而你又不知道如何沟通的人，你就说，‘这家伙是个数学家，把他交给弗赖伊吧’。”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/18.jpg" /></p><p>香农和约翰·皮尔斯（如图）同巴尼·奥利弗一起组成了实验室的天才三人组。一位同辈开玩笑道：“他们三个人的智商高到令人难以忍受的程度。”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/19.jpg" /></p><p>在曼哈顿，香农是一名单身汉（在第一段婚姻终结之后），他有一间位于格林尼治村的小公寓和一份要求颇高的工作。他保留了一些时间以满足自己古怪的爱好，包括用力弹琴，以及欣赏纽约爵士乐</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/2.jpg" /></p><p>对抗纳粹德国一定程度上是一场“科学战争”，香农在这方面的贡献包括研究密码学、防空火力控制和“绿色大黄蜂”系统（如图），后者是迄今为止最具野心的语言加扰系</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/21.jpg" /></p><p>对香农的信息论研究影响最大的是拉尔夫·哈特利。他1927年关于“信息传递”的论文是当时最接近掌握信息本质的方法，解释了科学家是怎样从物理的角度而非心理的角度思索这一问题的</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/22.jpg" /></p><p>在《通信的数学理论》之前，长达一个世纪的常识与反复进行的工程试验，都认为通信必然会伴有噪声——这是物理世界要求我们付出的代价。然而香农证明了信道噪声是可以被克服的，由A点发出的信息“总是可以”，而不仅仅是“经常能够”在B点被完全接收。他向工程师提供了使信息数字化并将其可靠发送（或者，准确地说，可能伴有少量随机误差）的概念工具，直到香农证实噪声可控之前，该结论一直被当作不可能实现的乌托邦</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/23.jpg" /></p><p>1948年之后，香农被媒体赞誉为科学名人。他接受电视采访，被国家出版物报道，并被授予一些荣誉学位</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/24.jpg" /></p><p>1948年，克劳德·香农认识了贝蒂·摩尔，她是贝尔实验室的一名工作人员，他鼓起勇气邀请她共进晚餐。之后他们又有了第二顿、第三顿，直到他们每天都在一起吃晚餐</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/25.jpg" /></p><p>克劳德·香农和贝蒂·摩尔的关系发展得非常迅速：他们相识于1948年秋，而到了1949年年初，克劳德便求婚了。根据贝蒂的回忆，求婚是以“不太正式”的方式进行的。她不仅有幽默感，而且同他一样热爱数学，这奠定了他们伙伴关系的基础，直到克劳德生命的终结</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/26.jpg" /></p><p>贝蒂为克劳德购买了他的第一辆独轮车，这开启了他与这种机器一辈子的不解之缘。他为自己亲手打造了各种量身定制的独轮车，并在贝尔实验室狭窄的甬道里骑车。这使得访客们对他的灵活性印象深刻</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/27.jpg" /></p><p>即使成了科学名人，香农也仍然是一名发明家。他最著名的发明“忒修斯”是一只能够自动穿越迷宫并“记住”金属乳酪位置的人工老鼠。（倘若乳酪被拿走了，“忒修斯”就只会漫无目的地走来走去。一名科学家说：“这一切都太人性化了。”）</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/28.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/29.jpg" /></p><p>贝尔实验室墨累山园区——“设想与设计未来的地方，这里的未来指的是我们所谓的现在。”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/30.jpg" /></p><p>香农建造了世界上最早的弈棋机。它在1949年完工，香农的机器只能控制六枚棋子，聚焦于棋局中的走位。它使用了超过150个继电器计算走棋，它的处理能力使得机器能够在10~15秒做出决定</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/31.jpg" /></p><p>“我是机器，你也是机器，我们都会思考，不是吗？”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/32.jpg" /></p><p>香农为人工智能设定了四个目标：到2001年，创造出打败世界冠军的象棋程序；写出被《纽约客》认可的诗文的诗歌程序；写出能够证明难以捉摸的黎曼假设的数学程序；以及“最重要的”，设计出收益超过50%的选股软件。他半开玩笑地说，“这些目标可能意味着逐步淘汰愚蠢的、熵增加的、好战的人类，转而支持更加合乎逻辑的、节约能源的、友善的物种，即计算机。”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/33.jpg" /></p><p>诺伯特·维纳（图片中间，右侧为香农，左侧为麻省理工学院校长朱利叶斯·斯特拉顿）曾是一名神童，“控制论”的提出者，也是唯一可能挑战香农“信息论之父”地位的人</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/34.jpg" /></p><p>香农在马萨诸塞州温彻斯特买下了一幢房子，是一处位于麻省理工学院以北8千米的近郊住宅区。这幢房子建于1858年，是为天才发明家托马斯·杰斐逊的曾孙女艾伦·德怀特修建的。它占地约48.6平方千米，受蒙蒂塞洛启发而设计</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/35.jpg" /></p><p>在一次前往俄罗斯的旅程中，香农想要与苏联国际象棋冠军、计算机工程师米哈伊尔·鲍特维尼克下一场友谊赛。鲍特维尼克并没有给予足够的重视，直至香农在对弈中吃掉了他的马和兵。在走了42步之后，香农推倒了他的王，认输了。但能与鲍特维尼克对弈几十步，仍然为香农赢得了值得终生吹嘘的资本，因为前者一直被认为是最具天赋的棋手之一</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/36.jpg" /></p><p>1957年，香农回到麻省理工学院做教授。然而，他的研究生名额并未招满，“你必须有足够的自信，才能请求像香农这样的人做你的导师”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/37.jpg" /></p><p>1967年2月6日，林登·B.约翰逊总统向克劳德·香农颁发了美国国家科学奖奖章，以表彰他“对通信和信息处理的数学理论的杰出贡献”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/38.jpg" /></p><p>早期，香农在麻省理工学院的讲座场场爆满，但这些都比不过他做的关于股票市场的报告，这场在学校里最大的报告厅内举办的讲座挤满了听众</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/39.jpg" /></p><p>在美国马萨诸塞州，香农教授留起了胡子，继续杂耍事业。他也完全沉浸在自己发明创造的兴趣爱好里，在自己家颇具规模的工作坊里设计出许多自己最著名的发明</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/40.jpg" /></p><p>世界上第一台可穿戴计算机是由香农和爱德华·索普开发的，可用来计算轮盘赌一类的东西。他们佩戴着它，在拉斯维加斯的赌场里成功了几次，却最终放弃了这个项目，因为他们惧怕卷入与黑手党的麻烦之中</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/41.jpg" /></p><p>香农使用建造者套件模仿W.C.菲尔兹打造了这个机器人，它可以抛接三个球。球从钢鼓上弹起，机器人以摇摆的动作挥舞桨臂。“每次手臂摆下来它都能接住球，而当手臂摆上去它又会抛出球。”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/42.jpg" /></p><p>香农自己既是一名技艺高超的杂耍者，也是第一位将杂耍数学写成论文的作者。他写道，他的读者应当“尽量不要忘记杂耍的诗歌、喜剧和音乐……我听起来是不是很自以为是？”</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/43.jpg" /></p><p>香农在生命中的最后几年，仍旧保持着骑独轮车的爱好</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/44.jpg" /></p><p>香农终其一生都在追求满足自己的好奇心，认真地做游戏：他是罕见的科学天才，像满足于探索数字电路一样，满足于制作杂耍机器人和喷射小号</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/45.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/46.jpg" /></p><p>六步解法</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/52.jpg" /></p><h2 id="庥省理工学院">庥省理工学院</h2><p>乔治·布尔</p><ul><li>和（And）<code>*</code></li><li>或（Or）<code>+</code></li><li>非（Not）<code>‘</code></li><li>如果（If）</li></ul><p>电流</p><ul><li>串联 =&gt; “和”</li><li>并联 =&gt; “或”</li><li>两个并联 =&gt; 1+1=1</li></ul><h2 id="贝尔实验室">贝尔实验室</h2><h2 id="密码学研究">密码学研究</h2><h2 id="与图灵的友谊">与图灵的友谊</h2><p>1942年，图灵跟随英国政府发起的军事密码项目访问团来到美国。</p><p>1936年，他设计出图灵机，这一里程碑式的思想实验为现代电脑的发明打下了理论基础。</p><p>除此之外，图灵还开始破解密码。</p><h2 id="从情报到信息">从情报到信息</h2><p><code>情报减去含义即为信息。</code></p><ol type="1"><li>奈奎斯特曾使用“情报”这个模糊的概念，</li><li>哈特利曾努力地解释撇开心理学和语义学的价值，</li><li>而到香农的时代，他已经理所当然地认为含义是可以被忽略的。</li></ol><p>最简本质</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/64.jpg" /></p><ul><li>信源生成信息。</li><li>发射器将信息转码成能够发射的信号。</li><li>信道是信号通过的媒介。</li><li>噪声源代表了信号在被接收的过程中遭到的扭曲与破坏。</li><li>接收器解码消息，与发射器原理相反。</li><li>信宿是信息的接收者。</li></ul><p>这种精简模式的妙处在于它的普遍适用性。信息对于一则故事来说无意义，却能将它播放出来，包括人的信息、电路中的信息、神经元中的信息、血液中的信息。你对着电话听筒讲话（信源）；电话将你声音的声压编码成电信号（发射器）；信号通过电线（信道）；附近的电线会干扰信号（噪声）；信号被解码回声音（接收器）；声音到达另一个人的耳中（信宿）。</p><p>在细胞中，DNA链指导蛋白质的生成（信源）；它通过编码转录储存在信使RNA链中（发射器）；信使RNA携带代码到细胞的蛋白质合成处（信道）；RNA编码中的一个“字母”随机地在“点突变”（噪声）中切换；每3个“字母”代码被翻译成氨基酸–蛋白质的构件（接收器）；氨基酸被结合到蛋白质链中。DNA由此得到复制（信宿）。</p><p>香农认为，信息科学仍然未能发现对信息至关重要的东西——<code>概率的本质</code>。</p><p>新生科学需要新的计量单位，或者至少要证明他们一直谈论的概念最终能否被数字捕捉。香农研究的信息科学的新单位要表达选择的基本情况。由于它是在0或1之间的选择，所以它是“二进制数字”。在香农的全部设计中，仅有有限的部分是他允许与其他人进行合作的，这包括他在午餐时提出与贝尔实验室的同事们商讨一个更简洁的名字。二进制符号“binit”和二进制数字“bigit”在经过考量后被放弃了，最终赢得大家认可的方案是由一名在贝尔实验室工作的普林斯顿大学的教授约翰·图基提出的 <code>“比特”（bit）</code>。</p><p>比特是在两个等概率的可能性之中进行选择后所产生的信息量。所以<code>“一台拥有两种稳定状态的设备……能够存储1比特信息”</code>。这种设备的位元（包括具有两种位置的开关，具有正反两面的硬币，以及具有两种状态的数字）不在选择的结果之中，而在可能选择的数量以及进行选择的概率之中。两台这样的器件能够代表全部4种选择，并且可被称作存储2比特。</p><blockquote><p>因为香农的标准是呈对数出现的（以2为基础，换句话说，就是“倒过来”将2赋予给定数字的权力），每当可供选择的数量平方之后，比特的数量将增加1倍：</p></blockquote><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/65.jpg" /></p><blockquote><p>信息到底衡量了什么？</p></blockquote><p><strong>它衡量了我们所克服的不确定性。</strong></p><p>它衡量了我们学习新事物的可能性，或者更具体地说，当一件事物承载了关于其他事物的信息（正如计数器能够告诉我们一个物理量，或一本书能够告诉我们人生），它所涵盖的信息数量反映了未知情况的减少。能够解析最大程度的不确定性的消息，即以最公平的方式从最广泛的符号集合中选择出的包含最丰富的信息。但当一切都充满确定性的时候，并不能产生信息，因为没有什么可以传达。</p><p>香农用 <code>抛硬币</code> 事件（判断给定正反面的可能性，这里称作p，有0、50%、100%三种情况）展示了利益攸关的信息量：</p><p><img src="https://2020.iosdevlog.com/2020/03/12/Shannon/66.jpg" /></p><p>H =–p log p – q log q</p><p>在这一公式中，p和q分别是两种结果的概率（硬币的任一面或者能被传输的任一符号），两者的数值加起来刚好为100%。</p><h2 id="维纳的控制论">维纳的控制论</h2><blockquote><p>维纳自以为到了家，笨拙地摸索出钥匙，却发现打不开门。他转向街道上玩耍的孩子们问道：“你们能告诉我维纳家住在哪里吗？”一个小女孩回答道：“爸爸，跟我走。妈妈派我来告诉你我们的新家在哪里。”</p></blockquote><p>他对这一领域的贡献是广泛而深入的：量子力学、布朗运动、控制论、随机过程、谐波分析。他涉猎了几乎所有数学世界的领域。1948年，他的简历上已经布满了闪闪发光的奖项和荣誉。维纳的合作伙伴与联系人名单也同样引人注目：范内瓦·布什、G. H.哈代、伯特兰·罗素、保罗·莱维、库尔特·哥德尔……和克劳德·香农。</p><p>《控制论》</p><h2 id="人造机器">人造机器</h2><blockquote><p>机器能够思考吗？它会痛吗？我们可以说人体也是机器吗？无疑，人体和机器非常相似，但是机器绝对不会思考！我们这样说有什么实证根据吗？并没有，我们可以这样说人，说一切可以思考的东西；而我们也可以这样说布娃娃，毫无疑问，还有鬼神。</p></blockquote><p>——路德维希·维特根斯坦</p><blockquote><p>我是机器，你也是机器，我们都会思考，不是吗？</p></blockquote><p>——克劳德·香农</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/12/Shannon/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《香农传》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：香农传&lt;br /&gt;
作者：[美]吉米·索尼，[美]罗伯·古德曼&lt;br /&gt;
译者：杨晔&lt;br /&gt;
出版社：中信出版集团&lt;br /&gt;
出版时间：2019-02&lt;br /&gt;
ISBN：9787508694801&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>看见统计</title>
    <link href="https://2020.iosdevlog.com/2020/03/11/seeing-theory/"/>
    <id>https://2020.iosdevlog.com/2020/03/11/seeing-theory/</id>
    <published>2020-03-11T10:51:29.000Z</published>
    <updated>2020-03-11T15:44:36.263Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/11/seeing-theory/1.png" alt="" /><figcaption>看见统计</figcaption></figure><p>看见统计由Daniel Kunin在布朗大学读本科的时候开始制作。致力于用数据可视化让统计概念更容易理解。 (数据可视化使用Mike Bostock的javascript库D3.js制作。)</p><a id="more"></a><h2 id="基础概率论">基础概率论</h2><p>主要介绍了概率论中的一些基本概念</p><h3 id="随机事件概率事件">随机事件(概率事件)</h3><p>生活中充满了随机性。概率论是一门用数学语言来刻画这些随机事件的学科。一个随机事件的概率是一个介于0与1之间的实数，这个实数的大小反映了这个事件发生的可能性。因此，概率为0意味着这个事件不可能发生（不可能事件），概率为1意味着这个事件必然发生（必然事件）。</p><p>以一个投掷一枚公平的硬币（出现正面和反面的概率相等，均为1/2）的经典的概率实验为例：。在现实中，如果我们重复抛一枚硬币，出现正面的频率可能不会恰好是50%。但是当抛硬币的次数增加时，出现正面的概率会越来越接近50%。</p><p>如果硬币两面的重量不一样， 出现正面的概率就和出现反面的概率不一样了。上下拖动屏幕右侧蓝色柱状图来改变硬币正面和反面的的重量分布。如果我们用一个实数来代表抛硬币的结果：比如说1表示正面，0表示反面，那么我们称这个数为 随机变量。</p><h3 id="期望">期望</h3><p>一个随机变量的期望刻画的是这个随机变量的概率分布的“中心”。简而言之，当有无穷多来自同一个概率分布的独立样本时，它们的平均值就是期望。数学上对期望的定义是以概率（或密度）为权重的加权平均值。</p><p><span class="math display">\[\mathrm{E}[X]=\sum_{x \in \mathcal{X}} x P(x)\]</span></p><p>现在以另一个经典的概率实验为例：扔一枚公平的骰子，每一面出现的概率相等，均为1/6。当试验的次数越来越多时，扔出的结果的平均值慢慢趋向于它的期望3.5。</p><h3 id="方差">方差</h3><p>如果说随机变量的期望刻画了它的概率分布的“中心”，那么方差则刻画了概率分布的分散度。方差的定义是一个随机变量与它的期望之间的差的平方的加权平均值。这里的权重仍然是概率（或者密度）。</p><p><span class="math display">\[\operatorname{Var}(X)=\mathrm{E}\left[(X-\mathrm{E}[X])^{2}\right]\]</span></p><p>随机从下面十张牌中抽牌。当抽取的次数越来越多时，可以观察到样本平方差的平均值（绿色）逐渐趋向于它的方差（蓝色）。</p><h2 id="进阶概率论">进阶概率论</h2><p>概率论中的一些核心知识</p><h3 id="集合论">集合论</h3><p>广而言之，一个集合指的是一些物体的总体。在概率论中，我们用一个集合来表示一些事件的组合。比如，我们可以用集合 <span class="math inline">\(\{2,4,6\}\)</span> 来表示“投骰子投出偶数”这个事件。因此我们有必要掌握一些基本的集合的运算。</p><h3 id="古典概型">古典概型</h3><p>古典概型本质上就是数数。但是在概率论中，数数有时候比想象中要困难的多。因为我们有时要数清楚符合一些性质的事件或者轨道个数的，而这些性质往往比较复杂，因此数数的任务也变得困难起来。假设我们有一袋珠子，每个珠子的颜色都不相同。如果我们无放回地从袋子里抽取珠子，一共有多少种可能出现的颜色序列（排列）呢？有多少种可能出现的没有顺序的序列（组合）呢？</p><h3 id="条件概率">条件概率</h3><p>条件概率让我们可以利用已有的信息。举个例子，在今天多云 的情况下，我们会估计“明天下雨”的概率小于“今天下雨”。这种基于已有的相关信息得出的概率称为条件概率。</p><p>数学上，条件概率的计算一般会把的样本空间缩小到一个我们已知信息的事件。再以之前举的下雨为例，我们现在只考虑所有前一天多云的日子，而不是考虑所有的日子。然后我们确定在这些天中有多少天下雨，这些下雨天数在所有我们考虑的天数中的比例即为条件概率。</p><h2 id="概率分布">概率分布</h2><p>描述了随机变量取值的规律</p><h3 id="随机变量">随机变量</h3><p>随机变量是一个函数，它用数字来表示一个可能出现的事件。你可以定义你自己的随机变量，然后生成一些样本来观察它的经验分布。</p><h3 id="离散型和连续型随机变量">离散型和连续型随机变量</h3><p>常见的随机变量类型有两种：</p><ul><li>离散型随机变量</li></ul><p>一个离散型随机变量可能的取值范围只有有限个或可列个值。离散型随机变量的定义是：如果 <span class="math inline">\(X\)</span> 是一个随机变量，存在非负函数 <span class="math inline">\(f(x)\)</span> 和 <span class="math inline">\(F(X)\)</span>,使得</p><p><span class="math display">\[\begin{array}{l}P(X=x)=f(x) \\P(X&lt;x)=F(x)\end{array}\]</span></p><p>则称 <span class="math inline">\(X\)</span> 是一个离散型随机变量。</p><p><code>伯努利分布(Bernoulli)</code></p><p>如果一个随机变量 <span class="math inline">\(X\)</span> 只取值 <span class="math inline">\(0\)</span> 或 <span class="math inline">\(1\)</span>，概率分布是</p><p><span class="math display">\[P(X=1)=p, \quad P(X=0)=1-p\]</span></p><p>则称 <span class="math inline">\(X\)</span> 符合伯努利分布(Bernoulli)。我们常用伯努利分布来模拟只有两种结果的试验，如抛硬币。</p><p>在概率论中，概率质量函数（probability mass function，简写为 <code>pmf</code>）是离散随机变量在各特定取值上的概率。</p><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>PMF</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(f(x ; p)=\left\{\begin{array}{ll}p &amp; \text { if } x=1 \\ 1-p &amp; \text { if } x=0\end{array}\right.\)</span></td><td><span class="math inline">\(p\)</span></td><td><span class="math inline">\(p(1-p)\)</span></td></tr></tbody></table><p><code>二项分布(Binomial)</code></p><p>如果随机变量 <span class="math inline">\(X\)</span> 是 <span class="math inline">\(n\)</span> 个参数为p的独立伯努利随机变量之和，则称 <span class="math inline">\(X\)</span> 是二项分布(binomial)。我们常用二项分布来模拟若干独立同分布的伯努利试验中的成功次数。比如说，抛五次硬币，其中正面的次数可以用二项分布来表示：<span class="math inline">\(\operatorname{Bin}\left(5, \frac{1}{2}\right)\)</span>。</p><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>PMF</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(f(x ; n, p)=\left(\begin{array}{l}n \\ x\end{array}\right) p^{x}(1-p)^{n-x}\)</span></td><td><span class="math inline">\(np\)</span></td><td><span class="math inline">\(np(1-p)\)</span></td></tr></tbody></table><p><code>几何分布(Geometric)</code></p><p>一个服从几何分布的随机变量表示了在重复独立同分布的伯努利试验中获得一次成功所需要的试验此时。比如说，如果我们重复投一枚骰子，我们则可以用几何分布来表示投出一个6所需要的试验次数。</p><table><thead><tr class="header"><th>PMF</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(f(x ; p)=(1-p)^{x} p\)</span></td><td><span class="math inline">\(\frac{1}{p}\)</span></td><td><span class="math inline">\(\frac{1-p}{p^{2}}\)</span></td></tr></tbody></table><p><code>泊松分布(Poisson)</code></p><p>表示了一个事件在固定时间或者空间中发生的次数。泊松分布的参数 <span class="math inline">\(λ\)</span> 是这个时间发生的频率。比方说，我们可以用泊松分布来刻画流星雨或者足球比赛中的进球数。</p><table><thead><tr class="header"><th>PMF</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(f(x ; \lambda)=\frac{\lambda^{x} e^{-\lambda}}{x !}\)</span></td><td><span class="math inline">\(\lambda\)</span></td><td><span class="math inline">\(\lambda\)</span></td></tr></tbody></table><p><code>负二项分布(Negative Binomial)</code></p><p>一个负二项分布的随机变量X表示的是若干独立同分布的参数为p的伯努利试验中获得r次失败前成功的次数。比方说，如果我们重复抛一枚硬币，我们则可以用负二项分布来表示抛出三次反面之前抛出正面的次数。</p><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>PMF</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(f(x ; n, r, p)=\left(\begin{array}{c}x+r-1 \\ x\end{array}\right) p^{x}(1-p)^{r}\)</span></td><td><span class="math inline">\(\frac{p r}{1-p}\)</span></td><td><span class="math inline">\(\frac{p r}{(1-p)^{2}}\)</span></td></tr></tbody></table><p><code>概率分布表</code></p><table><colgroup><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /></colgroup><thead><tr class="header"><th>概率分布</th><th>PMF</th><th style="text-align: center;">期望</th><th style="text-align: center;">方差</th></tr></thead><tbody><tr class="odd"><td>伯努利分布(Bernoulli)</td><td><span class="math inline">\(f(x ; p)=\left\{\begin{array}{ll}p &amp; \text { if } x=1 \\ 1-p &amp; \text { if } x=0\end{array}\right.\)</span></td><td style="text-align: center;"><span class="math inline">\(p\)</span></td><td style="text-align: center;"><span class="math inline">\(p(1-p)\)</span></td></tr><tr class="even"><td>二项分布(Binomial)</td><td><span class="math inline">\(f(x ; n, p)=\left(\begin{array}{l}n \\ x\end{array}\right) p^{x}(1-p)^{n-x}\)</span></td><td style="text-align: center;"><span class="math inline">\(np\)</span></td><td style="text-align: center;"><span class="math inline">\(np(1-p)\)</span></td></tr><tr class="odd"><td>几何分布(Geometric)</td><td><span class="math inline">\(f(x ; p)=(1-p)^{x} p\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{1}{p}\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{1-p}{p^{2}}\)</span></td></tr><tr class="even"><td>泊松分布(Poisson)</td><td><span class="math inline">\(f(x ; \lambda)=\frac{\lambda^{x} e^{-\lambda}}{x !}\)</span></td><td style="text-align: center;"><span class="math inline">\(\lambda\)</span></td><td style="text-align: center;"><span class="math inline">\(\lambda\)</span></td></tr><tr class="odd"><td>负二项分布(Negative Binomial)</td><td><span class="math inline">\(f(x ; n, r, p)=\left(\begin{array}{c}x+r-1 \\ x\end{array}\right) p^{x}(1-p)^{r}\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{p r}{1-p}\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{p r}{(1-p)^{2}}\)</span></td></tr></tbody></table><ul><li>连续型随机变量</li></ul><p>连续型随机变量可能取值的范围是一个无限不可数集合（如全体实数)。连续型随机变量的定义是：设X为随机变量，存在非负函数f(x)使得：</p><p><span class="math display">\[\begin{aligned}P(a \leq X \leq b) &amp;=\int_{a}^{b} f(x) d x \\P(X&lt;x) &amp;=F(x)\end{aligned}\]</span></p><p><code>均匀分布(Uniform)</code></p><p>如果随机变量X在其支撑集上所有相同长度的区间上有相同的概率，即如果 <span class="math inline">\(b_{1}-a_{1}=b_{2}-a_{2}\)</span>,则</p><p><span class="math display">\[P\left(X \in\left[a_{1}, b_{1}\right]\right)=P\left(X \in\left[a_{2}, b_{2}\right]\right)\]</span></p><p>那么我们称 <span class="math inline">\(X\)</span> 服从均匀分布(Uniform)。比方说，我们一般可以假设人在一年中出生的概率是相等的，因此可以用均匀分布来模拟人的出生时间。</p><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>概率分布</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(f(x ; a, b)=\left\{\begin{array}{l}\frac{1}{b-a} \text { for } x \in[a, b] \\ 0 \quad \text { otherwise }\end{array}\right.\)</span></td><td><span class="math inline">\(\frac{a+b}{2}\)</span></td><td><span class="math inline">\(\frac{(b-a)^{2}}{12}\)</span></td></tr></tbody></table><p><code>正态分布/高斯分布(Normal)</code></p><p>正态分布（也称高斯分布）的密度函数是一个钟形曲线。科学中常用正态分布来模拟许多小效应的叠加。比方说，我们知道人的身高是许多微小的基因和环境效应的叠加。因此可以用正态分布来表示人的身高，</p><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>概率分布</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(f\left(x ; \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}\)</span></td><td><span class="math inline">\(\mu\)</span></td><td><span class="math inline">\(\sigma^{2}\)</span></td></tr></tbody></table><p><code>学生t分布(Student T)</code></p><p>学生t分布（也称t分布）往往在估计正态总体期望时出现。当我们只有较少的样本和未知的方差时，许多大样本性质并不适用，此时我们则需要用到t分布。</p><table><thead><tr class="header"><th>概率分布</th><th>说明</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\frac{Z}{\sqrt{U / k}}\)</span></td><td><span class="math inline">\(Z \sim N(0,1)\)</span> <span class="math inline">\(U \sim \chi_{k}\)</span></td><td><span class="math inline">\(0\)</span></td><td><span class="math inline">\(\frac{k}{k-2}\)</span></td></tr></tbody></table><p><code>卡方分布(Chi Squared)</code></p><p>如果随机变量 <span class="math inline">\(X\)</span> 是 <span class="math inline">\(k\)</span> 个独立的标准正态随机变量的平方和，则称 <span class="math inline">\(X\)</span> 是自由度为k的卡方随机变量：<span class="math inline">\(X \sim \chi_{k}^{2}\)</span>. 卡方分布常见于假设检验和构造置信区间.</p><table><thead><tr class="header"><th>概率分布</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\sum_{i=1}^{k} Z_{i}^{2} \quad Z_{i} \stackrel{i: i, d}{\sim} N(0,1)\)</span></td><td><span class="math inline">\(k\)</span></td><td><span class="math inline">\(2 k\)</span></td></tr></tbody></table><p><code>指数分布(Exponential)</code></p><p>指数分布可以看作是几何分布的连续版本，其常用于描述等待时间。</p><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>概率分布</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(f(x ; \lambda)=\left\{\begin{array}{ll}\lambda e^{-\lambda x} &amp; \text { if } x \geq 0 \\ 0 &amp; \text { otherwise }\end{array}\right.\)</span></td><td><span class="math inline">\(\frac{1}{\lambda}\)</span></td><td><span class="math inline">\(\frac{1}{\lambda^{2}}\)</span></td></tr></tbody></table><p><code>F分布(F)</code></p><p>F分布(Fisher–Snedecor分布)常在假设检验中出现，一个比较有名的例子是 方差分析。</p><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>概率分布</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\begin{array}{ll}\frac{U_{1} / d_{1}}{U_{2} / d_{2}} &amp; U_{1} \sim \chi_{d_{1}} \\ &amp; U_{2} \sim \chi_{d_{2}}\end{array}\)</span></td><td><span class="math inline">\(\frac{d_{2}}{d_{2}-2}\)</span></td><td><span class="math inline">\(\frac{2 d_{2}^{2}\left(d_{1}+d_{2}-2\right)}{d_{1}\left(d_{2}-2\right)^{2}\left(d_{2}-4\right)}\)</span></td></tr></tbody></table><p><code>Gamma分布(Gamma)</code></p><p>Gamma分布是一组连续型概率密度。</p><blockquote><p>指数分布和卡方分布是Gamma分布的两个特殊情形。</p></blockquote><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>概率分布</th><th>期望</th><th>方差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(f(x ; k, \theta)=\frac{1}{\Gamma(k) \theta^{k}} x^{k-1} e^{-\frac{x}{\theta}}\)</span></td><td><span class="math inline">\(k\theta\)</span></td><td><span class="math inline">\(k\theta^{2}\)</span></td></tr></tbody></table><p><code>连续概率分布表</code></p><table><colgroup><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /></colgroup><thead><tr class="header"><th>连续分布</th><th>概率分布</th><th style="text-align: center;">期望</th><th style="text-align: center;">方差</th></tr></thead><tbody><tr class="odd"><td>均匀分布(Uniform)</td><td><span class="math inline">\(f(x ; a, b)=\left\{\begin{array}{l}\frac{1}{b-a} \text { for } x \in[a, b] \\ 0 \quad \text { otherwise }\end{array}\right.\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{a+b}{2}\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{(b-a)^{2}}{12}\)</span></td></tr><tr class="even"><td>正态分布/高斯分布(Normal)</td><td><span class="math inline">\(f\left(x ; \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}\)</span></td><td style="text-align: center;"><span class="math inline">\(\mu\)</span></td><td style="text-align: center;"><span class="math inline">\(\sigma^{2}\)</span></td></tr><tr class="odd"><td>学生t分布(Student T)</td><td><span class="math inline">\(\frac{Z}{\sqrt{U / k}}\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{k}{k-2}\)</span></td></tr><tr class="even"><td>卡方分布(Chi Squared)</td><td><span class="math inline">\(\sum_{i=1}^{k} Z_{i}^{2} \quad Z_{i} \stackrel{i: i, d}{\sim} N(0,1)\)</span></td><td style="text-align: center;"><span class="math inline">\(k\)</span></td><td style="text-align: center;"><span class="math inline">\(2 k\)</span></td></tr><tr class="odd"><td>指数分布(Exponential)</td><td><span class="math inline">\(f(x ; \lambda)=\left\{\begin{array}{ll}\lambda e^{-\lambda x} &amp; \text { if } x \geq 0 \\ 0 &amp; \text { otherwise }\end{array}\right.\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{1}{\lambda}\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{1}{\lambda^{2}}\)</span></td></tr><tr class="even"><td>F分布(F)</td><td><span class="math inline">\(\begin{array}{ll}\frac{U_{1} / d_{1}}{U_{2} / d_{2}} &amp; U_{1} \sim \chi_{d_{1}} \\ &amp; U_{2} \sim \chi_{d_{2}}\end{array}\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{d_{2}}{d_{2}-2}\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{2 d_{2}^{2}\left(d_{1}+d_{2}-2\right)}{d_{1}\left(d_{2}-2\right)^{2}\left(d_{2}-4\right)}\)</span></td></tr><tr class="odd"><td>Gamma分布(Gamma)</td><td><span class="math inline">\(f(x ; k, \theta)=\frac{1}{\Gamma(k) \theta^{k}} x^{k-1} e^{-\frac{x}{\theta}}\)</span></td><td style="text-align: center;"><span class="math inline">\(k\theta\)</span></td><td style="text-align: center;"><span class="math inline">\(k\theta^{2}\)</span></td></tr></tbody></table><h3 id="中心极限定理">中心极限定理</h3><p>中心极限定理告诉我们，对于一个（性质比较好的）分布，如果我们有足够大的独立同分布的样本，其样本均值会（近似地）呈正态分布。样本数量越大，其分布与正态越接近。</p><h2 id="统计推断">统计推断</h2><p>通过观察数据来确定背后的概率分布</p><h3 id="频率学派">频率学派</h3><h4 id="点估计理论">点估计理论</h4><p>统计学中一个主要的问题是估计参数。我们用一个取值为样本的函数来估计我们感兴趣的参数，并称这个函数为估计量。这里我们用一个估计圆周率π的例子来具体说明这个想法。 我们知道π可以由圆与其外切正方形的面积比来表示：</p><h4 id="置信区间">置信区间</h4><p>与点估计不同，置信区间用估计的是一个参数的范围。一个置信区间对应着一个置信水平：一个置信水平为95%的置信区间表示这个置信区间包含了真实参数的概率为95%。</p><h4 id="bootstrap方法">Bootstrap方法</h4><p>许多频率学派的统计推断侧重于使用一些“性质比较良好”的估计量。但是我们知道这些统计量本身是样本的函数，因此往往比较难分析它们自己的概率分布。而Bootstrap方法则给我们提供了一种方便的近似确定估计量性质的方法。下面我们通过一个例子来说明Bootstrap方法。假设我们现在有 <span class="math inline">\(n\)</span> 个独立的样本 <span class="math inline">\(X_{1}, \ldots, X_{n}\)</span>，基于这些样本我们就有了一个经验分布函数：</p><p><span class="math display">\[F_{n}(x)=\sum_{i=1}^{n} 1_{\left\{X_{i} \leq x\right\}}\]</span></p><p>我们可以重复根据这个经验分布函数生成样本，利用这些新的样本来估计元样本均值的标准差。</p><h3 id="贝叶斯学派">贝叶斯学派</h3><p>用数据来更新特定假设的概率</p><h4 id="贝叶斯公式">贝叶斯公式</h4><h4 id="似然函数">似然函数</h4><p><span class="math display">\[L(\theta | x)=P(x | \theta)\]</span></p><p>似然函数的概念在频率学派和贝叶斯学派中都有重要的作用。</p><h4 id="从先验概率到后验概率">从先验概率到后验概率</h4><p>贝叶斯统计的核心思想是利用观察到的数据来更新先验信息。</p><h2 id="回归分析">回归分析</h2><p>建立两个变量之间线性模型的方法</p><h3 id="最小二乘法">最小二乘法</h3><p>最小二乘法是一个估计线性模型参数的方法。这个方法的目标是找到一组线性模型参数，使得这个模型预测的数据和实际数据间的平方误差达到最小。</p><h3 id="相关性">相关性</h3><p>相关性是一种刻画两个变量之间线性关系的度量。相关性的数学定义是</p><p><span class="math display">\[r=\frac{s_{x y}}{\sqrt{s_{x x}} \sqrt{s_{y y}}}\]</span></p><p>其中</p><p><span class="math display">\[\begin{aligned}&amp;\begin{array}{l}s_{x y}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right) \\s_{x x}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\end{array}\\&amp;s_{y y}=\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}\end{aligned}\]</span></p><p>由上述定义我们可以看出 <span class="math inline">\(r \in[-1.1]\)</span>。</p><h3 id="方差分析">方差分析</h3><p>方差分析（ANONA，Analysis of Variace）是一种检验各组数据是否有相同均值的统计学方法。方差分析将t检验从检验两组数据均值推广到检验多组数据均值，其主要方法是比较组内和组间平方误差。</p><p>可视化网站：<a href="https://seeing-theory.brown.edu/cn.html" target="_blank" rel="noopener" class="uri">https://seeing-theory.brown.edu/cn.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/11/seeing-theory/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;看见统计&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;看见统计由Daniel Kunin在布朗大学读本科的时候开始制作。致力于用数据可视化让统计概念更容易理解。 (数据可视化使用Mike Bostock的javascript库D3.js制作。)&lt;/p&gt;
    
    </summary>
    
    
      <category term="math" scheme="https://2020.iosdevlog.com/categories/math/"/>
    
    
      <category term="Godot" scheme="https://2020.iosdevlog.com/tags/Godot/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习：Python实践》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/03/10/9787121331107/"/>
    <id>https://2020.iosdevlog.com/2020/03/10/9787121331107/</id>
    <published>2020-03-10T13:53:06.000Z</published>
    <updated>2020-03-11T15:48:23.891Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/1.jpg" alt="" /><figcaption>《机器学习：Python实践》</figcaption></figure><p>书名：机器学习：Python实践<br />作者：魏贞原出版社：电子工业出版社<br />出版时间：2018-01<br />ISBN：9787121331107</p><p>GitHub: <a href="https://github.com/weizy1981/MachineLearning" target="_blank" rel="noopener" class="uri">https://github.com/weizy1981/MachineLearning</a></p><p>这本书是写给对机器学习感兴趣和立志学习机器学习的 <code>Python</code> 程序员的，是一本关于机器学习实践的书籍。</p><a id="more"></a><p><strong>这不是一本关于机器学习的教科书</strong></p><p>本书只会简单介绍机器学习的基本原理和算法。在这里假设你已经掌握了机器学习的基础知识，或者有能力自己来完成机器学习的基础知识的学习。</p><p><strong>这不是一本算法书</strong></p><p>本书不会详细介绍机器学习的算法。在这里假设你已经掌握了机器学习的相关算法，或者能够独立完成相关算法知识的学习。这不是一本关于Python的语法书。</p><p><strong>本书不会花费大量的篇幅来讲解Python的语法</strong></p><p>在这里假设你是一个经验丰富的开发人员，能够快速掌握一种类似于C语言的开发语言。</p><h2 id="内容简介">内容简介</h2><p>本书系统地讲解了机器学习的基本知识，以及在实际项目中使用机器学习的基本步骤和方法；详细地介绍了在进行数据处理、分析时怎样选择合适的算法，以及建立模型并优化等方法，通过不同的例子展示了机器学习在具体项目中的应用和实践经验，是一本非常好的机器学习入门和实践的书籍。</p><p>不同于很多讲解机器学习的书籍，本书以实践为导向，使用scikit-learn作为编程框架，强调简单、快速地建立模型，解决实际项目问题。读者通过对本书的学习，可以迅速上手实践机器学习，并利用机器学习解决实际问题。</p><h2 id="第一部分-初始">第一部分 初始</h2><blockquote><p>像一个优秀的工程师一样使用机器学习，而不要像一个机器学习专家一样使用机器学习方法。</p></blockquote><p>—— Google</p><h3 id="初识机器学习">初识机器学习</h3><p>学习机器学习的误区</p><ol type="1"><li>必须非常熟悉Python的语法和擅长Python的编程。</li><li>非常深入地学习和理解在scikit-learn中使用的机器学习的理论和算法。</li><li>避免或者很少参与完成项目，除机器学习之外的部分。</li></ol><h4 id="什么是机器学习">什么是机器学习</h4><p>机器学习（Machine Learning, ML）是一门多领域的交叉学科，涉及概率论、统计学、线性代数、算法等多门学科。</p><p>它专门研究计算机如何模拟和学习人的行为，以获取新的知识或技能，重新组织已有的知识结构使之不断完善自身的性能。机器学习已经有了十分广泛的应用，例如：数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。</p><h4 id="python-中的机器学习">Python 中的机器学习</h4><p>利用机器学习的预测模型来解决问题共有六个基本步骤</p><ol type="1"><li>定义问题：研究和提炼问题的特征，以帮助我们更好地理解项目的目标。</li><li>数据理解：通过描述性统计和可视化来分析现有的数据。</li><li>数据准备：对数据进行格式化，以便于构建一个预测模型。</li><li>评估算法：通过一定的方法分离一部分数据，用来评估算法模型，并选取一部分代表数据进行分析，以改善模型。</li><li>优化模型：通过调参和集成算法提升预测结果的准确度。</li><li>结果部署：完成模型，并执行模型来预测结果和展示。</li></ol><p><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/3.jpg" /></p><h4 id="学习机器学习的原则">学习机器学习的原则</h4><ul><li>学习机器学习是一段旅程。<ul><li>需要知道自己具备的技能、目前所掌握的知识，以及明确要达到的目标。要实现自己的目标需要付出时间和辛勤的工作，但是在目标的实现过程中，有很多工具可以帮助你快速达成目标。</li></ul></li><li>创建半正式的工作产品。<ul><li>以博客文章、技术报告和代码存储的形式记下学习和发现的内容，快速地为自己和他人提供一系列可以展示的技能、知识及反思。</li></ul></li><li>实时学习。<ul><li>不能仅在需要的时候才学习复杂的主题，例如，应该实时学习足够的概率和线性代数的指示来帮助理解正在处理的算法。在开始进入机器学习领域之前，不需要花费太多的时间来专门学习统计和数学方面的知识，而是要在平时进行实时学习，积累知识。</li></ul></li><li>利用现有的Skills。<ul><li>如果可以编码，那么通过实现算法来理解它们，而不是研究数学理论。使用自己熟悉的编程语言，让自己专注于正在学习的一件事情上，不要同时学习一种新的语言、工具或类库，这样会使学习过程复杂化。</li></ul></li><li>掌握是理想。<ul><li>掌握机器学习需要持续不断的学习。也许你永远不可能实现掌握机器学习的目标，只能持续不断地学习和改进所掌握的知识。</li></ul></li></ul><h4 id="学习机器学习的技巧">学习机器学习的技巧</h4><ul><li>启动一个可以在一个小时内完成的小项目。</li><li>通过每周完成一个项目来保持你的学习势头，并建立积累自己的项目工作区。</li><li>在微博、微信、Github等社交工具上分享自己的成果，或者随时随地地展示自己的兴趣，增加技能、知识，并获得反馈。</li></ul><h3 id="python-机器学习的生态圈">Python 机器学习的生态圈</h3><h4 id="python">Python</h4><figure><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/4.jpg" alt="" /><figcaption>TIOBE 2017年6月</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/5.jpg" alt="" /><figcaption>PYPL</figcaption></figure><h4 id="scipy">SciPy</h4><p>SciPy是在数学运算、科学和工程学方面被广泛应用的Python类库。它包括统计、优化、整合、线性代数模块、傅里叶变换、信号和图像处理、常微分方程求解器等，因此被广泛地应用在机器学习项目中。SciPy依赖以下几个与机器学习相关的类库。</p><ul><li><strong>NumPy</strong>：是Python的一种开源数值计算扩展。它可用来存储和处理大型矩阵，提供了许多高级的数值编程工具，如矩阵数据类型、矢量处理、精密的运算库。</li><li><strong>Matplotlib</strong>:Python中最著名的2D绘图库，十分适合交互式地进行制图；也可以方便地将它作为绘图控件，嵌入GUI应用程序中。</li><li><strong>Pandas</strong>：是基于NumPy的一种工具，是为了解决数据分析任务而创建的。Pandas纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具，也提供了大量能使我们快速、便捷地处理数据的函数和方法。</li></ul><h4 id="scikit-learn">scikit-learn</h4><p>scikit-learn是Python中开发和实践机器学习的著名类库之一，依赖于SciPy及其相关类库来运行。scikit-learn的基本功能主要分为六大部分：</p><ol type="1"><li>分类</li><li>回归</li><li>聚类</li><li>数据降维</li><li>模型选择</li><li>数据预处理</li></ol><h4 id="环境安装">环境安装</h4><h3 id="第一个机器学习项目">第一个机器学习项目</h3><h4 id="机器学习中的-hello-world-项目">机器学习中的 Hello World 项目</h4><p>鸢尾花（Iris Flower）进行分类</p><h4 id="导入数据">导入数据</h4><h4 id="概述数据">概述数据</h4><h4 id="数据可视化">数据可视化</h4><h4 id="评估算法">评估算法</h4><h4 id="实施预测">实施预测</h4><h3 id="python-和-scipy-速成">Python 和 SciPy 速成</h3><h4 id="python-速成">Python 速成</h4><h4 id="numpy-速成">NumPy 速成</h4><h4 id="matplotlib-速成">Matplotlib 速成</h4><h4 id="pandas-速成">Pandas 速成</h4><h2 id="第二部分-数据理解">第二部分 数据理解</h2><h3 id="数据导入">数据导入</h3><h4 id="csv-文件">CSV 文件</h4><h4 id="pima-indians-数据集">Pima Indians 数据集</h4><h4 id="采用标准-python-类库导入数据">采用标准 Python 类库导入数据</h4><h4 id="采用-numpy-导入数据">采用 NumPy 导入数据</h4><h4 id="采用-pandas-导入数据">采用 Pandas 导入数据</h4><h3 id="数据理解">数据理解</h3><h4 id="简单地查看数据">简单地查看数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="comment"># 显示数据的前10行</span></span><br><span class="line">filename = <span class="string">'pima_data.csv'</span></span><br><span class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>, <span class="string">'class'</span>]</span><br><span class="line">data = read_csv(filename, names=names)</span><br><span class="line">peek = data.head(<span class="number">10</span>)</span><br><span class="line">print(peek)</span><br></pre></td></tr></table></figure><p><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/6.jpg" /></p><h4 id="数据的维度">数据的维度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="comment"># 显示数据的行和列数据</span></span><br><span class="line">filename = <span class="string">'pima_data.csv'</span></span><br><span class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>,</span><br><span class="line"><span class="string">'class'</span>]</span><br><span class="line">data = read_csv(filename, names=names)</span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(768, 9)</span><br></pre></td></tr></table></figure><h4 id="数据属性和类型">数据属性和类型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="comment"># 显示数据的行和列数据</span></span><br><span class="line">filename = <span class="string">'pima_data.csv'</span></span><br><span class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>,</span><br><span class="line"><span class="string">'class'</span>]</span><br><span class="line">data = read_csv(filename, names=names)</span><br><span class="line">print(data.dtypes)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">preg     int64</span><br><span class="line">plas     int64</span><br><span class="line">pres     int64</span><br><span class="line">skin     int64</span><br><span class="line">test     int64</span><br><span class="line">mass   float64</span><br><span class="line">pedi   float64</span><br><span class="line">age       int64</span><br><span class="line">class     int64</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure><h4 id="描述性统计">描述性统计</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> set_option</span><br><span class="line"><span class="comment"># 描述性统计</span></span><br><span class="line">filename = <span class="string">'pima_data.csv'</span></span><br><span class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>,</span><br><span class="line"><span class="string">'class'</span>]</span><br><span class="line">data = read_csv(filename, names=names)</span><br><span class="line">set_option(<span class="string">'display.width'</span>, <span class="number">100</span>)</span><br><span class="line"><span class="comment"># 设置数据的精确度</span></span><br><span class="line">set_option(<span class="string">'precision'</span>, <span class="number">4</span>)</span><br><span class="line">print(data.describe())</span><br></pre></td></tr></table></figure><p><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/7.jpg" /></p><h4 id="数据分组分布适用于分类算法">数据分组分布（适用于分类算法）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="comment"># 数据分类分布统计</span></span><br><span class="line">filename = <span class="string">'pima_data.csv'</span></span><br><span class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>,</span><br><span class="line"><span class="string">'class'</span>]</span><br><span class="line">data = read_csv(filename, names=names)</span><br><span class="line">print(data.groupby(<span class="string">'class'</span>).size())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class</span><br><span class="line">0   500</span><br><span class="line">1   268</span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure><h4 id="数据属性的相关性">数据属性的相关性</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> set_option</span><br><span class="line"><span class="comment"># 显示数据的相关性</span></span><br><span class="line">filename = <span class="string">'pima_data.csv'</span></span><br><span class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>,</span><br><span class="line"><span class="string">'class'</span>]</span><br><span class="line">data = read_csv(filename, names=names)</span><br><span class="line">set_option(<span class="string">'display.width'</span>, <span class="number">100</span>)</span><br><span class="line"><span class="comment"># 设置数据的精确度</span></span><br><span class="line">set_option(<span class="string">'precision'</span>, <span class="number">2</span>)</span><br><span class="line">print(data.corr(method=<span class="string">'pearson'</span>))</span><br></pre></td></tr></table></figure><p><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/8.jpg" /></p><h4 id="数据的分布分析">数据的分布分析</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="comment"># 计算数据的高斯偏离</span></span><br><span class="line">filename = <span class="string">'pima_data.csv'</span></span><br><span class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>,</span><br><span class="line"><span class="string">'class'</span>]</span><br><span class="line">data = read_csv(filename, names=names)</span><br><span class="line">print(data.skew())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">preg   0.901674</span><br><span class="line">plas   0.173754</span><br><span class="line">pres   -1.843608</span><br><span class="line">skin   0.109372</span><br><span class="line">test   2.272251</span><br><span class="line">mass   -0.428982</span><br><span class="line">pedi   1.919911</span><br><span class="line">age     1.129597</span><br><span class="line">class   0.635017</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><h3 id="数据可视化-1">数据可视化</h3><h4 id="单一图表">单一图表</h4><h4 id="多重图表">多重图表</h4><h2 id="第三部分-数据准备">第三部分 数据准备</h2><blockquote><p>特征选择是困难耗时的，也需要对需求的理解和专业知识的掌握。在机器学习的应用开发中，最基础的是特征工程。</p></blockquote><p>——吴恩达</p><p>“使用正确的特征来构建正确的模型，以完成既定的任务”。</p><h3 id="数据预处理">数据预处理</h3><p>数据预处理大致分为三个步骤：</p><ol type="1"><li>数据的准备</li><li>数据的转换</li><li>数据的输出</li></ol><h4 id="为什么需要数据预处理">为什么需要数据预处理</h4><h4 id="格式化数据">格式化数据</h4><h4 id="调整数据尺度">调整数据尺度</h4><h4 id="正态化数据">正态化数据</h4><h4 id="标准化数据">标准化数据</h4><h4 id="二值数据">二值数据</h4><h3 id="数据特征选定">数据特征选定</h3><h4 id="特征选定">特征选定</h4><h4 id="单变量特征选定">单变量特征选定</h4><h4 id="递归特征消除">递归特征消除</h4><h4 id="主要成分分析">主要成分分析</h4><h4 id="特征重要性">特征重要性</h4><h2 id="第四部分-选择模型">第四部分 选择模型</h2><h3 id="评估算法-1">评估算法</h3><h4 id="评估算法的方法">评估算法的方法</h4><h4 id="分离训练数据集和评估数据集">分离训练数据集和评估数据集</h4><h4 id="k-折交叉验证分离">K 折交叉验证分离</h4><h4 id="弃一交叉验证分离">弃一交叉验证分离</h4><h4 id="重复随机分离评估数据集与训练数据集">重复随机分离评估数据集与训练数据集</h4><h3 id="算法评估矩阵">算法评估矩阵</h3><h4 id="算法评估矩阵-1">算法评估矩阵</h4><h4 id="分类算法矩阵">分类算法矩阵</h4><h4 id="回归算法矩阵">回归算法矩阵</h4><h3 id="审查分类算法">审查分类算法</h3><h4 id="算法审查">算法审查</h4><h4 id="算法概述">算法概述</h4><h4 id="线性算法">线性算法</h4><h4 id="非线性算法">非线性算法</h4><h3 id="审查回归算法">审查回归算法</h3><h4 id="算法概述-1">算法概述</h4><h4 id="线性算法-1">线性算法</h4><h4 id="非线性算法-1">非线性算法</h4><h3 id="算法比较">算法比较</h3><h4 id="选择最佳的机器学习算法">选择最佳的机器学习算法</h4><h4 id="机器学习算法的比较">机器学习算法的比较</h4><h3 id="自动流程">自动流程</h3><h4 id="机器学习的自动流程">机器学习的自动流程</h4><h4 id="数据准备和生成模型的-pipeline">数据准备和生成模型的 Pipeline</h4><h4 id="特征选择和生成模型的-pipeline">特征选择和生成模型的 Pipeline</h4><h2 id="第五部分-优化模型">第五部分 优化模型</h2><h3 id="集成算法">集成算法</h3><h4 id="集成的方法">集成的方法</h4><h4 id="装袋算法">装袋算法</h4><h4 id="提升算法">提升算法</h4><h4 id="投票算法">投票算法</h4><h3 id="算法调参">算法调参</h3><h4 id="机器学习算法调参">机器学习算法调参</h4><h4 id="网格搜索优化参数">网格搜索优化参数</h4><h4 id="随机搜索优化参数">随机搜索优化参数</h4><h2 id="第六部分-结果部署">第六部分 结果部署</h2><h3 id="持久化加载模型">持久化加载模型</h3><h4 id="通过-pickle-序列化和反序列化机器学习的模型">通过 pickle 序列化和反序列化机器学习的模型</h4><h4 id="通过-joblib-序列化和反序列化机器学习的模型">通过 joblib 序列化和反序列化机器学习的模型</h4><h4 id="生成模型的技巧">生成模型的技巧</h4><h2 id="第七部分-项目实践">第七部分 项目实践</h2><h3 id="预测模型项目模板">预测模型项目模板</h3><h4 id="在项目中实践机器学习">在项目中实践机器学习</h4><h4 id="机器学习项目的-python-模板">机器学习项目的 Python 模板</h4><h4 id="各步骤的详细说明">各步骤的详细说明</h4><h4 id="使用模板的小技巧">使用模板的小技巧</h4><h3 id="回归项目实例">回归项目实例</h3><h4 id="定义问题">定义问题</h4><h4 id="导入数据-1">导入数据</h4><h4 id="理解数据">理解数据</h4><h4 id="数据可视化-2">数据可视化</h4><h4 id="分离评估数据集">分离评估数据集</h4><h4 id="评估算法-2">评估算法</h4><h4 id="调参改善算法">调参改善算法</h4><h4 id="集成算法-1">集成算法</h4><h4 id="集成算法调参">集成算法调参</h4><h4 id="确定最终模型">20.10 确定最终模型</h4><h3 id="二分类实例">二分类实例</h3><h4 id="问题定义">问题定义</h4><h4 id="导入数据-2">导入数据</h4><h4 id="分析数据">分析数据</h4><h4 id="分离评估数据集-1">分离评估数据集</h4><h4 id="评估算法-3">评估算法</h4><h4 id="算法调参-1">算法调参</h4><h4 id="集成算法-2">集成算法</h4><h4 id="确定最终模型-1">确定最终模型</h4><h3 id="文本分类实例">文本分类实例</h3><h4 id="问题定义-1">问题定义</h4><h4 id="导入数据-3">导入数据</h4><h4 id="文本特征提取">文本特征提取</h4><h4 id="评估算法-4">评估算法</h4><h4 id="算法调参-2">算法调参</h4><h4 id="集成算法-3">集成算法</h4><h4 id="集成算法调参-1">集成算法调参</h4><h4 id="确定最终模型-2">确定最终模型</h4><p><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/9.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/10.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/11.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/12.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/13.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/14.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/15.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/16.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/17.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/18.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/19.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/20.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/21.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/22.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/23.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/24.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/25.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/10/9787121331107/26.jpg" /></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/10/9787121331107/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《机器学习：Python实践》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：机器学习：Python实践&lt;br /&gt;
作者：魏贞原出版社：电子工业出版社&lt;br /&gt;
出版时间：2018-01&lt;br /&gt;
ISBN：9787121331107&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a href=&quot;https://github.com/weizy1981/MachineLearning&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://github.com/weizy1981/MachineLearning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这本书是写给对机器学习感兴趣和立志学习机器学习的 &lt;code&gt;Python&lt;/code&gt; 程序员的，是一本关于机器学习实践的书籍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="ML" scheme="https://2020.iosdevlog.com/tags/ML/"/>
    
      <category term="Python" scheme="https://2020.iosdevlog.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>2020年每位数据科学家必读的50本免费书籍</title>
    <link href="https://2020.iosdevlog.com/2020/03/09/50-books/"/>
    <id>https://2020.iosdevlog.com/2020/03/09/50-books/</id>
    <published>2020-03-09T14:35:40.000Z</published>
    <updated>2020-03-09T15:53:54.853Z</updated>
    
    <content type="html"><![CDATA[<p>数据科学是一个跨学科领域，包含来自统计，机器学习，贝叶斯等领域的方法和技术。它们都旨在从数据中产生特定的见解。在本文中，我们列出了一些出色的数据科学书籍，其中涵盖了数据科学下的各种主题。</p><a id="more"></a><h3 id="数据分析风格的要素-the-element-of-data-analytic-style">1. <a href="https://leanpub.com/datastyle" target="_blank" rel="noopener">数据分析风格的要素 / The Element of Data Analytic Style</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/B1.png" alt="" /><figcaption>The Element of Data Analytic Style</figcaption></figure><p>本书概述了数据科学。数据科学是一个很大的概括性术语，对于那些初次尝试涉足这一领域的人来说，这本书非常有用。阅读它以了解什么是数据科学，什么是一些常规任务和算法，以及一些常规技巧和窍门。</p><h3 id="数据科学基础-foundations-of-data-science">2. <a href="https://www.cs.cornell.edu/jeh/book.pdf" target="_blank" rel="noopener">数据科学基础 / Foundations of Data Science</a></h3><p>数据科学的基础是一些选定领域的论文，这些领域构成了数据科学的基础，例如线性代数，LDA，马尔可夫链，机器学习基础和统计。本书的理想读者是希望使他们在数学和理论上更好地掌握该领域的初学者数据科学家。</p><h3 id="海量数据集的挖掘-mining-of-massive-datasets">3. <a href="http://infolab.stanford.edu/~ullman/mmds/book0n.pdf" target="_blank" rel="noopener">海量数据集的挖掘 / Mining of Massive Datasets</a></h3><p>本书基于斯坦福大学CS246和CS35A课程，可帮助用户学习在大型数据集上进行数据挖掘的主题。数据科学家通常需要解决的一个非常普遍的问题是在非常大的数据集上执行简单的数字任务（您可以通过编写小程序来完成）。MMDS正是为此而努力。除此之外，您还有诸如降维和推荐系统之类的主题，可以帮助您了解线性代数和度量距离在现实世界中的应用。所有数据科学家的必读资料。</p><h3 id="python数据科学手册-python-data-science-handbook">4. <a href="https://jakevdp.github.io/PythonDataScienceHandbook/" target="_blank" rel="noopener">Python数据科学手册 / Python Data Science Handbook</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/PDSH-cover.png" alt="" /><figcaption>PDSH-cover</figcaption></figure><p>Python数据科学手册教了各种数据科学概念在Python中的应用。也许这是学习Python数据科学的最好书（仅相当于  <a href="https://github.com/wesm/pydata-book" target="_blank" rel="noopener">Wes McKinney的鼠标书</a>），这本书也可以在Github上免费阅读。因此，您无需花任何钱就可以学习。</p><h3 id="动手机器学习和大数据-hands-on-machine-learning-and-big-data">5. <a href="http://www.kareemalkaseer.com/books/ml/" target="_blank" rel="noopener">动手机器学习和大数据 / Hands-on Machine Learning and Big Data</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/Hands_on.png" alt="" /><figcaption>Hands_on</figcaption></figure><h3 id="统计思想-think-stats">6. <a href="http://greenteapress.com/thinkstats/" target="_blank" rel="noopener">统计思想 / Think Stats</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/think_stats_comp.png" alt="" /><figcaption>think_stats_comp</figcaption></figure><p>Think Stats教给读者统计学的基础知识，也就是说，读者将在现实世界的数据集上应用统计学的概念和分布，并尝试使用数学特征来了解有关数据的更多信息。如果您想使用Python学习统计信息，可能是最好的入门书籍之一。</p><h3 id="贝叶斯思想-think-bayes">7. <a href="https://greenteapress.com/wp/think-bayes/" target="_blank" rel="noopener">贝叶斯思想 / Think Bayes</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/think_bayes_cover_medium.png" alt="" /><figcaption>think_bayes_cover_medium</figcaption></figure><p>贝叶斯统计的工作原理与正常统计有所不同。不确定性和对真实数据集的拟合分布的概念使贝叶斯方法更适合于学习真实数据集。唐尼教授非常酷的“通过使用Python进行编程学习”的风格使这本书成为那些开始使用贝叶斯方法的人的不二之选。</p><h3 id="线性动力系统简介-introduction-to-linear-dynamical-systems">8. <a href="http://ee263.stanford.edu/" target="_blank" rel="noopener">线性动力系统简介 / Introduction to Linear Dynamical Systems</a></h3><p>这本书讲授了实际系统中的应用线性代数。这些应用程序涉及电路，信号处理，通信和控制系统。可在<a href="https://web.stanford.edu/class/archive/ee/ee263/ee263.1082/notes/ee263coursereader.pdf" target="_blank" rel="noopener">此处</a>找到与博伊德教授前几年课程笔记的链接  。</p><h3 id="凸优化-convex-optimization">9. <a href="http://stanford.edu/~boyd/cvxbook/" target="_blank" rel="noopener">凸优化 / Convex Optimization</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/bv_cvxbook_cover.jpg" alt="" /><figcaption>bv_cvxbook_cover</figcaption></figure><p>凸优化是许多机器学习（以及几乎所有深度学习算法）算法在后台用于获得最佳参数集的功能。</p><h3 id="启发式方法基础-essentials-of-metaheuristics">10. <a href="https://cs.gmu.edu/~sean/book/metaheuristics/" target="_blank" rel="noopener">启发式方法基础 / Essentials of Metaheuristics</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/Luke2013.jpg" alt="" /><figcaption>Luke2013</figcaption></figure><p>启发式方法是一种快速学习概率方法来完成任务的方法，这些方法可能需要您编写程序才能使用蛮力搜索。对于也许较小的数据，蛮力方法的实现工作量较小，但是随着添加的数据量的增加，它们将很快耗尽。这本书可能是遗传算法，爬山，协同进化和（基本）强化学习等元启发式方法的最佳介绍。</p><h3 id="python中的机器学习数据科学机器学习和人工智能的主要发展和技术趋势-machine-learning-in-python-main-developments-and-technology-trends-in-data-science-machine-learning-and-artificial-intelligence">11. <a href="https://arxiv.org/abs/2002.04803" target="_blank" rel="noopener">Python中的机器学习：数据科学，机器学习和人工智能的主要发展和技术趋势 / Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence</a></h3><p>数据科学中的Python工具的良好概述。对于想要进入数据科学领域的高级Python开发人员或从R for Data Science进入Python的人员来说，这是一个非常不错的文档。总体而言，如果您想了解Python对数据科学的作用，则应阅读本文。</p><h3 id="应用数据科学-applied-data-science">12. <a href="https://columbia-applied-data-science.github.io/appdatasci.pdf" target="_blank" rel="noopener">应用数据科学 / Applied Data Science</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/ads.png" alt="" /><figcaption>ads</figcaption></figure><p>Langmore和Krasner撰写的Applied Data Science是一本采用非常实用的方法教授Data Science的书。通过使用Git进行基础Python的教学，本书继续构建了各种算法的基础知识，这些算法在数据科学领域中经​​常使用。</p><h3 id="强盗书-bandit-book">13. <a href="https://tor-lattimore.com/downloads/book/book.pdf" target="_blank" rel="noopener">强盗书 / Bandit Book</a></h3><p>随着越来越多的数据积累，决策不再只是直觉的功能，而是所收集数据的功能。电子商务网站上用于进行药物测试和财务投资组合决策的“购买”按钮的正确颜色是什么，随处都使用强盗算法？一本很好的让自己熟悉“匪徒”的书！</p><h3 id="注释算法-annotated-algorithms">14. <a href="https://tor-lattimore.com/downloads/book/book.pdf" target="_blank" rel="noopener">注释算法 / Annotated Algorithms</a></h3><p>教您用Python编写许多数值算法的书。如果您想学习数学程序的实现方式，或者想通过有趣的问题陈述学习Python，这是一个极好的资源。</p><h3 id="计算机时代统计推断-computer-age-statistical-inference">15. <a href="https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf" target="_blank" rel="noopener">计算机时代统计推断 / Computer Age Statistical Inference</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/casi.png" alt="" /><figcaption>casi</figcaption></figure><p>埃夫隆（Efron）和传奇的哈斯提（Hastie）所著的一本书，思考了如何利用当今可用的计算能力，而不是大多数其他书籍所采用的笔纸方法，在现代进行统计推断（常客和贝叶斯）。这是打算在现实生活中使用“统计信息”的任何人（初学者或有经验的人）必须阅读的内容。</p><h3 id="因果推论书-causal-inference-book">16. <a href="https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/" target="_blank" rel="noopener">因果推论书 / Causal Inference Book</a></h3><p>“关联不是因果关系”是数据科学家经常使用的短语。但是如何将两者分开？本书通过向数据科学家介绍因果推理技术来提供答案。您将需要良好的概率基础来阅读它，而不是针对初学者。</p><h3 id="计算最优运输-computational-optimal-transport">17. <a href="https://arxiv.org/abs/1803.00567" target="_blank" rel="noopener">计算最优运输 / Computational Optimal Transport</a></h3><p>最优运输是从一组分布到另一组分布的分配数学。这可能是数据科学领域中获得过多个菲尔兹奖章（数学领域的最高荣誉）的少数领域之一。数学概念已在许多机器学习和深度学习算法中用作距离度量和分配问题解决方案。</p><h3 id="计算机科学与机器学习的代数拓扑微积分和优化理论-algebra-topology-differential-calculus-and-optimization-theory-for-computer-science-and-machine-learning">18. <a href="https://www.cis.upenn.edu/~jean/math-deep.pdf" target="_blank" rel="noopener">计算机科学与机器学习的代数，拓扑，微积分和优化理论 / Algebra, Topology, Differential Calculus and Optimization Theory for Computer Science and Machine Learning</a></h3><p>该书旨在教授计算机科学和机器学习所需的各种数学领域。对于那些想从数学重领域进入数据科学领域的人来说，这是相当不错的数学知识和丰富的资源。</p><h3 id="数据挖掘与分析-data-mining-and-analysis">19. <a href="http://www.dataminingbook.info/pmwiki.php" target="_blank" rel="noopener">数据挖掘与分析 / Data Mining and Analysis</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/bookpic.jpg" alt="" /><figcaption>bookpic</figcaption></figure><p>正如您在前面提到的更著名的MMDS书中可能已经看到的那样，数据挖掘是一种有效地对大型数据集进行计算的方法。这些计算可以通过蛮力方法完成，并且在小型数据集上可能效果很好，但是在大型数据集上运行可能要花很长时间。很好的数据挖掘入门和参考书。</p><h3 id="计算与推理-computational-and-inferential-thinking">20. <a href="https://www.inferentialthinking.com/chapters/intro.html" target="_blank" rel="noopener">计算与推理 / Computational and Inferential Thinking</a></h3><p>从使用Python进行编程，因果关系，表，可视化和基本统计​​信息的角度研究数据科学的各个方面。从加州大学伯克利分校的一门基础课程开始，对初学者来说是一个很好的资源。</p><h3 id="数据科学的数学基础-mathematical-foundations-of-data-science">21. <a href="https://mathematical-tours.github.io/book-sources/FundationsDataScience.pdf" target="_blank" rel="noopener">数据科学的数学基础 / Mathematical Foundations of Data Science</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/mfd.png" alt="" /><figcaption>mfd</figcaption></figure><p>顾名思义，本书给出并解释了诸如凸优化和降维之类的数据科学概念背后的数学论文。如果您喜欢数学，或者特别想学习这些概念背后的数学，则推荐这本书。</p><h3 id="聪明人的信息论-information-theory-for-intelligent-people">22. <a href="http://tuvalu.santafe.edu/~simon/it.pdf" target="_blank" rel="noopener">聪明人的信息论 / Information Theory for Intelligent People</a></h3><p>信息论是与线性代数，凸优化和统计一起在数据科学中发现的四种数学理论之一。这是理解理论的好教程。好消息是，初学者可以使用本教程。</p><h3 id="应用线性代数简介-vmls书-introduction-to-applied-linear-algebra-the-vmls-book">23. <a href="http://vmls-book.stanford.edu/" target="_blank" rel="noopener">应用线性代数简介– VMLS书 / Introduction to Applied Linear Algebra – The VMLS Book</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/vmls_cover.jpg" alt="" /><figcaption>vmls_cover</figcaption></figure><p>在此列表中我将提到的众多书中，我最喜欢的线性代数书。初学者可以使用它，并具有非常实用的感觉，而不会使读者迷失于许多数学概念中。</p><h3 id="线性代数-hefferon-linear-algebra-hefferon">24. <a href="http://joshua.smcvt.edu/linearalgebra/" target="_blank" rel="noopener">线性代数– Hefferon / Linear Algebra – Hefferon</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/bookcover.png" alt="" /><figcaption>bookcover</figcaption></figure><p>许多人认为，这是Strong圣经之后可获得的最好的初学者线性代数资源。（在SAGE中进行编程练习，基本上是Python）也很实用，但是对于初学者来说，比从业者更多。</p><h3 id="线性代数作为抽象数学的入门-linear-algebra-as-an-introduction-to-abstract-mathematics">25. <a href="https://www.math.ucdavis.edu/~anne/linear_algebra/" target="_blank" rel="noopener">线性代数–作为抽象数学的入门 / Linear Algebra – As An Introduction to Abstract Mathematics</a></h3><p>这本书感觉就像我的大学线性代数书（许多与我一起学习工程学的学生都喜欢它）。当数学过多而应用程序略少时，我会迷茫，但很多人会喜欢这类书的优雅。</p><h3 id="线性代数的基础和最优化-fundamentals-of-linear-algebra-and-optimizations">26. <a href="https://www.seas.upenn.edu/~cis515/linalg.pdf" target="_blank" rel="noopener">线性代数的基础和最优化 / Fundamentals of Linear Algebra and Optimizations</a></h3><p>本书将线性代数与优化算法结合在一起。同样，面向喜欢该样式的人的更多面向数学的书籍。</p><h3 id="线性代数讲义-lerner-linear-algebra-lecture-notes-lerner">27. <a href="https://www-labs.iro.umontreal.ca/~grabus/courses/ift6760_files/LANotes.lerner.pdf" target="_blank" rel="noopener">线性代数讲义– Lerner / Linear Algebra Lecture Notes – Lerner</a></h3><p>我发现它真的很好，就像向您展示了多个已解决的问题以使您学习一样。不像以前的书那样严格，而是通过表演来学习。对于长时间不接触线性代数的人来说，是不错的复习。</p><h3 id="随机线性代数的讲义-lecture-notes-on-randomized-linear-algebra">28. <a href="https://arxiv.org/abs/1608.04481" target="_blank" rel="noopener">随机线性代数的讲义 / Lecture Notes on Randomized Linear Algebra</a></h3><p>并非所有人都需要阅读本书，因为本书涉及解决线性代数问题的概率算法。如果您要处理大型矩阵和向量，而简单的算法将无法使用，则很有用。</p><h3 id="通过外部产品的线性代数-linear-algebra-via-exterior-products">29. <a href="https://www.academia.edu/32968283/Linear_Algebra_via_Exterior_Products" target="_blank" rel="noopener">通过外部产品的线性代数 / Linear Algebra via Exterior Products</a></h3><p>线性代数的另一种截然不同的方式。如果您发现线性代数很棒，则应尝试以这种新方式可视化问题。</p><h3 id="线性代数-linear-algebra-cherney-et-al">30. <a href="https://www.math.ucdavis.edu/~linear/" target="_blank" rel="noopener">线性代数 / Linear Algebra – Cherney et al</a></h3><p>另一本针对大学级线性代数的免费书籍。适合初学者。如果您想练习，它也会带来作业问题。</p><h3 id="深度学习所需的矩阵微积分-matrix-calculus-you-need-for-deep-learning">31. <a href="https://arxiv.org/abs/1802.01528" target="_blank" rel="noopener">深度学习所需的矩阵微积分 / Matrix Calculus you need for Deep Learning</a></h3><p>顾名思义，本教程可帮助您了解深度学习所需的矩阵演算。</p><h3 id="优化简介-optimization-an-introduction">32. <a href="http://www3.imperial.ac.uk/pls/portallive/docs/1/7288263.PDF" target="_blank" rel="noopener">优化：简介 / Optimization: An Introduction</a></h3><p>在整个工程领域的问题中都需要优化参数。虽然在许多深度学习算法中使用了凸优化，但了解线性算法等其他算法后，单纯形却开阔了眼界。</p><h3 id="scipy讲义scipy-lecture-notes">33. Scipy讲义/Scipy Lecture Notes</h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/Scipy.png" alt="" /><figcaption>Scipy</figcaption></figure><p>如果您打算从事数据科学工作，则需要学习科学的Python堆栈。可能是学习Numpy，Scipy，Scikit-Learn，Scikit-Image以及所需的所有库的最佳通用教程。</p><h3 id="熊猫超级教程-pandas-mega-tutorial">34. <a href="https://pandas.pydata.org/docs/pandas.pdf" target="_blank" rel="noopener">熊猫超级教程 / Pandas Mega Tutorial</a></h3><p>这个庞大的教程是由Pandas开发团队学习和理解该库的。如果您从事数据科学工作，Pandas是一个必须学习的图书馆。跑不了的。</p><h3 id="python中的卡尔曼和贝叶斯过滤器-kalman-and-bayesian-filters-in-python">35. <a href="https://drive.google.com/file/d/0By_SW19c1BfhSVFzNHc0SjduNzg/view" target="_blank" rel="noopener">Python中的卡尔曼和贝叶斯过滤器 / Kalman and Bayesian Filters in Python</a></h3><p>当处理随时间而来的噪声数据时，卡尔曼滤波器和其他贝叶斯滤波器非常有用，这些噪声数据可以拟合到具有推论参数的特定模型。这些模型的双重作用是推导参数以及对噪声进行建模。尽管最常用的示例是位置数据，但是类似的过滤器也可以很好地进行预测。（也可以在  <a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python" target="_blank" rel="noopener">Github上获得</a>）</p><h3 id="数据科学的统计推断-statistical-inference-for-data-science">36. <a href="https://leanpub.com/LittleInferenceBook" target="_blank" rel="noopener">数据科学的统计推断 / Statistical Inference for Data Science</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/hero.png" alt="" /><figcaption>hero</figcaption></figure><p>在此之前，我们已经看过多本《统计推断》书籍，但是写这本书的时候尤其要牢记数据科学家的思想。如果您是一名数据科学家，并且试图快速掌握统计推断，那么这就是您的书。</p><h3 id="机器学习数学-mathematics-for-machine-learning">37. <a href="https://mml-book.github.io/" target="_blank" rel="noopener">机器学习数学 / Mathematics for Machine Learning</a></h3><figure><img src="https://2020.iosdevlog.com/2020/03/09/50-books/mml-book-cover.jpg" alt="" /><figcaption>mml-book-cover</figcaption></figure><p>一本详细的教您数学的书需要理解其中的大多数机器学习算法。初学者的友好。</p><h3 id="看见统计---基础概率论-seeing-theory">38. <a href="https://seeing-theory.brown.edu/#firstPage" target="_blank" rel="noopener">看见统计 - 基础概率论 / Seeing Theory</a></h3><p>通过使用交互式可视化使学习概率容易的书。</p><h3 id="统计基础-basics-of-statistics">39. <a href="https://www.semanticscholar.org/paper/Basics-of-Statistics-Isotalo/c3cc90f6e11e9554f3de2c0da26e44ac22f8a1ff" target="_blank" rel="noopener">统计基础 / Basics of Statistics</a></h3><p>一本书向您介绍统计研究。从未学习过统计学的初学者应该从这里开始。</p><h3 id="公开统计-open-statistics">40. <a href="http://14.139.232.166/opstat/" target="_blank" rel="noopener">公开统计 / Open Statistics</a></h3><p>书和视频讲座相结合，向读者介绍统计学。</p><h3 id="从基本角度看高级数据分析-advanced-data-analysis-from-an-elementary-point-of-view">41. <a href="https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/" target="_blank" rel="noopener">从基本角度看高级数据分析 / Advanced Data analysis From an Elementary Point of View</a></h3><p>数据科学的不同概念的一般介绍。这包括因果模型，回归模型，因子模型等。示例程序在R中。</p><h3 id="快速智能大规模-fast-data-smart-and-at-scale">42. <a href="https://www.voltdb.com/wp-content/uploads/2017/03/hv-ebook-fast-data-smart-and-at-scale.pdf" target="_blank" rel="noopener">快速，智能，大规模 / Fast Data, Smart and at Scale</a></h3><p>本书介绍了如何优化数据库以实现快速查询。它讲述了现实世界中的各种可能模型。</p><h3 id="多武装匪徒简介-introduction-to-multi-armed-bandits">43. <a href="https://arxiv.org/abs/1904.07272" target="_blank" rel="noopener">多武装匪徒简介 / Introduction to Multi-Armed Bandits</a></h3><p>多臂强盗是在不确定性下会随着时间做出决策的算法。这本书是多臂匪的入门论文。</p><h3 id="量化经济学讲座-quant-economics-lectures">44. <a href="https://quantecon.org/lectures/" target="_blank" rel="noopener">量化经济学讲座 / Quant Economics Lectures</a></h3><p>使用您喜欢的编程语言进行的定量经济学和代码讲座：Python或Julia。</p><h3 id="julia-统计-statistics-with-julia">45. <a href="https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf" target="_blank" rel="noopener">Julia 统计 / Statistics With Julia</a></h3><p>统计学家学习Julia还是（不太可能）Julia程序员学习统计学？试试这本书。</p><h3 id="信息论推理与学习算法-information-theory-inference-and-learning-algorithms">46. <a href="https://www.inference.org.uk/itprnn/book.pdf" target="_blank" rel="noopener">信息论，推理与学习算法 / Information Theory, Inference and Learning Algorithms</a></h3><p>信息理论和推理通常以不同的方式处理，但已故的MacKay教授的书试图解决这两个问题。</p><h3 id="科学改进决策和风险管理-scientific-improvement-of-decision-making-and-risk-management">47. <a href="https://yngve.hoiseth.net/Empiricast_White_Paper.pdf" target="_blank" rel="noopener">科学改进决策和风险管理 / Scientific Improvement of Decision Making and Risk Management</a></h3><p>关于概率决策的技术性不太强的教程。</p><h3 id="三十三个缩图线性代数的数学和算法应用-thirty-three-miniatures-mathematical-and-algorithmic-applications-of-linear-algebra">48. <a href="https://kam.mff.cuni.cz/~matousek/stml-53-matousek-1.pdf" target="_blank" rel="noopener">三十三个缩图：线性代数的数学和算法应用 / Thirty-three Miniatures: Mathematical and Algorithmic Applications of Linear Algebra</a></h3><p>这实际上不是一本关于线性代数的书，而是一本汇编成书的线性书的一些很酷的应用程序。</p><h3 id="遗传算法教程-a-genetic-algorithm-tutorial">49. <a href="https://www.cs.colostate.edu/~genitor/MiscPubs/tutorial.pdf" target="_blank" rel="noopener">遗传算法教程 / A Genetic Algorithm Tutorial</a></h3><p>遗传算法是所有数据科学家一生中都需要使用的工具。本教程可帮助初学者了解遗传算法的工作原理。</p><h3 id="使用-julia-在运营研究中计算-computing-in-operations-research-using-julia">50. <a href="https://arxiv.org/abs/1312.1431" target="_blank" rel="noopener">使用 Julia 在运营研究中计算 / Computing in Operations Research using Julia</a></h3><p>如果您正在处理排队或其他运营研究问题，Julia可能是您很喜欢的一种编程语言。这些程序像Python一样易于读取，运行速度非常快。</p><p>原文：<a href="https://www.kdnuggets.com/2020/03/50-must-read-free-books-every-data-scientist-2020.html" target="_blank" rel="noopener">50 Must-Read Free Books For Every Data Scientist in 2020</a></p><p><strong>By <a href="https://blog.paralleldots.com/author/reashikaa%20/" target="_blank" rel="noopener">Reashikaa Verma</a>, <a href="https://www.paralleldots.com/" target="_blank" rel="noopener">ParellelDots</a></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据科学是一个跨学科领域，包含来自统计，机器学习，贝叶斯等领域的方法和技术。它们都旨在从数据中产生特定的见解。在本文中，我们列出了一些出色的数据科学书籍，其中涵盖了数据科学下的各种主题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="DS" scheme="https://2020.iosdevlog.com/tags/DS/"/>
    
      <category term="book" scheme="https://2020.iosdevlog.com/tags/book/"/>
    
  </entry>
  
  <entry>
    <title>节选：常见机器学习算法列表（Python 调用）</title>
    <link href="https://2020.iosdevlog.com/2020/03/08/algorithms/"/>
    <id>https://2020.iosdevlog.com/2020/03/08/algorithms/</id>
    <published>2020-03-08T14:49:41.000Z</published>
    <updated>2020-03-08T15:48:54.648Z</updated>
    
    <content type="html"><![CDATA[<ol type="1"><li>线性回归</li><li>逻辑回归</li><li>决策树</li><li>支持向量机</li><li>朴素贝叶斯</li><li>神经网络</li><li>K均值</li><li>随机森林</li><li>降维算法</li><li>梯度提升算法<ol type="1"><li>GBM</li><li>XGBoost</li><li>LightGBM</li><li>Catboost</li></ol></li></ol><a id="more"></a><p>大致而言，共有 3 种类型的机器学习算法</p><ul><li>监督学习</li></ul><p>工作原理：此算法由目标/结果变量（或因变量）组成，该目标/结果变量将从给定的一组预测变量（因变量）中进行预测。使用这些变量集，我们生成了一个将输入映射到所需输出的函数。训练过程将继续进行，直到模型在训练数据上达到所需的准确性水平为止。</p><p>监督学习的例子：线性回归，决策树，随机森林，KNN，逻辑回归等。</p><ul><li>无监督学习</li></ul><p>工作原理： 在此算法中，我们没有任何目标或结果变量可以预测/估算。它用于对不同组中的人群进行聚类，广泛用于对不同组中的客户进行细分以进行特定干预。</p><p>无监督学习的示例：Apriori算法，K均值。</p><ul><li>强化学习：</li></ul><p>工作原理：使用此算法，机器经过训练后可以做出特定决策。它是这样工作的：机器处于反复试验不断训练自身的环境中。该机器将从过去的经验中学习，并尝试捕获最佳的知识以做出准确的业务决策。</p><p>强化学习的例子：马尔可夫决策过程</p><h2 id="常见机器学习算法列表">常见机器学习算法列表</h2><h3 id="线性回归">1.线性回归</h3><p>它用于根据连续变量估算实际价值（房屋成本，通话次数，总销售额等）。在这里，我们通过拟合一条最佳线来建立自变量和因变量之间的关系。该最佳拟合线称为回归线，并由线性方程 <span class="math inline">\(Y = a * X + b\)</span> 表示。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Linear_Regression.webp" alt="" /><figcaption>Linear_Regression</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for the Linear Regression</span></span><br><span class="line"><span class="string">Created by- ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line"></span><br><span class="line">print(train_data.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'\nShape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'\nShape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Item_Outlet_Sales</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Item_Outlet_Sales'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Item_Outlet_Sales'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Item_Outlet_Sales'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Item_Outlet_Sales'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the Linear Regression model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : fit_intercept and normalize</span></span><br><span class="line"><span class="string">Documentation of sklearn LinearRegression: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># coefficeints of the trained model</span></span><br><span class="line">print(<span class="string">'\nCoefficient of model :'</span>, model.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># intercept of the model</span></span><br><span class="line">print(<span class="string">'\nIntercept of model'</span>,model.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'\nItem_Outlet_Sales on training data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Root Mean Squared Error on training dataset</span></span><br><span class="line">rmse_train = mean_squared_error(train_y,predict_train)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on train dataset : '</span>, rmse_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the testing dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'\nItem_Outlet_Sales on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Root Mean Squared Error on testing dataset</span></span><br><span class="line">rmse_test = mean_squared_error(test_y,predict_test)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on test dataset : '</span>, rmse_test)</span><br></pre></td></tr></table></figure><h3 id="logistic回归">2. Logistic回归</h3><p>不要被它的名字弄糊涂了！它是一种分类，而不是回归算法。它用于基于给定的一组独立变量来估计离散值（二进制值，如 <code>0/1，yes/no，true/false</code>）。简而言之，它通过将数据拟合到logit函数来预测事件发生的可能性。因此，这也称为 <strong>对数回归</strong>。由于可以预测概率，因此其输出值介于0和1之间（如预期）。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Logistic_Regression.webp" alt="" /><figcaption>Logistic_Regression</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Logistic Regression</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(train_data.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the Logistic Regression model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : fit_intercept and penalty</span></span><br><span class="line"><span class="string">Documentation of sklearn LogisticRegression: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># coefficeints of the trained model</span></span><br><span class="line">print(<span class="string">'Coefficient of model :'</span>, model.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># intercept of the model</span></span><br><span class="line">print(<span class="string">'Intercept of model'</span>,model.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'Target on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'accuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'Target on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'accuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="决策树">3. 决策树</h3><p>这是我最喜欢的算法之一，我经常使用它。它是一种监督学习算法，主要用于分类问题。令人惊讶的是，它适用于分类因变量和连续因变量。在此算法中，我们将总体分为两个或多个同构集合。这是基于最重要的属性/自变量来完成的，以尽可能地形成不同的组。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Decision_Tree.webp" alt="" /><figcaption>Decision Tree</figcaption></figure><p>理解决策树如何工作的最好方法是玩 Jezzball，这是 Microsoft 的经典游戏（下图）。本质上，您有一间活动墙的房间，您需要创建墙以使最大的区域在没有球的情况下被清除。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Jezzball.jpg" alt="" /><figcaption>Jezzball</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Decision Tree</span></span><br><span class="line"><span class="string">Created by - Analytics Vidhya</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the Decision Tree model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : max_depth and max_features</span></span><br><span class="line"><span class="string">Documentation of sklearn DecisionTreeClassifier: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = DecisionTreeClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># depth of the decision tree</span></span><br><span class="line">print(<span class="string">'Depth of the Decision Tree :'</span>, model.get_depth())</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'Target on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'accuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'Target on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'accuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="svm支持向量机">4. SVM（支持向量机）</h3><p>这是一种分类方法。在此算法中，我们将每个数据项绘制为n维空间（其中n是您拥有的特征数）中的一个点，其中每个特征的值就是特定坐标的值。</p><p>例如，如果我们只有两个特征，例如一个人的身高和头发长度，我们首先将这两个变量绘制在二维空间中，其中每个点都有两个坐标（这些坐标称为支持向量）</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/SVM1.webp" alt="" /><figcaption>SVM1</figcaption></figure><p>现在，我们将找到一行将数据划分为两个不同分类的数据组。这条线将使距两组中最近点的距离最远。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/SVM2.webp" alt="" /><figcaption>SVM2</figcaption></figure><p>在上面显示的示例中，将数据分为两个不同类别的组的线是黑线，因为两个最接近的点距离该线最远。这行是我们的分类器。然后，根据测试数据在行两边的位置，可以将新数据归类为该类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Support Vector Machines</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the Support Vector Classifier model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : kernal and degree</span></span><br><span class="line"><span class="string">Documentation of sklearn Support Vector Classifier: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = SVC()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'Target on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'accuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'Target on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'accuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="朴素贝叶斯">5. 朴素贝叶斯</h3><p>这是一种基于贝叶斯定理的分类技术，假设预测变量之间具有独立性。简单来说，朴素贝叶斯分类器假定类中某个特定功能的存在与任何其他功能的存在无关。例如，如果水果是红色，圆形且直径约3英寸，则可以将其视为苹果。即使这些特征相互依赖或依赖于其他特征的存在，朴素的贝叶斯分类器也会考虑所有这些特征，以独立地有助于该果实是苹果的可能性。</p><p>朴素贝叶斯模型易于构建，对于非常大的数据集特别有用。除简单之外，朴素的贝叶斯（Naive Bayes）甚至胜过非常复杂的分类方法。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Bayes_rule.webp" alt="" /><figcaption>Bayes_rule</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Naive Bayes</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the Naive Bayes model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : var_smoothing</span></span><br><span class="line"><span class="string">Documentation of sklearn GaussianNB: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = GaussianNB()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'Target on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'accuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'Target on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'accuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="knnk-最近邻">6. kNN（k-最近邻）</h3><p>它可以用于分类和回归问题。但是，它更广泛地用于行业中的分类问题。K个最近邻居是一种简单的算法，可以存储所有可用案例，并通过其k个邻居的多数票对新案例进行分类。在通过距离函数测得的K个最近邻居中，分配给该类别的案例最为常见。</p><p>这些距离函数可以是欧几里得距离，曼哈顿距离，明可夫斯基距离和汉明距离。前三个函数用于连续函数，第四个函数（汉明）用于分类变量。如果K = 1，则将案例简单分配给其最近邻居的类别。有时，执行kNN建模时选择K确实是一个挑战。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/KNN.webp" alt="" /><figcaption>KNN</figcaption></figure><p>KNN可以轻松地映射到我们的现实生活。如果您想了解一个没有信息的人，则可能想了解他的密友和他所进入的圈子并获得他/她的信息！</p><p>选择kNN之前要考虑的事项：</p><ol type="1"><li>KNN在计算上很昂贵</li><li>变量应归一化，否则范围较大的变量会对其产生偏差</li><li>在进行kNN处理之前（如离群值，噪声消除）在预处理阶段进行更多工作</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for the K-Nearest Neighbors</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the K-Nearest Neighbor model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : n_neighbors, leaf_size</span></span><br><span class="line"><span class="string">Documentation of sklearn K-Neighbors Classifier: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line">model = KNeighborsClassifier()  </span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of Neighbors used to predict the target</span></span><br><span class="line">print(<span class="string">'\nThe number of neighbors used to predict the target : '</span>,model.n_neighbors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'\nTarget on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'accuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'Target on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'accuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="k-均值聚类">7. K-均值聚类</h3><p>这是一种无监督算法，可以解决聚类问题。它的过程遵循一种简单的方法，可以通过一定数量的聚类（假设k个聚类）对给定的数据集进行分类。集群中的数据点对同级组是同质的，并且是异构的。</p><p>还记得从墨水印迹中找出形状吗？k表示此活动有点类似。您查看形状并展开以解释存在多少个不同的群集/种群！</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Ink.jpg" alt="" /><figcaption>Ink</figcaption></figure><p>K-均值如何形成聚类：</p><ol type="1"><li>K均值为每个聚类选择k个点，称为质心。</li><li>每个数据点形成一个具有最接近质心的聚类，即k个聚类。</li><li>根据现有集群成员查找每个集群的质心。在这里，我们有了新的质心。</li><li>当我们有了新的质心时，请重复步骤2和3。找到每个数据点与新质心的最近距离，并与新的k簇相关联。重复此过程，直到会聚即质心不变为止。</li></ol><p>如何确定K的值：</p><p>在K均值中，我们有簇，每个簇都有自己的质心。质心和群集中数据点之间的差平方和构成该群集的平方值之和。同样，当所有聚类的平方和相加时，它成为聚类解的平方和之和。</p><p>我们知道，随着簇数的增加，该值会不断减少，但是如果绘制结果，您可能会看到平方距离的总和急剧减小，直至达到某个k值，然后逐渐减小。在这里，我们可以找到最佳的群集数量。</p><figure><img src="https://2020.iosdevlog.com/2020/03/08/algorithms/Kmenas.webp" alt="" /><figcaption>Kmenas</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for the K-Means</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to divide the training data into differernt clusters</span></span><br><span class="line"><span class="comment"># and predict in which cluster a particular data point belongs.  </span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the K-Means model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : n_clusters and max_iter</span></span><br><span class="line"><span class="string">Documentation of sklearn KMeans: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html</span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line"></span><br><span class="line">model = KMeans()  </span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of Clusters</span></span><br><span class="line">print(<span class="string">'\nDefault number of Clusters : '</span>,model.n_clusters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the clusters on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_data)</span><br><span class="line">print(<span class="string">'\nCLusters on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_data)</span><br><span class="line">print(<span class="string">'Clusters on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we will train a model with n_cluster = 3</span></span><br><span class="line">model_n3 = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model_n3.fit(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of Clusters</span></span><br><span class="line">print(<span class="string">'\nNumber of Clusters : '</span>,model_n3.n_clusters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the clusters on the train dataset</span></span><br><span class="line">predict_train_3 = model_n3.predict(train_data)</span><br><span class="line">print(<span class="string">'\nCLusters on train data'</span>,predict_train_3) </span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test_3 = model_n3.predict(test_data)</span><br><span class="line">print(<span class="string">'Clusters on test data'</span>,predict_test_3)</span><br></pre></td></tr></table></figure><h3 id="随机森林">8. 随机森林</h3><p>随机森林是决策树集合的商标术语。在随机森林中，我们收集了决策树（也称为“森林”）。为了基于属性对新对象进行分类，每棵树都进行了分类，我们称该树为该类“投票”。森林选择投票最多的类别（在森林中的所有树木上）。</p><p>每棵树的种植和生长如下：</p><ol type="1"><li>如果训练集中的病例数为N，则随机抽取N个病例作为样本，并进行替换。该样本将成为树木生长的训练集。</li><li>如果有M个输入变量，则指定数字m &lt;&lt; M，以便在每个节点上从M个中随机选择m个变量，并使用对这m个变量的最佳分割来分割节点。在森林生长过程中，m的值保持恒定。</li><li>每棵树都尽可能地生长。没有修剪。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for the Random Forest</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># view the top 3 rows of the dataset</span></span><br><span class="line">print(train_data.head(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'\nShape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'\nShape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Create the object of the Random Forest model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : n_estimators and max_depth</span></span><br><span class="line"><span class="string">Documentation of sklearn RandomForestClassifier: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">model = RandomForestClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># number of trees used</span></span><br><span class="line">print(<span class="string">'Number of Trees used : '</span>, model.n_estimators)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'\nTarget on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'\naccuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'\nTarget on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'\naccuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h3 id="降维算法">9.降维算法</h3><p>在过去的4-5年中，每个可能阶段的数据捕获都呈指数级增长。公司/政府机构/研究组织不仅提供了新的来源，而且还非常详细地捕获数据。</p><p>例如：电子商务公司正在捕获有关客户的更多详细信息，例如他们的人口统计信息，网络爬网历史记录，他们喜欢或不喜欢的东西，购买历史记录，反馈以及许多其他信息，这些东西比最近的杂货店店主更能给予他们个性化的关注。</p><p>作为数据科学家，我们提供的数据还包含许多功能，这对于构建良好的鲁棒模型听起来不错，但仍存在挑战。您如何识别1000或2000中的高有效变量？在这种情况下，降维算法可与其他各种算法（例如决策树，随机森林，PCA，因子分析，基于相关矩阵识别，缺失值比率等）一起帮助我们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Principal Component Analysis (PCA)</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error  </span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># view the top 3 rows of the dataset</span></span><br><span class="line">print(train_data.head(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'\nShape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'\nShape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line"><span class="comment"># target variable - Item_Outlet_Sales</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Item_Outlet_Sales'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Item_Outlet_Sales'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Item_Outlet_Sales'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Item_Outlet_Sales'</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\nTraining model with &#123;&#125; dimensions.'</span>.format(train_x.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># create object of model</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">rmse_train = mean_squared_error(train_y,predict_train)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on train dataset : '</span>, rmse_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">rmse_test = mean_squared_error(test_y,predict_test)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on test dataset : '</span>, rmse_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create the object of the PCA (Principal Component Analysis) model</span></span><br><span class="line"><span class="comment"># reduce the dimensions of the data to 12</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : svd_solver, iterated_power</span></span><br><span class="line"><span class="string">Documentation of sklearn PCA:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">model_pca = PCA(n_components=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">new_train = model_pca.fit_transform(train_x)</span><br><span class="line">new_test  = model_pca.fit_transform(test_x)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\nTraining model with &#123;&#125; dimensions.'</span>.format(new_train.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># create object of model</span></span><br><span class="line">model_new = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model_new.fit(new_train,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the new train dataset</span></span><br><span class="line">predict_train_pca = model_new.predict(new_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">rmse_train_pca = mean_squared_error(train_y,predict_train_pca)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on new train dataset : '</span>, rmse_train_pca)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the new test dataset</span></span><br><span class="line">predict_test_pca = model_new.predict(new_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">rmse_test_pca = mean_squared_error(test_y,predict_test_pca)**(<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">'\nRMSE on new test dataset : '</span>, rmse_test_pca)</span><br></pre></td></tr></table></figure><h3 id="梯度提升算法">10. 梯度提升算法</h3><h4 id="gbm">10.1 GBM</h4><p>当我们处理大量数据以进行具有高预测能力的预测时，GBM是一种增强算法。Boosting实际上是一种学习算法的集合，该算法结合了多个基本估计量的预测，以提高单个估计量的鲁棒性。它将多个弱或平均预测变量组合为一个构建强的预测变量。这些增强算法在Kaggle，AV Hackathon，CrowdAnalytix等数据科学竞赛中始终能很好地发挥作用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for Gradient Boosting</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the GradientBoosting Classifier model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : learning_rate, n_estimators</span></span><br><span class="line"><span class="string">Documentation of sklearn GradientBoosting Classifier: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">model = GradientBoostingClassifier(n_estimators=<span class="number">100</span>,max_depth=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'\nTarget on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'\naccuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'\nTarget on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'\naccuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h4 id="xgboost">10.2 XGBoost</h4><p>在某些Kaggle比赛中，另一种经典的梯度提升算法是决定胜负的决定性选择。</p><p>XGBoost具有极高的预测能力，这使其成为事件准确性的最佳选择，因为它同时具有线性模型和树学习算法，这使得该算法比现有的梯度增强技术快了近10倍。</p><p>支持包括各种目标功能，包括回归，分类和排名。</p><p>关于XGBoost的最有趣的事情之一是，它也被称为正则化增强技术。这有助于减少过拟合模型，并且对Scala，Java，R，Python，Julia和C ++等多种语言提供了广泛的支持。</p><p>支持在包含GCE，AWS，Azure和Yarn群集的许多计算机上进行分布式和广泛的培训。XGBoost还可以与Spark，Flink和其他云数据流系统集成，在提升过程的每次迭代中都具有内置的交叉验证。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">The following code is for XGBoost</span></span><br><span class="line"><span class="string">Created by - ANALYTICS VIDHYA</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># importing required libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the train and test dataset</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train-data.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test-data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape of the dataset</span></span><br><span class="line">print(<span class="string">'Shape of training data :'</span>,train_data.shape)</span><br><span class="line">print(<span class="string">'Shape of testing data :'</span>,test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we need to predict the missing target variable in the test data</span></span><br><span class="line"><span class="comment"># target variable - Survived</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on training data</span></span><br><span class="line">train_x = train_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">train_y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the independent and target variable on testing data</span></span><br><span class="line">test_x = test_data.drop(columns=[<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">test_y = test_data[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Create the object of the XGBoost model</span></span><br><span class="line"><span class="string">You can also add other parameters and test your code here</span></span><br><span class="line"><span class="string">Some parameters are : max_depth and n_estimators</span></span><br><span class="line"><span class="string">Documentation of xgboost:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://xgboost.readthedocs.io/en/latest/</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">model = XGBClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model with the training data</span></span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the train dataset</span></span><br><span class="line">predict_train = model.predict(train_x)</span><br><span class="line">print(<span class="string">'\nTarget on train data'</span>,predict_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuray Score on train dataset</span></span><br><span class="line">accuracy_train = accuracy_score(train_y,predict_train)</span><br><span class="line">print(<span class="string">'\naccuracy_score on train dataset : '</span>, accuracy_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the target on the test dataset</span></span><br><span class="line">predict_test = model.predict(test_x)</span><br><span class="line">print(<span class="string">'\nTarget on test data'</span>,predict_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy Score on test dataset</span></span><br><span class="line">accuracy_test = accuracy_score(test_y,predict_test)</span><br><span class="line">print(<span class="string">'\naccuracy_score on test dataset : '</span>, accuracy_test)</span><br></pre></td></tr></table></figure><h4 id="lightgbm">10.3 LightGBM</h4><p>LightGBM 是使用基于树的学习算法的梯度增强框架。它被设计为分布式且高效的，具有以下优点：</p><ul><li>更快的训练速度和更高的效率</li><li>降低内存使用量</li><li>精度更高</li><li>支持并行和GPU学习</li><li>能够处理大规模数据</li></ul><p>该框架是基于决策树算法的一种快速，高性能的梯度提升算法，用于排名，分类和许多其他机器学习任务。它是在Microsoft的分布式机器学习工具包项目下开发的。</p><p>由于LightGBM基于决策树算法，因此它以最佳拟合的方式对树进行拆分，而其他增强算法则对树的深度或层次进行拆分，而不是对叶进行拆分。因此，当在Light GBM中的同一叶上生长时，与逐级算法相比，逐叶算法可以减少更多的损失，因此可以得到更好的精度，而现有的任何增强算法都很少达到这种精度。</p><p>而且，它出奇地快，因此是“ Light”一词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">data = np.random.rand(<span class="number">500</span>, <span class="number">10</span>) <span class="comment"># 500 entities, each contains 10 features</span></span><br><span class="line">label = np.random.randint(<span class="number">2</span>, size=<span class="number">500</span>) <span class="comment"># binary target</span></span><br><span class="line"></span><br><span class="line">train_data = lgb.Dataset(data, label=label)</span><br><span class="line">test_data = train_data.create_valid(<span class="string">'test.svm'</span>)</span><br><span class="line"></span><br><span class="line">param = &#123;<span class="string">'num_leaves'</span>:<span class="number">31</span>, <span class="string">'num_trees'</span>:<span class="number">100</span>, <span class="string">'objective'</span>:<span class="string">'binary'</span>&#125;</span><br><span class="line">param[<span class="string">'metric'</span>] = <span class="string">'auc'</span></span><br><span class="line"></span><br><span class="line">num_round = <span class="number">10</span></span><br><span class="line">bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])</span><br><span class="line"></span><br><span class="line">bst.save_model(<span class="string">'model.txt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7 entities, each contains 10 features</span></span><br><span class="line">data = np.random.rand(<span class="number">7</span>, <span class="number">10</span>)</span><br><span class="line">ypred = bst.predict(data)</span><br></pre></td></tr></table></figure><h4 id="catboost">10.4 Catboost</h4><p>CatBoost是Yandex最近提供的开源机器学习算法。它可以轻松地与深度学习框架（如Google的TensorFlow和Apple的Core ML）集成。</p><p>关于CatBoost的最好之处在于，它不需要像其他ML模型一样进行大量的数据培训，并且可以处理多种数据格式。不会破坏它的坚固性。</p><p>在继续实施之前，请确保处理好丢失的数据。</p><p>Catboost可以自动处理分类变量，而不会显示类型转换错误，这可以帮助您专注于更好地调整模型，而不是解决琐碎的错误。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> catboost <span class="keyword">import</span> CatBoostRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment">#Read training and testing files</span></span><br><span class="line">train = pd.read_csv(<span class="string">"train.csv"</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">"test.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Imputing missing values for both train and test</span></span><br><span class="line">train.fillna(<span class="number">-999</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test.fillna(<span class="number">-999</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Creating a training set for modeling and validation set to check model performance</span></span><br><span class="line">X = train.drop([<span class="string">'Item_Outlet_Sales'</span>], axis=<span class="number">1</span>)</span><br><span class="line">y = train.Item_Outlet_Sales</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=<span class="number">0.7</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">categorical_features_indices = np.where(X.dtypes != np.float)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#importing library and building model</span></span><br><span class="line"><span class="keyword">from</span> catboost <span class="keyword">import</span> CatBoostRegressormodel=CatBoostRegressor(iterations=<span class="number">50</span>, depth=<span class="number">3</span>, learning_rate=<span class="number">0.1</span>, loss_function=<span class="string">'RMSE'</span>)</span><br><span class="line"></span><br><span class="line">model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">submission = pd.DataFrame()</span><br><span class="line"></span><br><span class="line">submission[<span class="string">'Item_Identifier'</span>] = test[<span class="string">'Item_Identifier'</span>]</span><br><span class="line">submission[<span class="string">'Outlet_Identifier'</span>] = test[<span class="string">'Outlet_Identifier'</span>]</span><br><span class="line">submission[<span class="string">'Item_Outlet_Sales'</span>] = model.predict(test)</span><br></pre></td></tr></table></figure><p>节选自：<a href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms" target="_blank" rel="noopener">Commonly used Machine Learning Algorithms (with Python and R Codes)</a><br />作者：<a href="https://www.analyticsvidhya.com/blog/author/sunil-ray/" target="_blank" rel="noopener">SUNIL RAY</a></p>]]></content>
    
    <summary type="html">
    
      &lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;线性回归&lt;/li&gt;
&lt;li&gt;逻辑回归&lt;/li&gt;
&lt;li&gt;决策树&lt;/li&gt;
&lt;li&gt;支持向量机&lt;/li&gt;
&lt;li&gt;朴素贝叶斯&lt;/li&gt;
&lt;li&gt;神经网络&lt;/li&gt;
&lt;li&gt;K均值&lt;/li&gt;
&lt;li&gt;随机森林&lt;/li&gt;
&lt;li&gt;降维算法&lt;/li&gt;
&lt;li&gt;梯度提升算法
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;GBM&lt;/li&gt;
&lt;li&gt;XGBoost&lt;/li&gt;
&lt;li&gt;LightGBM&lt;/li&gt;
&lt;li&gt;Catboost&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="algorithm" scheme="https://2020.iosdevlog.com/categories/algorithm/"/>
    
    
      <category term="ml" scheme="https://2020.iosdevlog.com/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>《神经网络与深度学习》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/03/07/nn/"/>
    <id>https://2020.iosdevlog.com/2020/03/07/nn/</id>
    <published>2020-03-07T11:53:40.000Z</published>
    <updated>2020-03-08T15:02:17.574Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/07/nn/1.png" alt="" /><figcaption>神经网络与深度学习</figcaption></figure><a id="more"></a><h2 id="使用神经网络识别手写数字">使用神经网络识别手写数字</h2><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/001014.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000774.jpg" /></p><h3 id="感知机">感知机</h3><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/001124.jpg" /></p><ul><li>输入</li><li>输出</li><li>权重（weight）</li><li>阈值（threshold value）</li></ul><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000011.jpg" /></p><ul><li><p>偏置（bias）</p></li><li><p>与非门 (NAND gate)</p></li></ul><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000155.jpg" /></p><p>感知机网络计算任何逻辑函数</p><figure><img src="https://2020.iosdevlog.com/2020/03/07/nn/000495.jpg" alt="" /><figcaption>NAND</figcaption></figure><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000706.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000303.jpg" /></p><p>一个没有输入的感知机，那么加权和恒为 0。</p><p>所以，我们最好不要将输入感知机当做感知机，而是理解为一个特殊的单元，它能够输出我们想要的值。</p><figure><img src="https://2020.iosdevlog.com/2020/03/07/nn/000617.jpg" alt="" /><figcaption>输入感知机</figcaption></figure><h3 id="sigmoid神经元">sigmoid神经元</h3><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000110.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000277.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000127.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000625.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000586.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000463.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/001018.jpg" /></p><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000626.jpg" /></p><h3 id="神经网络的结构">神经网络的结构</h3><h3 id="用简单的网络结构解决手写数字识别">用简单的网络结构解决手写数字识别</h3><h3 id="通过梯度下降法学习参数">通过梯度下降法学习参数</h3><h3 id="实现我们的神经网络来分类数字">实现我们的神经网络来分类数字</h3><h3 id="迈向深度学习">迈向深度学习</h3><h2 id="反向传播算法是如何工作的">反向传播算法是如何工作的</h2><h3 id="热身一个基于矩阵的快速计算神经网络输出的方法">热身：一个基于矩阵的快速计算神经网络输出的方法</h3><h3 id="关于代价函数的两个假设">关于代价函数的两个假设</h3><h3 id="hadamard积st">Hadamard积，s⨀t</h3><h3 id="反向传播背后的四个基本等式">反向传播背后的四个基本等式</h3><h3 id="四个基本方程的证明自选">四个基本方程的证明（自选）</h3><h3 id="反向传播算法">反向传播算法</h3><h3 id="反向传播算法代码">反向传播算法代码</h3><h3 id="为什么说反向传播算法很高效">为什么说反向传播算法很高效</h3><h3 id="反向传播整体描述">反向传播：整体描述</h3><h2 id="改进神经网络的学习方法">改进神经网络的学习方法</h2><h3 id="交叉熵代价函数">交叉熵代价函数</h3><h3 id="用交叉熵解决手写数字识别问题">用交叉熵解决手写数字识别问题</h3><h3 id="交叉熵的意义是什么它又是怎么来的">交叉熵的意义是什么？它又是怎么来的？</h3><h3 id="softmax">Softmax</h3><h3 id="过拟合和正则化">过拟合和正则化</h3><ul><li><p>正则化</p></li><li><p>为什么正则化能够降低过拟合</p></li></ul><h3 id="其它正则化技术">其它正则化技术</h3><h3 id="参数初始化">参数初始化</h3><h3 id="重温手写数字识别代码">重温手写数字识别：代码</h3><h3 id="如何选择神经网络的超参数">如何选择神经网络的超参数</h3><h3 id="其它技术">其它技术</h3><h2 id="神经网络可以计算任何函数的可视化证明">神经网络可以计算任何函数的可视化证明</h2><h3 id="两个预先声明">两个预先声明</h3><h3 id="一个输入和一个输出的普遍性">一个输入和一个输出的普遍性</h3><h3 id="多个输入变量">多个输入变量</h3><h3 id="s型神经元的延伸">S型神经元的延伸</h3><h3 id="修补阶跃函数">修补阶跃函数</h3><h3 id="结论">结论</h3><h2 id="为什么深度神经网络的训练是困难的">为什么深度神经网络的训练是困难的</h2><h3 id="梯度消失问题">梯度消失问题</h3><h3 id="什么导致了消失的梯度问题深度神经网络中的梯度不稳定性">什么导致了消失的梯度问题？深度神经网络中的梯度不稳定性</h3><h3 id="在更加复杂网络中的不稳定梯度">在更加复杂网络中的不稳定梯度</h3><h3 id="其他深度学习的障碍">其他深度学习的障碍</h3><h2 id="深度学习">深度学习</h2><h3 id="介绍卷积网络">介绍卷积网络</h3><h3 id="卷积神经网络在实际中的应用">卷积神经网络在实际中的应用</h3><h3 id="卷积网络的代码">卷积网络的代码</h3><h3 id="图像识别领域中的近期进展">图像识别领域中的近期进展</h3><h3 id="其他的深度学习模型">其他的深度学习模型</h3><h3 id="神经网络的未来">神经网络的未来</h3><p><img src="https://2020.iosdevlog.com/2020/03/07/nn/000058.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000073.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000077.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000085.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000217.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000264.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000275.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000280.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000294.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000296.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000476.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000508.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000543.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000546.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000570.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000610.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000657.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000700.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000729.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000742.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000771.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000805.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000817.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000883.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000894.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000906.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000911.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/000916.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001008.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001035.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001047.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001059.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001070.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001089.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001110.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001119.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001120.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/001145.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/03/07/nn/.jpg" /></p><p>源码：<a href="https://github.com/mnielsen/neural-networks-and-deep-learning" target="_blank" rel="noopener" class="uri">https://github.com/mnielsen/neural-networks-and-deep-learning</a></p><p>英文：<a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="noopener" class="uri">http://neuralnetworksanddeeplearning.com/</a></p><p>中文：<a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/" target="_blank" rel="noopener" class="uri">https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/07/nn/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;神经网络与深度学习&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="nn" scheme="https://2020.iosdevlog.com/tags/nn/"/>
    
      <category term="dl" scheme="https://2020.iosdevlog.com/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>深度学习简介</title>
    <link href="https://2020.iosdevlog.com/2020/03/06/dl/"/>
    <id>https://2020.iosdevlog.com/2020/03/06/dl/</id>
    <published>2020-03-06T14:23:02.000Z</published>
    <updated>2020-03-06T14:27:19.310Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/06/dl/1.png" alt="" /><figcaption>深度学习简介</figcaption></figure><a id="more"></a><h2 id="统计学基础随机性是如何改变数据拟合的本质的">统计学基础：随机性是如何改变数据拟合的本质的？</h2><h3 id="出发点">出发点</h3><ul><li><p>数学定理表明，任何一个函数都可以用多项式无限接近的拟合。</p></li><li><p>为什么我们不用多项式呢？</p></li></ul><h3 id="随机性是如何改变数据拟合本质的">随机性是如何改变数据拟合本质的</h3><ul><li><p>数据的拟合有两种随机性：</p><ul><li><p>噪声-&gt;无法消除</p></li><li><p>函数拟合的随机性-&gt;可以提升</p></li></ul></li><li><p>过拟合和欠拟合</p></li><li><p>引入其他信息的必要性</p></li><li><p>多角度考虑问题</p></li></ul><h3 id="随机性对算法工程师意味着什么">随机性对算法工程师意味着什么</h3><ul><li><p>过拟合和欠拟合是对神经网络设计和训练很重要的一点，但不是全部</p></li><li><p>能否解决问题在很大程度上取决于数据是否有足够信息</p><ul><li><p>引入结构化数据的必要性</p></li><li><p>为什么人解决不了的问题机器也解决不了</p></li></ul></li><li><p>算法除了考虑数学之外，还需要考虑实际数据的情况</p></li><li><p>训练集和测试集不同是机器学习算法最大的坑</p></li></ul><h2 id="神经网络基础神经网络还是复合函数">神经网络基础：神经网络还是复合函数</h2><h3 id="关于神经网络错误的说法">关于神经网络错误的说法</h3><ul><li>神经网络是大家根据神经科学得到的最伟大的发明</li></ul><h3 id="神经网络的数学本质">神经网络的数学本质</h3><ul><li><p>由于神经网络复合函数的本质，使得神经网络可以很方便地组合出很多复杂的函数</p></li><li><p>由于复合函数求导法则，所以大部分神经网络的训练过程可以自动化（反向梯度传递）</p></li></ul><h3 id="一些神经网络术语">一些神经网络术语</h3><ul><li><p>神经网络的训练</p></li><li><p>神经层</p></li><li><p>激活函数</p></li><li><p>隐含层</p></li></ul><h2 id="神经网络基础训练神经网络">神经网络基础：训练神经网络</h2><h3 id="传统优化求解方法的问题">传统优化求解方法的问题</h3><ul><li><p>传统的求解方法：</p><ul><li><p>拟牛顿法等</p></li><li><p>Proximal Methods</p></li></ul></li><li><p>传统求解方法的问题</p><ul><li>传统方法需要根据全部样本计算梯度（导数），这导致对于非常复杂的网络，求解计算上根本不可行</li></ul></li></ul><h3 id="基础的梯度下降算法">基础的梯度下降算法</h3><ul><li><p>经典的优化算法</p><ul><li><p>SGD</p></li><li><p>SGD with Momentum</p></li><li><p>Adagrad</p></li><li><p>Adam</p></li></ul></li></ul><h3 id="梯度消失和梯度爆炸">梯度消失和梯度爆炸</h3><ul><li><p>由于求解过程的复杂性，这使得神经网络的求解并不一定会收敛到最优解</p></li><li><p>对于神经网络训练最大的敌人是梯度消失和梯度爆炸</p></li><li><p>解决梯度消失和梯度爆炸往往是网络设计和优化算法需要考虑的问题</p></li></ul><h2 id="神经网络基础神经网络的基础构成">神经网络基础：神经网络的基础构成</h2><h3 id="全连接层">全连接层</h3><h3 id="激活函数">激活函数</h3><h3 id="dropout">Dropout</h3><h3 id="batch-normalization">Batch Normalization</h3><h2 id="embedding-简介">Embedding 简介</h2><h3 id="什么是-embedding">什么是 Embedding</h3><h3 id="为什么我们需要-embedding">为什么我们需要 Embedding</h3><h3 id="embedding-是怎么训练的">Embedding 是怎么训练的</h3><h2 id="rnn-简介">RNN 简介</h2><h3 id="rnn">RNN</h3><h3 id="lstm">LSTM</h3><h3 id="马尔科夫过程">马尔科夫过程</h3><h3 id="隐马尔科夫过程">隐马尔科夫过程</h3><h2 id="cnn-简介">CNN 简介</h2><h3 id="cnn">CNN</h3><h3 id="cnn-如何应用在文本当中">CNN 如何应用在文本当中</h3><ul><li><p>传统来说，NLP 当中的 CNN 一般较浅，但有证据表明更深的 CNN 更有效。</p></li><li><p>在传统文本分类模型当中，CNN 效果往往要比 LSTM 分类效果好（但不一定）。</p></li></ul><p>参考：<a href="https://github.com/geektime-geekbang/NLP" target="_blank" rel="noopener" class="uri">https://github.com/geektime-geekbang/NLP</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/06/dl/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;深度学习简介&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="dl" scheme="https://2020.iosdevlog.com/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>反向图灵测试 Reverse Turing test</title>
    <link href="https://2020.iosdevlog.com/2020/03/05/Reverse-Turing-test/"/>
    <id>https://2020.iosdevlog.com/2020/03/05/Reverse-Turing-test/</id>
    <published>2020-03-05T13:27:58.000Z</published>
    <updated>2020-03-05T13:54:26.717Z</updated>
    
    <content type="html"><![CDATA[<p>如果漫画家手冢治虫还活着，会在漫画中描绘出什么样的未来？AI 是否能够帮他呈现？</p><figure><img src="https://2020.iosdevlog.com/2020/03/05/Reverse-Turing-test/1.gif" alt="" /><figcaption>AI复活已故漫画家手冢治虫</figcaption></figure><p>通过分析其作品，人工智能产生了角色设计和基本故事情节。据悉，新漫画的主人公是 AI 学习了 6000 张角色图像之后生成的。之后由专业创作者添加诸如服装和对话之类的元素以完善作品。</p><a id="more"></a><h2 id="验证码-captcha">验证码 / CAPTCHA</h2><p>全自动区分计算机和人类的公开图灵测试（英语：Completely Automated Public Turing test to tell Computers and Humans Apart，简称CAPTCHA），俗称验证码，是一种区分用户是计算机或人的公共全自动程序。在 CAPTCHA 测试中，作为服务器的计算机会自动生成一个问题由用户来解答。这个问题可以由计算机生成并评判，但是必须只有人类才能解答。由于计算机无法解答 CAPTCHA 的问题，所以回答出问题的用户就可以被认为是人类。</p><p>一种常用的 CAPTCHA 测试是让用户输入一个扭曲变形的图片上所显示的文字或数字，扭曲变形是为了避免被 <code>光学字符识别</code>（<strong>OCR</strong>, Optical Character Recognition）之类的计算机程序自动识别出图片上的文数字而失去效果。由于这个测试是由计算机来考人类，而不是标准图灵测试中那样由人类来考计算机，人们有时称 CAPTCHA 是一种 <em>反向图灵测试</em>。</p><h2 id="图灵测试">图灵测试</h2><p>图灵测试（英语：Turing test，又译图灵试验）是图灵于1950年提出的一个关于判断机器是否能够思考的著名思想实验，测试某机器是否能表现出与人等价或无法区分的智能。测试的谈话仅限于使用唯一的文本管道，例如计算机键盘和屏幕，这样的结果不依赖于计算机把单词转换为音频的能力。</p><h3 id="测试内容">测试内容</h3><blockquote><p>如果一个人（代号C）使用测试对象皆理解的语言去询问两个他不能看见的对象任意一串问题。对象为：一个是正常思维的人（代号B）、一个是机器（代号A）。如果经过若干询问以后，C不能得出实质的区别来分辨A与B的不同，则此机器A通过图灵测试。</p></blockquote><h3 id="完成图灵测试涉及的技术课题">完成图灵测试涉及的技术课题</h3><p>根据人们的大体判断，达成能够通过图灵测试的技术涉及以下课题</p><ul><li>自然语言处理</li><li>知识表示</li><li>自动推理</li><li>机器学习</li></ul><p>但是为了通过完全图灵测试，还需要另外两项额外技术课题：</p><ul><li>计算机视觉</li><li>机器人学</li></ul><h3 id="反向图灵测试和验证码">反向图灵测试和验证码</h3><p>验证码（CAPTCHA）是一种反向图灵测试。在网站上运行一些动作之前，用户被给予一个扭曲的图形，并要求用户输入图中的字母或数字。这是为了防止网站被自动化系统用来滥用。理由是能够精细地阅读和准确地重现扭曲的形象的系统并不存在（或不提供给普通用户），所以能够做到这一点的任何系统可能是一个人类。</p><p>可以破解验证码的软件正在被积极开发，软件拥有一个有一定的准确性的验证码分析模式生成引擎。而在破解验证码软件被积极开发的同时，另一种通过反向图灵测试的准则也被提出来。其认为即使破解验证码软件被成功研发，也只是具有智能的人类透过编程对验证码所作出的破解手段而已，并非真正通过反向图灵测试或图灵测试。而如果一台机器能够规划出如同验证码一类的防止自动化系统的规避程序，此台机器才算是真正通过了反向图灵测试。</p><h3 id="完全图灵测试">完全图灵测试</h3><p>普通的图灵测试一般避免审问者与被测试计算机发生物理上的互动，因为物理上模拟人（比如像模拟人的外表）并不是人工智能的研究范畴。然而一些人工智能可能涉及一些人机在物理上的交互，所以人们又拓展出了“完全图灵测试”。在完全图灵测试中，可以包含必要的人机在物理层面上的交互。但是为了通过完全图灵测试，还需要在普通图灵测试之外另外两项额外技术课题。询问者还可以测试的受试者感知能力（需要计算机视觉），和受试者操纵物体的能力（需要机器人学）。</p><h2 id="反向图灵测试-reverse-turing-test">反向图灵测试 Reverse Turing test</h2><p>可以说，反向图灵测试的标准形式是受试者试图表现为计算机而非人类的形式。</p><p>在反向图灵测试中表现最佳的人员是最了解计算机的人员，因此知道计算机在对话中可能会犯的错误的类型。</p><p>在计算机编程尤其是调试过程中，反向图灵测试的技巧与思维上模拟程序操作的技巧之间有着很多共识。结果，<code>程序员（尤其是黑客）</code> 有时会沉迷于非正式的反向图灵测试中以进行娱乐。</p><p>参考：<a href="https://www.wikipedia.org" target="_blank" rel="noopener" class="uri">https://www.wikipedia.org</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果漫画家手冢治虫还活着，会在漫画中描绘出什么样的未来？AI 是否能够帮他呈现？&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/05/Reverse-Turing-test/1.gif&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;AI复活已故漫画家手冢治虫&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;通过分析其作品，人工智能产生了角色设计和基本故事情节。据悉，新漫画的主人公是 AI 学习了 6000 张角色图像之后生成的。之后由专业创作者添加诸如服装和对话之类的元素以完善作品。&lt;/p&gt;
    
    </summary>
    
    
      <category term="game" scheme="https://2020.iosdevlog.com/categories/game/"/>
    
    
      <category term="Godot" scheme="https://2020.iosdevlog.com/tags/Godot/"/>
    
  </entry>
  
  <entry>
    <title>线性回归  Manim 演示</title>
    <link href="https://2020.iosdevlog.com/2020/03/04/manim/"/>
    <id>https://2020.iosdevlog.com/2020/03/04/manim/</id>
    <published>2020-03-04T14:00:06.000Z</published>
    <updated>2020-03-19T06:32:54.566Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/04/manim/0.jpg" alt="" /><figcaption>Linear Regression Overview</figcaption></figure><p><a href="https://youtube.com/watch?v=3vl17ymkODg" target="_blank" rel="noopener" class="uri">https://youtube.com/watch?v=3vl17ymkODg</a></p><p>机器学习算法如果能向 3B1B 一样展示，必能加深对算法的理解。</p><p>准备做一些视频展示，可以直接调用 sklearn，当然也可以自己写一套算法，为了了解原理，还是直接调用 sklearn等成熟的算法库好了（自己写有点慢，把代码看懂还是有必要的）。</p><p>Manim 非官方文档：<a href="https://elteoremadebeethoven.github.io/manim_3feb_docs.github.io/html/index.html" target="_blank" rel="noopener" class="uri">https://elteoremadebeethoven.github.io/manim_3feb_docs.github.io/html/index.html</a></p><a id="more"></a><p><img src="https://2020.iosdevlog.com/2020/03/04/manim/1.png" alt="1" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/2.png" alt="2" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/3.png" alt="3" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/4.png" alt="4" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/5.png" alt="5" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/6.png" alt="6" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/7.png" alt="7" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/8.png" alt="8" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/9.png" alt="9" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/10.png" alt="10" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/11.png" alt="11" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/12.png" alt="12" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/13.png" alt="13" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/14.png" alt="14" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/15.png" alt="15" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/16.png" alt="16" /><br /><img src="https://2020.iosdevlog.com/2020/03/04/manim/17.png" alt="17" /></p><h1 id="manim">Manim</h1><h2 id="文件的执行">文件的执行</h2><h3 id="保存为视频并立即播放">保存为视频并立即播放</h3><ul><li>-p, --preview Automatically open the saved file once its done</li></ul><h3 id="保留帧">保留帧</h3><ul><li>-s, --save_last_frame Save the last frame</li></ul><h3 id="视频尺寸">视频尺寸</h3><ul><li>-r RESOLUTION, --resolution RESOLUTION Resolution, passed as "height,width"</li></ul><h3 id="视频通道">视频通道</h3><ul><li>-t, --transparent Render to a movie file with an alpha channel</li></ul><h3 id="保留进度显示条">保留进度显示条</h3><ul><li>--leave_progress_bars Leave progress bars displayed in terminal</li></ul><h3 id="从某一个动画状态animation开始存为视频">从某一个动画状态（animation）开始存为视频</h3><ul><li>-n START_AT_ANIMATION_NUMBER, --start_at_animation_number START_AT_ANIMATION_NUMBER</li></ul><h3 id="gif">Gif</h3><ul><li>-i, --save_as_gif Save the video as gif</li></ul><h2 id="显示操作">显示操作</h2><h3 id="基本步骤">基本步骤</h3><h3 id="位置相关的函数">位置相关的函数</h3><ul><li><p>to_edge()</p><ul><li>0.1<em>RIGHT+0.2</em>UP+LEFT+DOWN</li></ul></li><li><p>to_corner()</p><ul><li>UR，UL，DR，DL</li></ul></li><li><p>move_to()</p><ul><li><p>另一个 Obj 位置</p></li><li><p>在当前位置移动到对应的位置</p></li></ul></li><li><p>next_to()</p><ul><li>someObject1对于someObject2的相对位置</li></ul></li><li><p>shift()</p><ul><li>自己垂直方向平移</li></ul></li><li><p>rotate()</p><ul><li>逆时针旋转</li></ul></li><li><p>flip()</p><ul><li>按照矢量量方向翻转180度，遵循右手定则</li></ul></li></ul><h3 id="播放动画">播放动画</h3><ul><li><p>play()</p><ul><li><p>多线程的</p></li><li><p>显示动画函数</p><ul><li><p>fading.py 渐变效果</p><ul><li><p>FadeIn</p></li><li><p>FadeInFromDown</p></li><li><p>FadeOutAndShiftDown</p></li><li><p>FadeInFromPoint</p></li><li><p>FadeInFromLarge</p></li></ul></li><li><p>creation.py 书写效果</p><ul><li><p>ShowCreation</p></li><li><p>UnCreate</p><ul><li>倒放ShowCreation动画效果</li></ul></li><li><p>DrawBorderThenFill</p><ul><li>画出边界然后填充</li></ul></li><li><p>Write</p><ul><li>从左往右逐渐显示的效果</li></ul></li></ul></li><li><p>transform.py 渐换</p><ul><li><p>Transform</p><ul><li>永远是 1 个</li></ul></li><li><p>ReplacementTransform</p><ul><li>copy</li></ul></li></ul></li><li><p>growing.py</p><ul><li><p>GrowFromCenter</p></li><li><p>GrowFromPoint</p></li><li><p>GrowFromEdge</p></li><li><p>GrowArrow</p></li></ul></li><li><p>update.py</p><ul><li><p>UpdateFromFunc</p><ul><li>同步播放</li></ul></li></ul></li></ul></li><li><p>runtime</p><ul><li>动画的快慢</li></ul></li><li><p>rate_func</p><ul><li><p>there_and_back 来回</p></li><li><p>linear 一去不复返</p></li><li><p>smooth 平滑</p></li></ul></li></ul></li><li><p>add()</p><ul><li>无动画添加文字，可以是多个text</li></ul></li><li><p>wait()</p><ul><li>等待动画停留时间</li></ul></li><li><p>remove()</p><ul><li>移除text，可以是多个text</li></ul></li></ul><h3 id="设置颜色">设置颜色</h3><ul><li>set_color(COLOR)</li></ul><h3 id="缩放大小">缩放大小</h3><ul><li>scale(number)</li></ul><h3 id="并行动画">并行动画</h3><ul><li><p>add_updater</p><ul><li>跟随另一个物体的移动</li></ul></li><li><p>remove_updater</p><ul><li>取消并行播放函数</li></ul></li><li><p>clear_updaters</p><ul><li>取消所有的并行播放函数</li></ul></li><li><p>UpdateFromFunc(Animation)</p><ul><li>play函数中生效</li></ul></li></ul><h2 id="text数组">text数组</h2><h3 id="texmobjectabovercde">TexMobject("A","{B","\over","C}","D","E")</h3><h2 id="scene基础动画">Scene 基础动画</h2><h3 id="camera_config">camera_config</h3><ul><li>background_image</li></ul><h2 id="graphscene">GraphScene</h2><p>二维坐标</p><h3 id="setup_axesanimatetrue">setup_axes(animate=True)</h3><ul><li>显示动画</li></ul><h3 id="get_graph">get_graph()</h3><ul><li>坐标系的句柄</li></ul><h3 id="coords_to_pointx-y">coords_to_point(x, y)</h3><ul><li>坐标变成对应的帧中的点</li></ul><h3 id="point_to_coordspoint">point_to_coords(point)</h3><ul><li>帧中的点转换为坐标，返回x,y组成的元组</li></ul><h3 id="get_graph_label">get_graph_label()</h3><ul><li>坐标标签</li></ul><h3 id="get_vertical_line_to_graph">get_vertical_line_to_graph()</h3><ul><li>竖线</li></ul><h3 id="getverticallinestograph">getverticallinestograph()</h3><ul><li>多条竖线</li></ul><h3 id="改变坐标标签的颜色">改变坐标标签的颜色</h3><ul><li><p>"x_label_color":RED,</p></li><li><p>"y_label_color":BLUE</p></li></ul><h2 id="textmobject">TextMobject</h2><h3 id="普通字符串">普通字符串</h3><h3 id="latex">LaTeX</h3><h3 id="混合">混合</h3><ul><li>“$<span class="math inline">\(”或者“\)</span>”</li></ul><h3 id="多个字符串">多个字符串</h3><ul><li>二维数组</li></ul><h2 id="texmobject">TexMobject</h2><h3 id="latex字符串数组">LaTeX字符串数组</h3><h2 id="二维图形类">二维图形类</h2><h3 id="点dot">点Dot</h3><ul><li><p>"radius": DEFAULT_DOT_RADIUS,</p></li><li><p>"stroke_width": 0,</p></li><li><p>"fill_opacity": 1.0,</p></li><li><p>"color": WHITE</p></li></ul><h3 id="圆形circle">圆形Circle</h3><ul><li><p>"color": RED,</p></li><li><p>"close_new_points": True,</p></li><li><p>"anchors_span_full_range": False</p></li></ul><h3 id="环annulus">环Annulus</h3><ul><li><p>"inner_radius": 1,</p></li><li><p>"outer_radius": 2,</p></li><li><p>"fill_opacity": 1,</p></li><li><p>"stroke_width": 0,</p></li><li><p>"color": WHITE,</p></li><li><p>"mark_paths_closed": False,</p></li></ul><h3 id="长方形rectangle">长方形Rectangle</h3><ul><li><p>"color": WHITE,</p></li><li><p>"height": 2.0,</p></li><li><p>"width": 4.0,</p></li><li><p>"mark_paths_closed": True,</p></li><li><p>"close_new_points": True,</p></li></ul><h3 id="方形square">方形Square</h3><ul><li>"side_length": 2.0,</li></ul><h3 id="椭圆ellipse">椭圆Ellipse</h3><ul><li><p>"width": 2,</p></li><li><p>"height": 1</p></li></ul><h3 id="弧arc">弧Arc</h3><ul><li><p>"radius": 1.0,</p></li><li><p>"num_components": 9,</p></li><li><p>"anchors_span_full_range": True,</p></li><li><p>"arc_center": ORIGIN,</p></li></ul><h3 id="线line">线Line</h3><h2 id="三维图形类">三维图形类</h2><h3 id="球-sphere">球 Sphere</h3><ul><li><p>"resolution": (12, 24),</p></li><li><p>"radius": 1,</p></li><li><p>"u_min": 0.001,</p></li><li><p>"u_max": PI - 0.001,</p></li><li><p>"v_min": 0,</p></li><li><p>"v_max": TAU,</p></li></ul><h3 id="立方-cube">立方 Cube</h3><ul><li><p>"fill_opacity": 0.75,</p></li><li><p>"fill_color": BLUE,</p></li><li><p>"stroke_width": 0,</p></li><li><p>"side_length": 2,</p></li></ul><h3 id="棱柱-prism">棱柱 Prism</h3><ul><li>"dimensions": [3, 2, 1]</li></ul><h3 id="参数曲面">参数曲面</h3><ul><li><p>ParametricSurface</p></li><li><p>ParametricFunction</p></li></ul><h2 id="群组类vgroup">群组类VGroup</h2><h3 id="批量同步操作">批量同步操作</h3><h3 id="arrange">arrange()</h3><h2 id="相机参数3d动画类threedscene">相机参数 3D动画类(ThreeDScene)</h2><h3 id="set_camera_orientation">set_camera_orientation()</h3><ul><li>相机的角度</li></ul><h3 id="move_camera">move_camera()</h3><h3 id="set_to_default_angled_camera_orientation">set_to_default_angled_camera_orientation()</h3><ul><li>还原为默认角度</li></ul><h3 id="add_fixed_in_frame_mobjects">add_fixed_in_frame_mobjects()</h3><ul><li>固定在屏幕的图像</li></ul><h2 id="坐标系类">坐标系类</h2><h3 id="numberline-数轴类">NumberLine 数轴类</h3><h3 id="坐标系抽象类-coordinatesystem">坐标系抽象类 CoordinateSystem</h3><h3 id="axes-二维坐标类">Axes 二维坐标类</h3><h3 id="threedaxes三维坐标系类">ThreeDAxes三维坐标系类</h3><h3 id="numberplane">NumberPlane</h3><h3 id="complexplane复数坐标系">ComplexPlane复数坐标系</h3><h2 id="常见的常数">常见的常数</h2><h3 id="颜色">颜色</h3><ul><li>constants.py中COLOR_MAP</li></ul><h3 id="方向">方向</h3><ul><li><p>ORIGIN = np.array((0., 0., 0.))</p></li><li><p>UP = np.array((0., 1., 0.))</p></li><li><p>DOWN = np.array((0., -1., 0.))</p></li><li><p>RIGHT = np.array((1., 0., 0.))</p></li><li><p>LEFT = np.array((-1., 0., 0.))</p></li><li><p>IN = np.array((0., 0., -1.))</p></li><li><p>OUT = np.array((0., 0., 1.))</p></li><li><p>X_AXIS = np.array((1., 0., 0.))</p></li><li><p>Y_AXIS = np.array((0., 1., 0.))</p></li><li><p>Z_AXIS = np.array((0., 0., 1.))</p></li><li><h1 id="useful-abbreviations-for-diagonals">Useful abbreviations for diagonals</h1></li><li><p>UL = UP + LEFT</p></li><li><p>UR = UP + RIGHT</p></li><li><p>DL = DOWN + LEFT</p></li><li><p>DR = DOWN + RIGHT</p></li><li><p>TOP = FRAME_Y_RADIUS * UP</p></li><li><p>BOTTOM = FRAME_Y_RADIUS * DOWN</p></li><li><p>LEFT_SIDE = FRAME_X_RADIUS * LEFT</p></li><li><p>RIGHT_SIDE = FRAME_X_RADIUS * RIGHT</p></li></ul><h3 id="角度">角度</h3><ul><li><p>PI = np.pi</p></li><li><p>TAU = 2 * PI</p></li><li><p>DEGREES = TAU / 360</p></li></ul><h3 id="距离">距离</h3><ul><li><p>SMALL_BUFF = 0.1</p></li><li><p>MED_SMALL_BUFF = 0.25</p></li><li><p>MED_LARGE_BUFF = 0.5</p></li><li><p>LARGE_BUFF = 1</p></li><li><p>DEFAULT_MOBJECT_TO_EDGE_BUFFER = MED_LARGE_BUFF</p></li><li><p>DEFAULT_MOBJECT_TO_MOBJECT_BUFFER = MED_SMALL_BUFF</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/04/manim/0.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;Linear Regression Overview&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;a href=&quot;https://youtube.com/watch?v=3vl17ymkODg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://youtube.com/watch?v=3vl17ymkODg&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;机器学习算法如果能向 3B1B 一样展示，必能加深对算法的理解。&lt;/p&gt;
&lt;p&gt;准备做一些视频展示，可以直接调用 sklearn，当然也可以自己写一套算法，为了了解原理，还是直接调用 sklearn等成熟的算法库好了（自己写有点慢，把代码看懂还是有必要的）。&lt;/p&gt;
&lt;p&gt;Manim 非官方文档：&lt;a href=&quot;https://elteoremadebeethoven.github.io/manim_3feb_docs.github.io/html/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://elteoremadebeethoven.github.io/manim_3feb_docs.github.io/html/index.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="dl" scheme="https://2020.iosdevlog.com/categories/dl/"/>
    
    
      <category term="manim" scheme="https://2020.iosdevlog.com/tags/manim/"/>
    
      <category term="lr" scheme="https://2020.iosdevlog.com/tags/lr/"/>
    
  </entry>
  
  <entry>
    <title>我的微信公众号头像侵权</title>
    <link href="https://2020.iosdevlog.com/2020/03/03/copyright/"/>
    <id>https://2020.iosdevlog.com/2020/03/03/copyright/</id>
    <published>2020-03-03T15:23:24.000Z</published>
    <updated>2020-03-03T15:55:29.538Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/03/03/copyright/1.jpg" alt="" /><figcaption>侵权</figcaption></figure><a id="more"></a><p>今天需要上传 <code>ipa</code> 到 <code>App Store</code>，使用 <code>Apple</code> 最近推出的上传工具 <code>Transporter</code> 上传。</p><p>期间遇到卡住问题，于是更新了一篇 Blog，准备也发到我的公众号：iOSDevLog。</p><p>好长时间没有登录这个帐号群发消息，一进入就发现我公众号的头像侵权了。</p><p>你的帐号经查涉嫌头像侵权，违规内容已清空处理。</p><p>你可重新进行设置但请遵守规范。如果再有类似违规情况，将加重处罚甚至永久性屏蔽所有功能。</p><p>违反规范：《微信公众平台运营规范》4.1.1条规定</p><p>微信公众平台已依法进行侵权投诉处理，法定的平台义务已经履行完毕。若你对于投诉方的意见有异议，建议你另行通过行政投诉、诉讼等方式与投诉方解决。</p><table><colgroup><col style="width: 21%" /><col style="width: 21%" /><col style="width: 15%" /><col style="width: 21%" /><col style="width: 21%" /></colgroup><tbody><tr class="odd"><td style="text-align: left;">权利人</td><td style="text-align: left;">姓名/名称</td><td style="text-align: left;">苹果</td><td style="text-align: left;">有效证件（复印件附后）</td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">通讯地址</td><td style="text-align: left;">中国 - 上海 - 浦东新区 外高桥保税区马吉路88号C区6号楼全幢</td><td style="text-align: left;">邮编</td><td style="text-align: left;">200000</td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;">联系人</td><td style="text-align: left;">O********</td><td style="text-align: left;">电话</td><td style="text-align: left;">182211*****</td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">E-mail</td><td style="text-align: left;">e******<span class="citation" data-cites="apple.com">@apple.com</span></td><td style="text-align: left;"></td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;">投诉帐号</td><td style="text-align: left;">iOS开发日志 （iOSDevLog）</td><td style="text-align: left;"></td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">投诉类型</td><td style="text-align: left;">头像侵权</td><td style="text-align: left;"></td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;">被侵权内容</td><td style="text-align: left;">商标</td><td style="text-align: left;"></td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">投诉描述</td><td style="text-align: left;">以下内容非微信官方提供，由权利人投诉时填写，请谨慎操作。</td><td style="text-align: left;">被投诉公众号未经苹果公司授权注册或运营。其账号、名称、头像及内容等多处未经授权使用苹果公司注册商标，侵犯我公司商标权，严重误导消费者。权利人要求立即停止上述侵权行为。</td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;">证明资料</td><td style="text-align: left;">商标注册书</td><td style="text-align: left;">营业执照</td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">保证声明</td><td style="text-align: left;">权利人及其代理人（统称为：声明人）诚意作如下声明</td><td style="text-align: left;">声明人在通知书中的陈述和提供的相关材料皆是真实、有效、合法的，并保证承担和赔偿腾讯因根据声明人的通知书对相关帐号的处理而给腾讯造成的任何损失，包括但不限于腾讯因向被投诉方或用户赔偿而产生的损失及腾讯名誉、商誉损害等。</td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr></tbody></table><p>对此我的态度就是 <strong>认错</strong>，要改头像。一时间又没有想好想改成什么样，就直接用我 <code>AIDevLog</code> 的二维码好了。</p><h2 id="苹果不让坏人用-iphone-好莱坞导演透露电影业潜规则1">苹果不让坏人用 iPhone !好莱坞导演透露电影业潜规则<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h2><p>瑞安·约翰逊（Rian Johnson），最受欢迎的电影制片人之一（布鲁克，布卢姆兄弟，洛珀（2012），《星球大战：最后的绝地武士》，《刀叉》）进行了有趣的采访 “名利场”。 他特别提到了安置协议 iPhone 在电影院里。 据约翰逊说， η 苹果公司 允许 使用 iPhone，但不允许坏人拥有它们 电影.</p><p>瑞安·约翰逊（Rian Johnson）具体说：</p><blockquote><p>“另外一件有趣的事，我不知道是否要说……不是因为它不好或什么，而是因为它会影响下一个 电影 我正在写一个谜..算了！ 我会说。 非常有趣</p><p>苹果…允许使用iPhone，但是-如果您正在观看一部神秘电影，这非常重要- 电影中的坏人无法拥有iPhone。</p><p>所以...哦，不！ 每一个 创造者 他的电影中有小人，他现在想杀了我！”</p></blockquote><p>众所周知 <strong>苹果公司</strong> 有 <strong>严格的规则</strong> 关于如何使用，显示和拍照iPhone和其他产品 家电 的。 例如，苹果公司报告说 该 制品 它应该只在“尽可能好的光线下”出现， 以便以最佳方式展示 iPhone。</p><p>过去许多人已经注意到，只有“好人”才能在电视节目和电影中使用Apple产品。 播放“ 24”时， 有线 写一个 理论 粉丝曾经说过 好孩子用 苹果电脑 而坏人会用它 个人电脑。 该理论似乎是正确的。</p><p>约翰逊发表声明后，观众肯定会更加专心，并观看 家电 演员用来了解坏人在电影中的适时出现。</p><p>看来 <strong>苹果公司</strong> 是非常注重它的品牌，以后对头像，图片，Code，数据等使用，最好选择无版权的或者是公开的，避免陷入不必要的麻烦（大公司让个人免费用盗版，等公司能交得起时...）。</p><section class="footnotes" role="doc-endnotes"><hr /><ol><li id="fn1" role="doc-endnote"><p>苹果不允许坏人在电影中使用iPhone！：<a href="https://zh-cn.secnews.gr/213337/iphone苹果酱kakoi/" target="_blank" rel="noopener" class="uri">https://zh-cn.secnews.gr/213337/iphone苹果酱kakoi/</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/03/03/copyright/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;侵权&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="运营" scheme="https://2020.iosdevlog.com/categories/%E8%BF%90%E8%90%A5/"/>
    
    
      <category term="公众号" scheme="https://2020.iosdevlog.com/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"/>
    
      <category term="Apple" scheme="https://2020.iosdevlog.com/tags/Apple/"/>
    
      <category term="版权" scheme="https://2020.iosdevlog.com/tags/%E7%89%88%E6%9D%83/"/>
    
  </entry>
  
</feed>
