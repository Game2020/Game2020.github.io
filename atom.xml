<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Game 2020</title>
  
  <subtitle>https://2020.iosdevlog.com</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://2020.iosdevlog.com/"/>
  <updated>2020-02-24T13:42:49.013Z</updated>
  <id>https://2020.iosdevlog.com/</id>
  
  <author>
    <name>iOSDevLog</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Coursera 课程免费旁听与下载</title>
    <link href="https://2020.iosdevlog.com/2020/02/24/coursera/"/>
    <id>https://2020.iosdevlog.com/2020/02/24/coursera/</id>
    <published>2020-02-23T18:36:14.000Z</published>
    <updated>2020-02-24T13:42:49.013Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/24/coursera/4.png" alt="" /><figcaption>4</figcaption></figure><a id="more"></a><h2 id="旁听-coursera">旁听 Coursera</h2><h3 id="打开-coursera-上的课程-tensorflow-data-and-deployment-专项课程">打开 Coursera 上的课程 <a href="ttps://www.coursera.org/specializations/tensorflow-data-and-deployment">TensorFlow: Data and Deployment 专项课程</a></h3><p><a href="https://www.coursera.org/specializations/tensorflow-data-and-deployment" target="_blank" rel="noopener" class="uri">https://www.coursera.org/specializations/tensorflow-data-and-deployment</a></p><figure><img src="https://2020.iosdevlog.com/2020/02/24/coursera/1.png" alt="" /><figcaption>课程</figcaption></figure><h3 id="点击里面的-课程-4-advanced-deployment-scenarios-with-tensorflow">点击里面的 <a href="https://www.coursera.org/learn/advanced-deployment-scenarios-tensorflow" target="_blank" rel="noopener">课程 4 Advanced Deployment Scenarios with TensorFlow</a></h3><p><a href="https://www.coursera.org/learn/advanced-deployment-scenarios-tensorflow" target="_blank" rel="noopener" class="uri">https://www.coursera.org/learn/advanced-deployment-scenarios-tensorflow</a></p><figure><img src="https://2020.iosdevlog.com/2020/02/24/coursera/2.png" alt="" /><figcaption>免费注册</figcaption></figure><h3 id="点击-免费注册">点击 <strong>免费注册</strong></h3><figure><img src="https://2020.iosdevlog.com/2020/02/24/coursera/3.png" alt="" /><figcaption>旁听</figcaption></figure><h3 id="点击-旁听">点击 <strong>旁听</strong></h3><figure><img src="https://2020.iosdevlog.com/2020/02/24/coursera/4.png" alt="" /><figcaption>完成</figcaption></figure><h2 id="下载视频和字幕">下载视频和字幕</h2><p>dl-coursera 0.1.2</p><p><a href="https://pypi.org/project/dl-coursera/" target="_blank" rel="noopener" class="uri">https://pypi.org/project/dl-coursera/</a></p><p><code>Chrome</code> 导出 <code>cookies.txt</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip3 install -U dl_coursera</span><br><span class="line">dl_coursera --version</span><br><span class="line">dl_coursera --cookies path/to/cookies.txt --slug advanced-deployment-scenarios-tensorflow --how <span class="built_in">builtin</span></span><br></pre></td></tr></table></figure><p><code>tree advanced-deployment-scenarios-tensorflow</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">advanced-deployment-scenarios-tensorflow&#x2F;</span><br><span class="line">├── 01@tensorflow-extended</span><br><span class="line">│   ├── 01@tf-serving-as-another-deployment-option-</span><br><span class="line">│   │   ├── 01@introduction-a-conversation-with-andrew-</span><br><span class="line">│   │   │   ├── 01@.mp4</span><br><span class="line">│   │   │   └── 01@.srt</span><br><span class="line">│   │   ├── 02@introduction</span><br><span class="line">│   │   │   ├── 01@.mp4</span><br><span class="line">│   │   │   └── 01@.srt</span><br><span class="line">│   │   ├── 03@downloading-the-coding-examples-and-exer</span><br><span class="line">│   │   │   ├── 01@downloading-the-coding-examples-and-exer.html</span><br><span class="line">│   │   │   ├── 1.png</span><br><span class="line">│   │   │   └── github_screenshot.png</span><br><span class="line">│   │   ├── 04@serving</span><br><span class="line">│   │   │   ├── 01@.mp4</span><br><span class="line">│   │   │   └── 01@.srt</span><br><span class="line">│   │   ├── 05@installing-tf-serving</span><br><span class="line">│   │   │   ├── 01@.mp4</span><br><span class="line">│   │   │   └── 01@.srt</span><br><span class="line">│   │   ├── 06@installation-link</span><br><span class="line">│   │   │   └── 01@installation-link.html</span><br><span class="line">│   │   └── 07@tensorflow-serving-summary</span><br><span class="line">│   │       ├── 01@.mp4</span><br></pre></td></tr></table></figure><h2 id="上传百度网盘">上传百度网盘</h2><p>BaiduPCS-Go</p><p><a href="https://github.com/iikira/BaiduPCS-Go" target="_blank" rel="noopener" class="uri">https://github.com/iikira/BaiduPCS-Go</a></p><p>后台上传</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohub /xxx/BaiduPCS-Go u XXX . &amp;</span><br></pre></td></tr></table></figure><p>查看进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep Baidu</span><br><span class="line">iosdevl+ 16986     1  0 14:46 ?        00:00:01 BaiduPCS-Go-v3.6.1-linux-amd64/BaiduPCS-Go u xx/ .</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/24/coursera/4.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;4&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="学习" scheme="https://2020.iosdevlog.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Coursera" scheme="https://2020.iosdevlog.com/tags/Coursera/"/>
    
      <category term="download" scheme="https://2020.iosdevlog.com/tags/download/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow-Data-and-Deployment</title>
    <link href="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/"/>
    <id>https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/</id>
    <published>2020-02-23T03:01:43.000Z</published>
    <updated>2020-02-23T12:14:47.699Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/1.png" alt="" /><figcaption>Layer</figcaption></figure><p>GitHub 源码：<a href="https://github.com/GameDevLog/TensorFlow-Data-and-Deployment-Specialization" target="_blank" rel="noopener" class="uri">https://github.com/GameDevLog/TensorFlow-Data-and-Deployment-Specialization</a></p><a id="more"></a><h2 id="使用tensorflow.js的基于浏览器的模型browser-based-models-with-tensorflow.js">1. 使用TensorFlow.js的基于浏览器的模型(Browser-based Models with TensorFlow.js)</h2><p>将机器学习模型带入现实世界不仅仅涉及建模。本专业知识将教您如何导航各种部署方案并更有效地使用数据来训练模型。在第一门课程中，您将使用TensorFlow.js在任何浏览器中训练和运行机器学习模型。您将学习在浏览器中处理数据的技术，最后将建立一个计算机视觉项目，该项目可以识别和分类来自网络摄像头的对象。该专业化基于我们的TensorFlow实践专业化。如果您不熟悉TensorFlow，我们建议您首先参加TensorFlow实践专业化课程。为了深入了解神经网络的工作原理，我们建议您参加“​​深度学习专业化”课程。</p><h3 id="building-the-model">Building the Model</h3><h3 id="first-html-page">First HTML Page</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span>First HTML Page.<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="tfjs-script">tfjs script</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -1,6 +1,7 @@</span></span><br><span class="line"> &lt;html&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;head&gt;&lt;/head&gt;</span><br><span class="line"><span class="addition">+&lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"&gt;&lt;/script&gt;</span></span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br><span class="line">     &lt;h1&gt;First HTML Page.&lt;/h1&gt;</span><br></pre></td></tr></table></figure><h3 id="model">model</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -1,7 +1,17 @@</span></span><br><span class="line"> &lt;html&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;head&gt;&lt;/head&gt;</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"> &lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"&gt;&lt;/script&gt;</span><br><span class="line"><span class="addition">+&lt;script lang="js"&gt;</span></span><br><span class="line"><span class="addition">+    const model = tf.sequential();</span></span><br><span class="line"><span class="addition">+    model.add(tf.layers.dense(&#123; units: 1, inputShape: [1] &#125;));</span></span><br><span class="line"><span class="addition">+    model.compile(&#123;</span></span><br><span class="line"><span class="addition">+        loss: 'meanSquaredError',</span></span><br><span class="line"><span class="addition">+        optimizer: 'sgd'</span></span><br><span class="line"><span class="addition">+    &#125;);</span></span><br><span class="line"><span class="addition">+    model.summary();</span></span><br><span class="line"><span class="addition">+&lt;/script&gt;</span></span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br><span class="line">     &lt;h1&gt;First HTML Page.&lt;/h1&gt;</span><br></pre></td></tr></table></figure><h3 id="data">data</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -11,6 +11,9 @@</span></span><br><span class="line">         optimizer: 'sgd'</span><br><span class="line">     &#125;);</span><br><span class="line">     model.summary();</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+    const xs = tf.tensor2d([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], [6, 1]);</span></span><br><span class="line"><span class="addition">+    const ys = tf.tensor2d([-3.0, -1.0, 2.0, 3.0, 5.0, 7.0], [6, 1]);</span></span><br><span class="line"> &lt;/script&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br></pre></td></tr></table></figure><h3 id="train">train</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -11,6 +11,13 @@</span></span><br><span class="line">         optimizer: 'sgd'</span><br><span class="line">     &#125;);</span><br><span class="line">     model.summary();</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+    const xs = tf.tensor2d([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], [6, 1]);</span></span><br><span class="line"><span class="addition">+    const ys = tf.tensor2d([-3.0, -1.0, 2.0, 3.0, 5.0, 7.0], [6, 1]);</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+    doTraining(model).then(() =&gt; &#123;</span></span><br><span class="line"><span class="addition">+        alert(model.predict(tf.tensor2d([10], [1, 1])));</span></span><br><span class="line"><span class="addition">+    &#125;);</span></span><br><span class="line"> &lt;/script&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br></pre></td></tr></table></figure><h3 id="dotraining">doTraining</h3><p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"> @@ -4,6 +4,23 @@</span><br><span class="line"> </span><br><span class="line"> &lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"&gt;&lt;/script&gt;</span><br><span class="line"> &lt;script lang="js"&gt;</span><br><span class="line"><span class="addition">+    async function doTraining(model) &#123;</span></span><br><span class="line"><span class="addition">+        const history =</span></span><br><span class="line"><span class="addition">+            await model.fit(xs, ys,</span></span><br><span class="line"><span class="addition">+                &#123;</span></span><br><span class="line"><span class="addition">+                    epochs: 500,</span></span><br><span class="line"><span class="addition">+                    callbacks: &#123;</span></span><br><span class="line"><span class="addition">+                        onEpochEnd: async (epoch, logs) =&gt; &#123;</span></span><br><span class="line"><span class="addition">+                            console.log("Epoch:"</span></span><br><span class="line"><span class="addition">+                                + epoch</span></span><br><span class="line"><span class="addition">+                                + " Loss:"</span></span><br><span class="line"><span class="addition">+                                + logs.loss);</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+                        &#125;</span></span><br><span class="line"><span class="addition">+                    &#125;</span></span><br><span class="line"><span class="addition">+                &#125;);</span></span><br><span class="line"><span class="addition">+    &#125;</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line">     const model = tf.sequential();</span><br><span class="line">     model.add(tf.layers.dense(&#123; units: 1, inputShape: [1] &#125;));</span><br><span class="line">     model.compile(&#123;</span><br></pre></td></tr></table></figure></p><h3 id="test">test</h3><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/2.png" alt="" /><figcaption>Safari</figcaption></figure><h3 id="iris">Iris</h3><p><a href="https://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" rel="noopener" class="uri">https://archive.ics.uci.edu/ml/datasets/Iris</a></p><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/3.png" alt="" /><figcaption>Iris</figcaption></figure><p><a href="https://commons.wikimedia.org/w/index.php?curid=46257808" target="_blank" rel="noopener" class="uri">https://commons.wikimedia.org/w/index.php?curid=46257808</a></p><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/Iris_dataset_scatterplot.svg" alt="" /><figcaption>Iris_dataset_scatterplot</figcaption></figure><h3 id="iris.csv">iris.csv</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sepal_length,sepal_width,petal_length,petal_width,species</span><br><span class="line">5.1,3.5,1.4,0.2,setosa</span><br><span class="line">4.9,3,1.4,0.2,setosa</span><br><span class="line">4.7,3.2,1.3,0.2,setosa</span><br><span class="line">4.6,3.1,1.5,0.2,setosa</span><br><span class="line">5,3.6,1.4,0.2,setosa</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="async">async</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -3,6 +3,8 @@</span></span><br><span class="line"> &lt;head&gt;&lt;/head&gt;</span><br><span class="line"> &lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"&gt;&lt;/script&gt;</span><br><span class="line"> &lt;script lang="js"&gt;</span><br><span class="line"><span class="addition">+    async function run() &#123;</span></span><br><span class="line"><span class="addition">+    &#125;</span></span><br><span class="line"> &lt;/script&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br></pre></td></tr></table></figure><p>### load iris.csv</p><p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> @@ -3,6 +3,16 @@</span><br><span class="line"> &lt;head&gt;&lt;/head&gt;</span><br><span class="line"> &lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"&gt;&lt;/script&gt;</span><br><span class="line"> &lt;script lang="js"&gt;</span><br><span class="line"><span class="addition">+    async function run() &#123;</span></span><br><span class="line"><span class="addition">+        const csvUrl = 'iris.csv';</span></span><br><span class="line"><span class="addition">+        const trainingData = tf.data.csv(csvUrl, &#123;</span></span><br><span class="line"><span class="addition">+            columnConfigs: &#123;</span></span><br><span class="line"><span class="addition">+                species: &#123;</span></span><br><span class="line"><span class="addition">+                    isLabel: true</span></span><br><span class="line"><span class="addition">+                &#125;</span></span><br><span class="line"><span class="addition">+            &#125;</span></span><br><span class="line"><span class="addition">+        &#125;);</span></span><br><span class="line"><span class="addition">+    &#125;</span></span><br><span class="line"> &lt;/script&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br></pre></td></tr></table></figure></p><h3 id="one-hot-encoder">One-Hot encoder</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -12,6 +12,18 @@</span></span><br><span class="line">                 &#125;</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;);</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        const numOfFeatures = (await trainingData.columnNames()).length - 1;</span></span><br><span class="line"><span class="addition">+        const numOfSamples = 150;</span></span><br><span class="line"><span class="addition">+        const convertedData =</span></span><br><span class="line"><span class="addition">+            trainingData.map((&#123; xs, ys &#125;) =&gt; &#123;</span></span><br><span class="line"><span class="addition">+                const labels = [</span></span><br><span class="line"><span class="addition">+                    ys.species == "setosa" ? 1 : 0,</span></span><br><span class="line"><span class="addition">+                    ys.species == "virginica" ? 1 : 0,</span></span><br><span class="line"><span class="addition">+                    ys.species == "versicolor" ? 1 : 0</span></span><br><span class="line"><span class="addition">+                ]</span></span><br><span class="line"><span class="addition">+                return &#123; xs: Object.values(xs), ys: Object.values(labels) &#125;;</span></span><br><span class="line"><span class="addition">+            &#125;).batch(10);</span></span><br><span class="line">     &#125;</span><br><span class="line"> &lt;/script&gt;</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/4.png" alt="" /><figcaption>One-Hot Encoder</figcaption></figure><h2 id="nn">NN</h2><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/5.png" alt="" /><figcaption>NN</figcaption></figure><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -24,6 +24,14 @@</span></span><br><span class="line">                 ]</span><br><span class="line">                 return &#123; xs: Object.values(xs), ys: Object.values(labels) &#125;;</span><br><span class="line">             &#125;).batch(10);</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        const model = tf.sequential();</span></span><br><span class="line"><span class="addition">+        model.add(tf.layers.dense(&#123;</span></span><br><span class="line"><span class="addition">+            inputShape: [numOfFeatures],</span></span><br><span class="line"><span class="addition">+            activation: "sigmoid", units: 5</span></span><br><span class="line"><span class="addition">+        &#125;))</span></span><br><span class="line"><span class="addition">+        model.add(tf.layers.dense(&#123; activation: "softmax", units: 3 &#125;));</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line">     &#125;</span><br><span class="line"> &lt;/script&gt;</span><br></pre></td></tr></table></figure><h3 id="compile">compile</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -32,6 +32,10 @@</span></span><br><span class="line">         &#125;))</span><br><span class="line">         model.add(tf.layers.dense(&#123; activation: "softmax", units: 3 &#125;));</span><br><span class="line"> </span><br><span class="line"><span class="addition">+        model.compile(&#123;</span></span><br><span class="line"><span class="addition">+            loss: "categoricalCrossentropy",</span></span><br><span class="line"><span class="addition">+            optimizer: tf.train.adam(0.06)</span></span><br><span class="line"><span class="addition">+        &#125;);</span></span><br><span class="line">     &#125;</span><br><span class="line"> &lt;/script&gt;</span><br></pre></td></tr></table></figure><h3 id="fit">fit</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -36,6 +36,15 @@</span></span><br><span class="line">             loss: "categoricalCrossentropy",</span><br><span class="line">             optimizer: tf.train.adam(0.06)</span><br><span class="line">         &#125;);</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        await model.fitDataset(convertedData, &#123;</span></span><br><span class="line"><span class="addition">+            epochs: 100,</span></span><br><span class="line"><span class="addition">+            callbacks: &#123;</span></span><br><span class="line"><span class="addition">+                onEpochEnd: async (epoch, logs) =&gt; &#123;</span></span><br><span class="line"><span class="addition">+                    console.log("Epoch: " + epoch + " Loss: " + logs.loss);</span></span><br><span class="line"><span class="addition">+                &#125;</span></span><br><span class="line"><span class="addition">+            &#125;</span></span><br><span class="line"><span class="addition">+        &#125;);</span></span><br><span class="line">     &#125;</span><br><span class="line"> &lt;/script&gt;</span><br></pre></td></tr></table></figure><p>### predict</p><p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -45,6 +45,10 @@</span></span><br><span class="line">                 &#125;</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;);</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        const testVal = tf.tensor2d([5.8, 2.7, 5.1, 1.9], [1, 4]);</span></span><br><span class="line"><span class="addition">+        const prediction = model.predict(testVal);</span></span><br><span class="line"><span class="addition">+        alert(prediction)</span></span><br><span class="line">     &#125;</span><br><span class="line"> &lt;/script&gt;</span><br></pre></td></tr></table></figure></p><h3 id="run">run</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -50,6 +50,8 @@</span></span><br><span class="line">         const prediction = model.predict(testVal);</span><br><span class="line">         alert(prediction)</span><br><span class="line">     &#125;</span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+    run();</span></span><br><span class="line"> &lt;/script&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;body&gt;</span><br></pre></td></tr></table></figure><p>HTTP Server</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python2 -m SimpleHTTPServer</span><br></pre></td></tr></table></figure></p><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/6.png" alt="" /><figcaption>run</figcaption></figure><p>第 2 种概率最高</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor</span><br><span class="line">     [[0.0000663, 0.8690438, 0.1308898],]</span><br></pre></td></tr></table></figure><h3 id="classname">className</h3><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -46,9 +46,20 @@</span></span><br><span class="line">             &#125;</span><br><span class="line">         &#125;);</span><br><span class="line"> </span><br><span class="line"><span class="addition">+        // Setosa</span></span><br><span class="line"><span class="addition">+        // const testVal = tf.tensor2d([4.4, 2.9, 1.4, 0.2], [1, 4]);</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        // Versicolor</span></span><br><span class="line"><span class="addition">+        // const testVal = tf.tensor2d([6.4, 3.2, 4.5, 1.5], [1, 4]);</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        // Virginica</span></span><br><span class="line">         const testVal = tf.tensor2d([5.8, 2.7, 5.1, 1.9], [1, 4]);</span><br><span class="line"><span class="addition">+</span></span><br><span class="line">         const prediction = model.predict(testVal);</span><br><span class="line"><span class="deletion">-        alert(prediction)</span></span><br><span class="line"><span class="addition">+        const pIndex = tf.argMax(prediction, axis = 1).dataSync();</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        const classNames = ["Setosa", "Virginica", "Versicolor"];</span></span><br><span class="line"><span class="addition">+        alert(classNames[pIndex])</span></span><br><span class="line">     &#125;</span><br><span class="line"> </span><br><span class="line">     run();</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/7.png" alt="" /><figcaption>className</figcaption></figure><h2 id="使用tensorflow-lite的基于设备的模型device-based-models-with-tensorflow-lite">2. 使用TensorFlow Lite的基于设备的模型(Device-based Models with TensorFlow Lite)</h2><p>将机器学习模型带入现实世界不仅仅涉及建模。本专业知识将教您如何导航各种部署方案并更有效地使用数据来训练模型。第二门课程教您如何在移动应用程序中运行机器学习模型。您将学习如何为低功耗，电池供电的设备准备模型，然后在Android和iOS平台上执行模型。最后，您将探索如何在Raspberry Pi和微控制器上使用TensorFlow在嵌入式系统上进行部署。该专业化基于我们的TensorFlow实践专业化。如果您不熟悉TensorFlow，我们建议您首先参加TensorFlow实践专业化课程。为了深入了解神经网络的工作原理，</p><h2 id="使用tensorflow数据服务的数据管道data-pipelines-with-tensorflow-data-services">3. 使用TensorFlow数据服务的数据管道(Data Pipelines with TensorFlow Data Services)</h2><p>将机器学习模型带入现实世界不仅仅涉及建模。本专业知识将教您如何导航各种部署方案并更有效地使用数据来训练模型。在这第三门课程中，您将在TensorFlow中使用一套工具来更有效地利用数据和训练模型。您将学习如何仅用几行代码就可以利用内置数据集，如何使用API​​控制如何拆分数据以及如何处理所有类型的非结构化数据。该专业化基于我们的TensorFlow实践专业化。如果您不熟悉TensorFlow，我们建议您首先参加TensorFlow实践专业化课程。为了深入了解神经网络的工作原理，我们建议您参加“​​深度学习专业化”课程。</p><h2 id="使用tensorflow的高级部署方案advanced-deployment-scenarios-with-tensorflow">4. 使用TensorFlow的高级部署方案(Advanced Deployment Scenarios with TensorFlow)</h2><p>将机器学习模型带入现实世界不仅仅涉及建模。本专业知识将教您如何导航各种部署方案并更有效地使用数据来训练模型。在这最后的课程中，您将探索在部署模型时会遇到的四种不同情况。将向您介绍TensorFlow Serving，该技术可让您通过Web进行推理。您将继续使用TensorFlow Hub，该模型库可用于转移学习。然后，您将使用TensorBoard评估并了解模型的工作方式，并与他人共享模型元数据。最后，您将探索联合学习，以及如何在保持数据隐私的同时使用用户数据重新训练已部署的模型。该专业化基于我们的TensorFlow实践专业化。如果您不熟悉TensorFlow，我们建议您首先参加TensorFlow实践专业化课程。为了深入了解神经网络的工作原理，我们建议您参加“​​深度学习专业化”课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/23/TensorFlow-Data-and-Deployment/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;Layer&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;GitHub 源码：&lt;a href=&quot;https://github.com/GameDevLog/TensorFlow-Data-and-Deployment-Specialization&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://github.com/GameDevLog/TensorFlow-Data-and-Deployment-Specialization&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="TensorFlow" scheme="https://2020.iosdevlog.com/tags/TensorFlow/"/>
    
      <category term="iOS" scheme="https://2020.iosdevlog.com/tags/iOS/"/>
    
      <category term="Deployment" scheme="https://2020.iosdevlog.com/tags/Deployment/"/>
    
      <category term="js" scheme="https://2020.iosdevlog.com/tags/js/"/>
    
      <category term="mobile" scheme="https://2020.iosdevlog.com/tags/mobile/"/>
    
      <category term="Android" scheme="https://2020.iosdevlog.com/tags/Android/"/>
    
  </entry>
  
  <entry>
    <title>《极简算法史：从数学到机器的故事》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/02/22/9787115500809/"/>
    <id>https://2020.iosdevlog.com/2020/02/22/9787115500809/</id>
    <published>2020-02-22T15:00:26.000Z</published>
    <updated>2020-02-22T17:44:16.115Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/1.jpg" alt="" /><figcaption>《极简算法史：从数学到机器的故事》</figcaption></figure><p>书名：极简算法史：从数学到机器的故事<br />作者：[法]吕克·德·布拉班迪尔<br />译者：任轶<br />出版社：人民邮电出版社<br />出版时间：2018-12<br />ISBN：9787115500809</p><p>一位工程师、一位数学家、一位逻辑学家和一位哲学家一起在苏格兰旅行。他们走在一条路上，栖息在悬岩上的一只黑山羊看着他们路过。</p><p>“你们看到了吗？”工程师说，“在苏格兰，山羊都是黑色的！”</p><p>数学家反驳道：“可能你想说的是：有些苏格兰山羊是黑色的。”</p><p>逻辑学家补充道：“先不要妄下结论。我们只能说：苏格兰至少有一只黑山羊！”</p><p>最后，哲学家总结道：“我们唯一能真正确定的是：在这个地方的这只山羊是黑色的！”</p><a id="more"></a><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/2.jpg" alt="" /><figcaption>序</figcaption></figure><p><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/3.jpg" alt="柏拉图" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/4.jpg" alt="亚里士多德" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/6.jpg" alt="阿尔·花拉子米" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/9.jpg" alt="笛卡儿" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/11.jpg" alt="伽利略" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/13.jpg" alt="布莱士·帕斯卡" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/15.jpg" alt="托马斯·贝叶斯" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/16.jpg" alt="莱布尼茨" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/19.jpg" alt="欧拉" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/23.jpg" alt="康德" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/24.jpg" alt="乔治·布尔" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/25.jpg" alt="库尔特·哥德尔" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/27.jpg" alt="伯特兰·罗素" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/28.jpg" alt="路德维希·维特根斯坦" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/29.jpg" alt="克劳德·香农" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/31.jpg" alt="诺伯特·维纳" /><br /><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/32.jpg" alt="阿兰·图灵" /></p><h2 id="第一部分-莱布尼茨之梦">第一部分 莱布尼茨之梦</h2><p>三次“抽象运动”的硕果</p><ul><li>算术</li><li>几何</li><li>代数</li></ul><p>第四次抽象运动</p><ul><li>逻辑学<ul><li>演绎法<ul><li>这条街上的所有房子都很漂亮。</li><li>这座房子在这条街上。</li><li>这座房子很漂亮。</li></ul></li><li>归纳法<ul><li>这座房子在这条街上。</li><li>这座房子很漂亮。</li><li>在这条街上所有的房子都很漂亮。</li></ul></li><li>溯因法<ul><li>这座房子很漂亮。</li><li>在这条街上所有的房子都很漂亮。</li><li>这座房子在这条街上。</li></ul></li></ul></li></ul><blockquote><p>数学与语言无关，逻辑却并非如此。</p></blockquote><h3 id="哥德尔证明罗素是在浪费时间">哥德尔证明，罗素是在浪费时间</h3><ol type="1"><li>三角形内角和为180°；</li><li>正方形的内角和为270°。</li><li>1 是正确的。</li><li>2 是错误的。</li><li>在定理中无法对 5 加以证明。</li></ol><p>于是有两种可能的情况：</p><ol type="1"><li>要么可以证明 5，但是，由于语句说明的情况与此相反，因而定理不具有逻辑的严密性；</li><li>要么无法证明 5，因而语句为真，但这意味着，定理不具有完备性。</li></ol><p><strong>悖论</strong>：“所有克里特人都是骗子”的现代版——</p><blockquote><p>埃庇米尼得斯虽然这么宣布了，但他自己就是克里特人，如果他说的是真的，那么既然他也是克里特人，那说明他也是个骗子，他的话就不可信；而如果他说谎了，那么就印证了“所有克里特人都是骗子”这句话，那说明他所言为真……</p></blockquote><h2 id="第二部分-三座丰碑">第二部分 三座丰碑</h2><h3 id="贝叶斯">贝叶斯</h3><p>《关于如何在机会论的框架下解决问题》（Anessay towards solving a problem in thedoctrine of Chance</p><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/59.jpg" alt="" /><figcaption>罐子</figcaption></figure><p>每个罐子里装有40颗石子。1号罐子里装了30颗白色石子和10颗黑色石子，2号罐子里装有黑白石子各20颗。假设随机拿起一个罐子，并从这个罐子里随机取出一颗石子，且这颗石子是白色的，那么这颗白色石子来自1号罐子的概率是多少？</p><ol type="1"><li>已知被选中的石子是白色的，用获得白色石子的概率乘以选择1号罐子的概率；</li><li>已知白色石子来自1号罐子，用选择1号罐子的概率乘以获得白色石子的概率。</li></ol><p>A：1号罐子的假设<br />B：白色石子的假设</p><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/60.jpg" alt="" /><figcaption>相同的答案</figcaption></figure><p><span class="math display">\[p(\mathrm{B}). p(\mathrm{A} / 已知 \mathrm{B})=p(\mathrm{A}). p(\mathrm{B} / 已知 \mathrm{A})\]</span></p><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/61.jpg" alt="" /><figcaption>贝叶斯公式</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/62.jpg" alt="" /><figcaption>答案</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/63.jpg" alt="" /><figcaption>贝叶斯网络</figcaption></figure><h3 id="香农证明如何计算11">香农证明，如何计算1+1</h3><p>信息动力学，两个定理分别探讨的是 <strong>信息量</strong> 和信息的 <strong>质量</strong>。</p><ul><li>第一个定理涉及信息的压缩<ul><li>编码一条信息所需的最少符号数量是多少？</li></ul></li><li>第二个定理涉及信息的传输<ul><li>为了在终点处获取从起点处发出的准确信息，需要哪些必要条件？</li></ul></li></ul><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/65.jpg" alt="" /><figcaption>通信的环境进行建模</figcaption></figure><p>测量单位。就像卡路里可以量化热交换一样，香农提出的“比特”（bit，也叫位）的概念用于测量信息量。</p><p>比特是一个二进制数字，可以取0或1的值。</p><p>信息的测量应该满足一个苛刻的关系</p><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/66.jpg" alt="" /><figcaption>对数函数</figcaption></figure><p>其中，<span class="math inline">\(f\)</span> 只能是对数函数，因为根据定义，同一底数的两个正数的对数之和等于这两个数的积的对数。</p><p>香农的公式能让我们借助经验计算出摩尔斯电码的效率为85%！我们不得不佩服公式发明者出色的直觉。</p><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/68.jpg" alt="" /><figcaption>形式逻辑</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/69.jpg" alt="" /><figcaption>与或非门</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/70.jpg" alt="" /><figcaption>逻辑电路</figcaption></figure><p>香农数：国际象棋棋局的理论数目，结果是 <span class="math inline">\(10^{120}\)</span></p><h3 id="诺伯特维纳与控制论cybernetics">诺伯特·维纳与控制论（cybernetics）</h3><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/73.jpg" alt="" /><figcaption>控制</figcaption></figure><h2 id="第三部分-自动化理性批判">第三部分 自动化理性批判</h2><blockquote><p>科学是父亲教给儿子。<br />技术是儿子教给父亲。<br />——米歇尔·塞尔</p></blockquote><h3 id="算法algorithm">算法（algorithm）</h3><p>三种不确定性</p><ol type="1"><li>圣诞节是在什么时候？你不知道，我也不知道。</li><li>谁将在2022年当选法国总统？你不知道，我也不知道。</li><li>因为我们甚至不知道“我们不知道”。</li></ol><p>第三类不确定性涉及了“没人提出的问题”。在这种情况下，超级计算机也没有用了……因为没有什么可以计算的！</p><p>这类事件通常被称为 <strong>“黑天鹅”</strong>。</p><p>这一说法是用来纪念一位18世纪的英国探险家，这位探险家曾确信所有天鹅都是白色的，然而，他在澳大利亚逗留期间，惊讶地看到了一只黑天鹅——没有一个欧洲人曾对这类水禽的颜色提出过疑问。于是，黑天鹅成为第三类不确定性的象征。</p><h3 id="全球化管理的重要性">全球化管理的重要性</h3><ul><li>互联网并非公共空间。</li><li>互联网并非全球性的。</li><li>互联网并非环保。</li><li>互联网既不是虚拟的，也不是非物质的。</li><li>互联网并非透明。（大数据）</li><li>互联网并非中立。（算法）</li><li>互联网在其运作过程中并没有完全被理解。</li><li>互联网并非市场经济的保障。</li><li>互联网并非民主的保障。</li><li>互联网并非掌握真相。</li><li>互联网积累的信息正在变得无用。</li><li>互联网并非友善。（暴力）</li><li>互联网是脆弱的。（Bug）</li><li>互联网并非自动。（参数）</li><li>互联网并非自由。（surf，原意是冲浪）</li><li>互联网只有部分是可访问的。（暗网）</li><li>互联网并非优质教育的保障。</li><li>互联网并非公平。</li><li>互联网并非免费。</li><li>互联网是我们的工具，而我们也是互联网的工具。（测试）</li><li>互联网不好也不坏。</li><li>互联网，尤其对民主国家来说，是空前的挑战。</li></ul><h3 id="死亡电脑社">死亡电脑社</h3><p>互联网上的预言大师名为 <strong>奇点</strong>。</p><p>奇点指的是机器与人类彻底融合的时刻，这种情况注定会在某一天发生。</p><h3 id="人工智能许多问题之一">人工智能：许多问题之一</h3><p>智商（IQ）</p><ul><li>音乐智力，这种智力体现为对声音和节奏的感知度。它寻找音符的含义，并想象改编为其他乐曲的可能性。</li><li>运动智力，这种智力能释放身体各个部位的潜能。它组织了用来解决特定问题的最佳动作序列。</li><li>人际关系智力（或情感智力），这种智力能识别他人的感受和意图。它能感知到对于谈判、合作和互动等行为来说，什么是重要的因素。</li><li>视觉智力，这种智力可以从三个维度进行思考。它能让我们在建模之前、在空间内移动物体之前、在看到被要求思考的东西之前，就先进行想象。</li><li>语言智力，这种智力是利用语言反应的能力。如果有必要，这种智力甚至能够催生新的语言。</li></ul><h2 id="答案">答案</h2><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/89.jpg" alt="" /><figcaption>89</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/22/9787115500809/90.jpg" alt="" /><figcaption>90</figcaption></figure><p>从古希腊哲学到数学，从逻辑推理到“无所不能”的计算机。柏拉图、莱布尼茨、罗素、香农、图灵……一个个伟大的思想家试图从数学公式中证明推理的合理性。</p><p>他们是凭借天赋制胜，还是在鲁莽地大胆一搏？</p><p>如何将逻辑赋予数学意义？</p><p>如何从简单运算，走向复杂智慧？</p><p>一场人类探索数学、算法与逻辑思维，并最终走向人工智能的梦想之旅，展现了哲学家、逻辑学家与数学家独特的思维方式，探讨了算法与人工智能对科学和社会的巨大影响。</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/22/9787115500809/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《极简算法史：从数学到机器的故事》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：极简算法史：从数学到机器的故事&lt;br /&gt;
作者：[法]吕克·德·布拉班迪尔&lt;br /&gt;
译者：任轶&lt;br /&gt;
出版社：人民邮电出版社&lt;br /&gt;
出版时间：2018-12&lt;br /&gt;
ISBN：9787115500809&lt;/p&gt;
&lt;p&gt;一位工程师、一位数学家、一位逻辑学家和一位哲学家一起在苏格兰旅行。他们走在一条路上，栖息在悬岩上的一只黑山羊看着他们路过。&lt;/p&gt;
&lt;p&gt;“你们看到了吗？”工程师说，“在苏格兰，山羊都是黑色的！”&lt;/p&gt;
&lt;p&gt;数学家反驳道：“可能你想说的是：有些苏格兰山羊是黑色的。”&lt;/p&gt;
&lt;p&gt;逻辑学家补充道：“先不要妄下结论。我们只能说：苏格兰至少有一只黑山羊！”&lt;/p&gt;
&lt;p&gt;最后，哲学家总结道：“我们唯一能真正确定的是：在这个地方的这只山羊是黑色的！”&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="Math" scheme="https://2020.iosdevlog.com/tags/Math/"/>
    
      <category term="DL" scheme="https://2020.iosdevlog.com/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Concepts</title>
    <link href="https://2020.iosdevlog.com/2020/02/22/ds/"/>
    <id>https://2020.iosdevlog.com/2020/02/22/ds/</id>
    <published>2020-02-22T11:10:39.000Z</published>
    <updated>2020-02-22T11:19:03.548Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/22/ds/Process.png" alt="" /><figcaption>Process</figcaption></figure><a id="more"></a><h2 id="types">Types</h2><h3 id="regression">Regression</h3><ul><li>A supervised problem, the outputs are continuous rather than discrete.</li></ul><h3 id="classification">Classification</h3><ul><li>Inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one or more (multi-label classification) of these classes. This is typically tackled in a supervised way.</li></ul><h3 id="clustering">Clustering</h3><ul><li>A set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.</li></ul><h3 id="density-estimation">Density Estimation</h3><ul><li>Finds the distribution of inputs in some space.</li></ul><h3 id="dimensionality-reduction">Dimensionality Reduction</h3><ul><li>Simplifies inputs by mapping them into a lower-dimensional space.</li></ul><h2 id="kind">Kind</h2><h3 id="parametric">Parametric</h3><ul><li><p>Step 1: Making an assumption about the functional form or shape of our function (f), i.e.: f is linear, thus we will select a linear model.</p></li><li><p>Step 2: Selecting a procedure to fit or train our model. This means estimating the Beta parameters in the linear function. A common approach is the (ordinary) least squares, amongst others.</p></li></ul><h3 id="non-parametric">Non-Parametric</h3><ul><li>When we do not make assumptions about the form of our function (f). However, since these methods do not reduce the problem of estimating f to a small number of parameters, a large number of observations is required in order to obtain an accurate estimate for f. An example would be the thin-plate spline model.</li></ul><h2 id="categories">Categories</h2><h3 id="supervised">Supervised</h3><ul><li>The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs.</li></ul><h3 id="unsupervised">Unsupervised</h3><ul><li>No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).</li></ul><h3 id="reinforcement-learning">Reinforcement Learning</h3><ul><li>A computer program interacts with a dynamic environment in which it must perform a certain goal (such as <a href="https://en.wikipedia.org/wiki/Autonomous_car" target="_blank" rel="noopener">driving a vehicle</a> or playing a game against an opponent). The program is provided feedback in terms of rewards and punishments as it navigates its problem space.</li></ul><h2 id="approaches">Approaches</h2><h3 id="decision-tree-learning">Decision tree learning</h3><h3 id="association-rule-learning">Association rule learning</h3><h3 id="artificial-neural-networks">Artificial neural networks</h3><h3 id="deep-learning">Deep learning</h3><h3 id="inductive-logic-programming">Inductive logic programming</h3><h3 id="support-vector-machines">Support vector machines</h3><h3 id="clustering-1">Clustering</h3><h3 id="bayesian-networks">Bayesian networks</h3><h3 id="reinforcement-learning-1">Reinforcement learning</h3><h3 id="representation-learning">Representation learning</h3><h3 id="similarity-and-metric-learning">Similarity and metric learning</h3><h3 id="sparse-dictionary-learning">Sparse dictionary learning</h3><h3 id="genetic-algorithms">Genetic algorithms</h3><h3 id="rule-based-machine-learning">Rule-based machine learning</h3><h3 id="learning-classifier-systems">Learning classifier systems</h3><h2 id="taxonomy">Taxonomy</h2><h3 id="generative-methods">Generative Methods</h3><ul><li><p>Popular models</p><ul><li><p>Mixtures of Gaussians, Mixtures of experts, Hidden Markov Models (HMM)</p></li><li><p>Gaussians, Naïve Bayes, Mixtures of multinomials</p></li><li><p>Sigmoidal belief networks, Bayesian networks, Markov random fields</p></li></ul></li><li><p>Model class-conditional pdfs and prior probabilities. “Generative” since sampling can generate synthetic data points.</p></li></ul><h3 id="discriminative-methods">Discriminative Methods</h3><ul><li><p>Directly estimate posterior probabilities. No attempt to model underlying probability distributions. Focus computational resources on given task– better performance</p></li><li><p>Popular Models</p><ul><li><p>Logistic regression, SVMs</p></li><li><p>Traditional neural networks, Nearest neighbor</p></li><li><p>Conditional Random Fields (CRF)</p></li></ul></li></ul><h2 id="selection-criteria">Selection Criteria</h2><h3 id="prediction-accuracy-vs-model-interpretability">Prediction Accuracy vs Model Interpretability</h3><ul><li>There is an inherent tradeoff between Prediction Accuracy and Model Interpretability, that is to say that as the model get more flexible in the way the function (f) is selected, they get obscured, and are hard to interpret. Flexible methods are better for inference, and inflexible methods are preferable for prediction.</li></ul><h2 id="libraries">Libraries</h2><h3 id="python">Python</h3><ul><li><p>Numpy</p><ul><li>Adds support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays</li></ul></li><li><p>Pandas</p><ul><li>Offers data structures and operations for manipulating numerical tables and time series</li></ul></li><li><p>Scikit-Learn</p><ul><li>It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.</li></ul></li><li><p>Tensorflow</p><ul><li><p>Components<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/708342AF-41CC-4702-B41B-08DE83166234.png" /></p><ul><li><p>Does lazy evaluation. Need to build the graph, and then run it in a session.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/B19BDBEE-22D5-4A3E-8861-4790CDDE01E0.png" /></p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/FC3EF1F8-AC76-4D03-9E45-036D76C4E216.png" /></p></li><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/AA360355-4A67-41ED-8475-81391BD62DB3.png" /></p></li></ul></li></ul></li></ul></li><li><p>MXNet</p><ul><li>Is an modern open-source deep learning framework used to train, and deploy deep neural networks. MXNet library is portable and can scale to multiple GPUs and multiple machines. MXNet is supported by major Public Cloud providers including AWS and Azure. Amazon has chosen MXNet as its deep learning framework of choice at AWS.</li></ul></li><li><p>Keras</p><ul><li>Is an open source neural network library written in Python. It is capable of running on top of MXNet, Deeplearning4j, Tensorflow, CNTK or Theano. Designed to enable fast experimentation with deep neural networks, it focuses on being minimal, modular and extensible.</li></ul></li><li><p>Torch</p><ul><li>Torch is an open source machine learning library, a scientific computing framework, and a script language based on the Lua programming language. It provides a wide range of algorithms for deep machine learning, and uses the scripting language LuaJIT, and an underlying C implementation.</li></ul></li><li><p>Microsoft Cognitive Toolkit</p><ul><li>Previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.</li></ul></li></ul><h2 id="tuning">Tuning</h2><h3 id="cross-validation">Cross-validation</h3><ul><li><p>One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/8EAF745C-496E-45EA-B94F-FEC16E431FAD.png" /></li></ul></li><li><p>Methods</p><ul><li><p>Leave-p-out cross-validation</p></li><li><p>Leave-one-out cross-validation</p></li><li><p>k-fold cross-validation</p></li><li><p>Holdout method</p></li><li><p>Repeated random sub-sampling validation</p></li></ul></li></ul><h3 id="hyperparameters">Hyperparameters</h3><ul><li><p>Grid Search</p><ul><li>The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.</li></ul></li><li><p>Random Search</p><ul><li>Since grid searching is an exhaustive and therefore potentially expensive method, several alternatives have been proposed. In particular, a randomized search that simply samples parameter settings a fixed number of times has been found to be more effective in high-dimensional spaces than exhaustive search.</li></ul></li><li><p>Gradient-based optimization</p><ul><li>For specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks. Since then, these methods have been extended to other models such as support vector machines or logistic regression.</li></ul></li></ul><h3 id="early-stopping-regularization">Early Stopping (Regularization)</h3><ul><li>Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit, and stop the algorithm then.</li></ul><h3 id="overfitting">Overfitting</h3><ul><li>When a given method yields a small training MSE (or cost), but a large test MSE (or cost), we are said to be overfitting the data. This happens because our statistical learning procedure is trying too hard to find pattens in the data, that might be due to random chance, rather than a property of our function. In other words, the algorithms may be learning the training data too well. If model overfits, try removing some features, decreasing degrees of freedom, or adding more data.</li></ul><h3 id="underfitting">Underfitting</h3><ul><li>Opposite of Overfitting. Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. It occurs when the model or algorithm does not fit the data enough. Underfitting occurs if the model or algorithm shows low variance but high bias (to contrast the opposite, overfitting from high variance and low bias). It is often a result of an excessively simple model.</li></ul><h3 id="bootstrap">Bootstrap</h3><ul><li>Test that applies Random Sampling with Replacement of the available data, and assigns measures of accuracy (bias, variance, etc.) to sample estimates.</li></ul><h3 id="bagging">Bagging</h3><ul><li>An approach to ensemble learning that is based on bootstrapping. Shortly, given a training set, we produce multiple different training sets (called bootstrap samples), by sampling with replacement from the original dataset. Then, for each bootstrap sample, we build a model. The results in an ensemble of models, where each model votes with the equal weight. Typically, the goal of this procedure is to reduce the variance of the model of interest (e.g. decision trees).</li></ul><h2 id="performance-analysis">Performance Analysis</h2><h3 id="confusion-matrix">Confusion Matrix</h3><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/1F3239B7-A891-4326-831C-0F01A7ACFA00.png" /></li></ul><h3 id="accuracy">Accuracy</h3><ul><li>Fraction of correct predictions, not reliable as skewed when the data set is unbalanced (that is, when the number of samples in different classes vary greatly)</li></ul><h3 id="f1-score">f1 score</h3><ul><li><p>Precision</p><ul><li>Out of all the examples the classifier labeled as positive, what fraction were correct?<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/7B2111F9-6BF0-43B2-B130-C24CAAC39365.png" /></li></ul></li><li><p>Recall</p><ul><li>Out of all the positive examples there were, what fraction did the classifier pick up?<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/77C64150-2B24-41B5-8F70-AD154A20EC66.png" /></li></ul></li><li><p>Harmonic Mean of Precision and Recall: (2 * p * r / (p + r))</p></li></ul><h3 id="roc-curve---receiver-operating-characteristics">ROC Curve - Receiver Operating Characteristics</h3><ul><li>True Positive Rate (Recall / Sensitivity) vs False Positive Rate (1-Specificity)<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/33911326-3EBD-4DF9-9875-B75F287E30D0.png" /></li></ul><h3 id="bias-variance-tradeoff">Bias-Variance Tradeoff</h3><ul><li><p>Bias refers to the amount of error that is introduced by approximating a real-life problem, which may be extremely complicated, by a simple model. If Bias is high, and/or if the algorithm performs poorly even on your training data, try adding more features, or a more flexible model.</p></li><li><p>Variance is the amount our model’s prediction would change when using a different training data set. High: Remove features, or obtain more data.</p></li></ul><h3 id="goodness-of-fit-r2">Goodness of Fit = R^2</h3><ul><li>1.0 - sum_of_squared_errors / total_sum_of_squares(y)</li></ul><h3 id="mean-squared-error-mse">Mean Squared Error (MSE)</h3><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/06091448-C605-4DEA-9650-D71A77C710C8.png" /></p><ul><li>The mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors or deviations—that is, the difference between the estimator and what is estimated.</li></ul></li></ul><h3 id="error-rate">Error Rate</h3><ul><li><p>The proportion of mistakes made if we apply out estimate model function the the training observations in a classification setting.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/6D6D7323-715D-42F8-9B71-39BE839F2C4B.png" /></li></ul></li></ul><h2 id="motivation">Motivation</h2><h3 id="prediction">Prediction</h3><ul><li>When we are interested mainly in the predicted variable as a result of the inputs, but not on the each way of the inputs affect the prediction. In a real estate example, Prediction would answer the question of: Is my house over or under valued? Non-linear models are very good at these sort of predictions, but not great for inference because the models are much less interpretable.</li></ul><h3 id="inference">Inference</h3><ul><li>When we are interested in the way each one of the inputs affect the prediction. In a real estate example, Inference would answer the question of: How much would my house cost if it had a view of the sea? Linear models are more suited for inference because the models themselves are easier to understand than their non-linear counterparts.</li></ul><h1 id="machine-learning-process">Machine Learning Process</h1><h2 id="data">Data</h2><h3 id="find">Find</h3><h3 id="collect">Collect</h3><h3 id="explore">Explore</h3><h3 id="clean-features">Clean Features</h3><h3 id="impute-features">Impute Features</h3><h3 id="engineer-features">Engineer Features</h3><h3 id="select-features">Select Features</h3><h3 id="encode-features">Encode Features</h3><h3 id="build-datasets">Build Datasets</h3><ul><li>Machine Learning is math. In specific, performing Linear Algebra on Matrices. Our data values must be numeric.</li></ul><h2 id="model">Model</h2><h3 id="select-algorithm-based-on-question-and-data-available">Select Algorithm based on question and data available</h3><h2 id="cost-function">Cost Function</h2><h3 id="the-cost-function-will-provide-a-measure-of-how-far-my-algorithm-and-its-parameters-are-from-accurately-representing-my-training-data.">The cost function will provide a measure of how far my algorithm and its parameters are from accurately representing my training data.</h3><h3 id="sometimes-referred-to-as-cost-or-loss-function-when-the-goal-is-to-minimise-it-or-objective-function-when-the-goal-is-to-maximise-it.">Sometimes referred to as Cost or Loss function when the goal is to minimise it, or Objective function when the goal is to maximise it.</h3><h2 id="optimization">Optimization</h2><h3 id="having-selected-a-cost-function-we-need-a-method-to-minimise-the-cost-function-or-maximise-the-objective-function.-typically-this-is-done-by-gradient-descent-or-stochastic-gradient-descent.">Having selected a cost function, we need a method to minimise the Cost function, or maximise the Objective function. Typically this is done by Gradient Descent or Stochastic Gradient Descent.</h3><h2 id="tuning-1">Tuning</h2><h3 id="different-algorithms-have-different-hyperparameters-which-will-affect-the-algorithms-performance.-there-are-multiple-methods-for-hyperparameter-tuning-such-as-grid-and-random-search.">Different Algorithms have different Hyperparameters, which will affect the algorithms performance. There are multiple methods for Hyperparameter Tuning, such as Grid and Random search.</h3><h2 id="results-and-benchmarking">Results and Benchmarking</h2><h3 id="analyse-the-performance-of-each-algorithms-and-discuss-results.">Analyse the performance of each algorithms and discuss results.</h3><h3 id="are-the-results-good-enough-for-production">Are the results good enough for production?</h3><h3 id="is-the-ml-algorithm-training-and-inference-completing-in-a-reasonable-timeframe">Is the ML algorithm training and inference completing in a reasonable timeframe?</h3><h2 id="scaling">Scaling</h2><h3 id="how-does-my-algorithm-scale-for-both-training-and-inference">How does my algorithm scale for both training and inference?</h3><h2 id="deployment-and-operationalisation">Deployment and Operationalisation</h2><h3 id="how-can-feature-manipulation-be-done-for-training-and-inference-in-real-time">How can feature manipulation be done for training and inference in real-time?</h3><h3 id="how-to-make-sure-that-the-algorithm-is-retrained-periodically-and-deployed-into-production">How to make sure that the algorithm is retrained periodically and deployed into production?</h3><h3 id="how-will-the-ml-algorithms-be-integrated-with-other-systems">How will the ML algorithms be integrated with other systems?</h3><h2 id="infrastructure">Infrastructure</h2><h3 id="can-the-infrastructure-running-the-machine-learning-process-scale">Can the infrastructure running the machine learning process scale?</h3><h3 id="how-is-access-to-the-ml-algorithm-provided-rest-api-sdk">How is access to the ML algorithm provided? REST API? SDK?</h3><h3 id="is-the-infrastructure-appropriate-for-the-algorithm-we-are-running-cpus-or-gpus">Is the infrastructure appropriate for the algorithm we are running? CPU's or GPU's?</h3><h2 id="direction">Direction</h2><h3 id="saas---pre-built-machine-learning-models">SaaS - Pre-built Machine Learning models</h3><ul><li><p>Google Cloud</p><ul><li><p>Vision API</p></li><li><p>Speech API</p></li><li><p>Jobs API</p></li><li><p>Video Intelligence API</p></li><li><p>Language API</p></li><li><p>Translation API</p></li></ul></li><li><p>AWS</p><ul><li><p>Rekognition</p></li><li><p>Lex</p></li><li><p>Polly</p></li></ul></li><li><p>… many others</p></li></ul><h3 id="data-science-and-applied-machine-learning">Data Science and Applied Machine Learning</h3><ul><li><p>Google Cloud</p><ul><li>ML Engine</li></ul></li><li><p>AWS</p><ul><li>Amazon Machine Learning</li></ul></li><li><p>Tools: Jupiter / Datalab / Zeppelin</p></li><li><p>… many others</p></li></ul><h3 id="machine-learning-research">Machine Learning Research</h3><ul><li><p>Tensorflow</p></li><li><p>MXNet</p></li><li><p>Torch</p></li><li><p>… many others</p></li></ul><h2 id="question">Question</h2><h3 id="is-this-a-or-b">Is this A or B?</h3><ul><li>Classification</li></ul><h3 id="how-much-or-how-many-of-these">How much, or how many of these?</h3><ul><li>Regression</li></ul><h3 id="is-this-anomalous">Is this anomalous?</h3><ul><li>Anomaly Detection</li></ul><h3 id="how-can-these-elements-be-grouped">How can these elements be grouped?</h3><ul><li>Clustering</li></ul><h3 id="what-should-i-do-now">What should I do now?</h3><ul><li>Reinforcement Learning</li></ul><h1 id="machine-learning-mathematics">Machine Learning Mathematics</h1><h2 id="costlossmin-objectivemax-functions">Cost/Loss(Min) Objective(Max) Functions</h2><h3 id="intuition">Intuition</h3><ul><li><p>The cost function will tell us how right the predictions of our model and weight matrix are, and the choice of cost function will drive how much we care about how wrong each prediction is. For instance, a hinge loss will assume the difference between each incorrect prediction value is linear. Were we to square the hinge loss and use that as our cost function, we would be telling the system that being very wrong gets exponentially worse as we get away from the right prediction. Cross Entropy would offer a probabilistic approach.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/AAAF1D3A-8184-4843-A1E3-AC53E43315FC.png" /></li></ul></li></ul><h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3><ul><li><p>Many cost functions are the result of applying Maximum Likelihood. For instance, the Least Squares cost function can be obtained via Maximum Likelihood. Cross-Entropy is another example.</p></li><li><p>The likelihood of a parameter value (or vector of parameter values), θ, given outcomes x, is equal to the probability (density) assumed for those observed outcomes given those parameter values, that is</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/A37BBE5F-E77F-411E-BA91-5A2D475063D0.png" /></li></ul></li><li><p>The natural logarithm of the likelihood function, called the log-likelihood, is more convenient to work with. Because the logarithm is a monotonically increasing function, the logarithm of a function achieves its maximum value at the same points as the function itself, and hence the log-likelihood can be used in place of the likelihood in maximum likelihood estimation and related techniques.</p></li><li><p>In general, for a fixed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the <a href="https://en.wikipedia.org/wiki/Likelihood_function" target="_blank" rel="noopener">likelihood function</a>. Intuitively, this maximizes the "agreement" of the selected model with the observed data, and for discrete random variables it indeed maximizes the probability of the observed data under the resulting distribution. Maximum-likelihood estimation gives a unified approach to estimation, which is <a href="https://en.wikipedia.org/wiki/Well_defined" target="_blank" rel="noopener">well-defined</a> in the case of the <a href="https://en.wikipedia.org/wiki/Normal_distribution" target="_blank" rel="noopener">normal distribution</a> and many other problems.</p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/396DE3E5-85C2-487D-9324-FB83FCC7F8FD.png" /></p></li><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/690903C2-BC9F-400F-877E-7B625F2A20BD.png" /></p></li><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/B8AA9D14-192E-4C93-A166-A429A293990C.png" /></p></li><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/2EE71F38-3B82-48E7-A8B8-44FC8B2A5805.png" /></p></li><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/B7F5DF41-7571-412D-81AD-4B683CC1812B.png" /></p></li></ul></li></ul><h3 id="cross-entropy">Cross-Entropy</h3><ul><li><p>Cross entropy can be used to define the loss function in machine learning and optimization. The true probability pi is the true label, and the given distribution qi is the predicted value of the current model.</p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/9E271C6A-6FBB-41FC-960B-4AA394E6EEA5.png" /></p></li><li><p>Cross-entropy error function and logistic regression<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/0397888B-C44B-4178-B1D5-F7FB2ED1F3EC.png" /></p></li></ul></li></ul><h3 id="logistic">Logistic</h3><ul><li><p>The logistic loss function is defined as:</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/8942D559-DDAA-4EE2-AD81-3D6DE9C1540C.png" /></li></ul></li></ul><h3 id="quadratic">Quadratic</h3><ul><li><p>The use of a quadratic loss function is common, for example when using least squares techniques. It is often more mathematically tractable than other loss functions because of the properties of variances, as well as being symmetric: an error above the target causes the same loss as the same magnitude of error below the target. If the target is t, then a quadratic loss function is:</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C2D73C61-1BAC-4146-B59C-2C6D75CAFCF6.png" /></li></ul></li></ul><h3 id="loss">0-1 Loss</h3><ul><li><ul><li><p>In <a href="https://en.wikipedia.org/wiki/Statistics" target="_blank" rel="noopener">statistics</a> and <a href="https://en.wikipedia.org/wiki/Decision_theory" target="_blank" rel="noopener">decision theory</a>, a frequently used loss function is the 0-1 loss function</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/4597CE60-7571-403E-98C5-B3087805A418.png" /></li></ul></li></ul></li></ul><h3 id="hinge-loss">Hinge Loss</h3><ul><li><p>The hinge loss is a loss function used for training classifiers. For an intended output t = ±1 and a classifier score y, the hinge loss of the prediction y is defined as:</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B96B2BB2-CF51-4989-8BE7-1A825082D2D6.png" /></li></ul></li></ul><h3 id="exponential">Exponential</h3><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/329F57B1-378D-47B7-B536-0AD0F626A708.png" /></li></ul><h3 id="hellinger-distance">Hellinger Distance</h3><ul><li><p>It is used to quantify the similarity between two probability distributions. It is a type of f-divergence.</p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/A9DC5C23-CACF-429F-83B2-F432DA3B3F1A.png" /></p></li><li><p>To define the Hellinger distance in terms of <a href="https://en.wikipedia.org/wiki/Measure_theory" target="_blank" rel="noopener">measure theory</a>, let P and Q denote two <a href="https://en.wikipedia.org/wiki/Probability_measure" target="_blank" rel="noopener">probability measures</a> that are <a href="https://en.wikipedia.org/wiki/Absolute_continuity" target="_blank" rel="noopener">absolutely continuous</a> with respect to a third probability measure λ. The square of the Hellinger distance between P and Q is defined as the quantity</p></li></ul></li></ul><h3 id="kullback-leibler-divengence">Kullback-Leibler Divengence</h3><ul><li><p>Is a measure of how one probability distribution diverges from a second expected probability distribution. Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference.</p><ul><li><p>Discrete<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/FC311DB8-C4BD-4D96-A9B0-786CF4091DE7.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/19746786-7A83-4A0A-B8C2-38F7E7189DFE.png" /></li></ul></li></ul></li></ul><h3 id="itakurasaito-distance">Itakura–Saito distance</h3><ul><li><p>is a measure of the difference between an original spectrum P(ω) and an approximation<br />P^(ω) of that spectrum. Although it is not a perceptual measure, it is intended to reflect perceptual (dis)similarity.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/55037B97-44EC-4173-9D7E-2755F6648E9C.png" /></li></ul></li></ul><h3 id="httpsstats.stackexchange.comquestions154879a-list-of-cost-functions-used-in-neural-networks-alongside-applications">https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications</h3><h3 id="httpsen.wikipedia.orgwikiloss_functions_for_classification">https://en.wikipedia.org/wiki/Loss_functions_for_classification</h3><h2 id="probability">Probability</h2><h3 id="concepts">Concepts</h3><ul><li><p>Frequentist vs Bayesian Probability</p><ul><li><p>Frequentist</p><ul><li>Basic notion of probability: # Results / # Attempts</li></ul></li><li><p>Bayesian</p><ul><li>The probability is not a number, but a distribution itself.</li></ul></li><li><p>http://www.behind-the-enemy-lines.com/2008/01/are-you-bayesian-or-frequentist-or.html</p></li></ul></li><li><p>Random Variable</p><ul><li><p>In <a href="https://en.wikipedia.org/wiki/Probability_and_statistics" target="_blank" rel="noopener">probability and statistics</a>, a random variable, random quantity, aleatory variable or stochastic variable is a <a href="https://en.wikipedia.org/wiki/Variable_(mathematics)" target="_blank" rel="noopener">variable</a> whose value is subject to variations due to chance (i.e. <a href="https://en.wikipedia.org/wiki/Randomness" target="_blank" rel="noopener">randomness</a>, in a mathematical sense). A random variable can take on a set of possible different values (similarly to other mathematical variables), each with an associated probability, in contrast to other mathematical variables.</p><ul><li><p>Expectation (Expected Value) of a Random Variable<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/2956137E-E540-471C-A34B-66BBE0507483.png" /></p><ul><li>Same, for continuous variables<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/54388388-46D3-4E97-A228-23669D4E1E88.png" /></li></ul></li></ul></li></ul></li><li><p>Independence</p><ul><li><p>Two <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)" target="_blank" rel="noopener">events</a> are independent, statistically independent, or stochastically independent if the occurrence of one does not affect the probability of the other.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/FE866A04-B684-4395-92EB-E73593004643.png" /></li></ul></li></ul></li><li><p>Conditionality</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/E07D7B09-0747-48C9-ADC6-87AA26B8D3BF.png" /></li></ul></li><li><p>Bayes Theorem (rule, law)</p><ul><li><p>Simple Form<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/870B8130-69C4-4DBD-9BCF-D8D96D3C7D77.png" /></p><ul><li>With Law of Total probability<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/E90A57EE-85B2-4601-BF52-1F71E557A13F.png" /></li></ul></li></ul></li><li><p>Marginalisation</p><ul><li><p>The marginal distribution of a <a href="https://en.wikipedia.org/wiki/Subset" target="_blank" rel="noopener">subset</a> of a collection of <a href="https://en.wikipedia.org/wiki/Random_variable" target="_blank" rel="noopener">random variables</a> is the <a href="https://en.wikipedia.org/wiki/Probability_distribution" target="_blank" rel="noopener">probability distribution</a> of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables.</p><ul><li><p>Continuous<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/4D1F3F76-3341-47A5-96C1-9097E5C87A38.png" /></p><ul><li><p>Discrete<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/860CED1C-EA17-44D3-A99E-B696A87A92BF.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/7E9DE18D-616C-45BB-A0C2-02C7E469F034.png" /></li></ul></li></ul></li></ul></li></ul></li><li><p>Law of Total Probability</p><ul><li><p>Is a fundamental rule relating <a href="https://en.wikipedia.org/wiki/Marginal_probability" target="_blank" rel="noopener">marginal probabilities</a> to <a href="https://en.wikipedia.org/wiki/Conditional_probabilities" target="_blank" rel="noopener">conditional probabilities</a>. It expresses the total probability of an outcome which can be realized via several distinct events - hence the name.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5F52537E-8AF1-42FA-8CB4-2B2549F3FDA7.png" /></li></ul></li></ul></li><li><p>Chain Rule</p><ul><li>Permits the calculation of any member of the <a href="https://en.wikipedia.org/wiki/Joint_distribution" target="_blank" rel="noopener">joint distribution</a> of a set of <a href="https://en.wikipedia.org/wiki/Random_variables" target="_blank" rel="noopener">random variables</a> using only <a href="https://en.wikipedia.org/wiki/Conditional_probabilities" target="_blank" rel="noopener">conditional probabilities</a>.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/4739C552-28D0-4FAB-80B1-10DC7BBE1014.png" /></li></ul></li><li><p>Bayesian Inference</p><ul><li>Bayesian inference derives the <a href="https://en.m.wikipedia.org/wiki/Posterior_probability" target="_blank" rel="noopener">posterior probability</a> as a <a href="https://en.m.wikipedia.org/wiki/Consequence_relation" target="_blank" rel="noopener">consequence</a> of two <a href="https://en.m.wikipedia.org/wiki/Antecedent_(logic)" target="_blank" rel="noopener">antecedents</a>, a <a href="https://en.m.wikipedia.org/wiki/Prior_probability" target="_blank" rel="noopener">prior probability</a> and a "<a href="https://en.m.wikipedia.org/wiki/Likelihood_function" target="_blank" rel="noopener">likelihood function</a>" derived from a <a href="https://en.m.wikipedia.org/wiki/Statistical_model" target="_blank" rel="noopener">statistical model</a> for the observed data. Bayesian inference computes the posterior probability according to <a href="https://en.m.wikipedia.org/wiki/Bayes%27_theorem" target="_blank" rel="noopener">Bayes' theorem</a>. It can be applied iteratively so to update the confidence on out hypothesis.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/5EFA0B04-EB55-4414-8DFF-69C42072527A.png" /></li></ul></li></ul><h2 id="distributions">Distributions</h2><h3 id="definition">Definition</h3><ul><li>Is a table or an equation that links each outcome of a statistical experiment with the probability of occurence. When Continuous, is is described by the Probability Density Function</li></ul><h3 id="types-density-function">Types (Density Function)</h3><ul><li><p>Normal (Gaussian)<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/6BAD212C-E812-4D94-887E-B8FC28594153.png" /></p><ul><li><p>Poisson<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/B3457C07-7876-49F4-9FDA-5516DECF2E65.png" /></p><ul><li>Uniform<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/CFF452C6-1F11-465E-BDE8-EA3B3D55251D.png" /></li></ul></li></ul></li><li><p>Bernoulli<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/8F02DF2E-0723-4EDA-83E1-04DEBB0A222F.png" /></p><ul><li><p>Gamma<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/F2C3B775-C53A-4AA9-A398-120FBDFF6EF3.png" /></p><ul><li>Binomial<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/FF9BC15A-3117-4FA4-B59D-9EBD6E46F3A6.png" /></li></ul></li></ul></li></ul><h3 id="cumulative-distribution-function-cdf">Cumulative Distribution Function (CDF)</h3><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D6AD4AEC-161F-4A81-A996-50C936E3752B.png" /></li></ul><h2 id="information-theory">Information Theory</h2><h3 id="entropy">Entropy</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/E0EC20FC-0906-4B7C-A75F-27338A35CB48.png" /></p><ul><li><p>Entropy is a measure of unpredictability of information content.</p><ul><li>To evaluate a language model, we should measure how much surprise it gives us for real sequences in that language. For each real word encountered, the language model will give a probability p. And we use -log(p) to quantify the surprise. And we average the total surprise over a long enough sequence. So, in case of a 1000-letter sequence with 500 A and 500 B, the surprise given by the 1/3-2/3 model will be:<br />[-500<em>log(1/3) - 500</em>log(2/3)]/1000 = 1/2 * Log(9/2)<br />While the correct 1/2-1/2 model will give:<br />[-500<em>log(1/2) - 500</em>log(1/2)]/1000 = 1/2 * Log(8/2)<br />So, we can see, the 1/3, 2/3 model gives more surprise, which indicates it is worse than the correct model.<br />Only when the sequence is long enough, the average effect will mimic the expectation over the 1/2-1/2 distribution. If the sequence is short, it won't give a convincing result.</li></ul></li></ul><h3 id="cross-entropy-1">Cross Entropy</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/BCFC979D-F514-45F2-8543-8AFC4539CD08.png" /></p><ul><li>Cross entropy between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an "unnatural" probability distribution q, rather than the "true" distribution p.</li></ul><h3 id="joint-entropy">Joint Entropy</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/250ECB8A-F48D-4814-B542-2C3FC45B2127.png" /></p><h3 id="conditional-entropy">Conditional Entropy</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/97F11D13-2F43-499D-9E53-5CE038479F74.png" /></p><h3 id="mutual-information">Mutual Information</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/B0FFBB3D-70E6-4C13-AFEC-4E1E4F133926.png" /></p><h3 id="kullback-leibler-divergence">Kullback-Leibler Divergence</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F669FC47-9F7C-45BB-9311-148E23624F68.png" /></p><h2 id="density-estimation-1">Density Estimation</h2><h3 id="mostly-non-parametric.-parametric-makes-assumptions-on-my-datarandom-variables-for-instance-that-they-are-normally-distributed.-non-parametric-does-not.">Mostly Non-Parametric. Parametric makes assumptions on my data/random-variables, for instance, that they are normally distributed. Non-parametric does not.</h3><h3 id="the-methods-are-generally-intended-for-description-rather-than-formal-inference">The methods are generally intended for description rather than formal inference</h3><h3 id="methods">Methods</h3><ul><li><p>Kernel Density Estimation<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/7886A650-E942-414E-B9C1-FF657BFA8738.png" /></p><ul><li><p>non-negative</p></li><li><p>it’s a type of PDF that it is symmetric</p></li><li><p>real-valued</p></li><li><p>symmetric</p></li><li><p>integral over function is equal to 1</p></li><li><p>non-parametric</p></li><li><p>calculates kernel distributions for every sample point, and then adds all the distributions</p></li><li><p>Uniform, Triangle, Quartic, Triweight, Gaussian, Cosine, others...</p></li></ul></li><li><p>Cubic Spline</p><ul><li>A cubic spline is a function created from cubic polynomials on each between-knot interval by pasting them together twice continuously differentiable at the knots.</li></ul></li></ul><h2 id="regularization">Regularization</h2><h3 id="l1-norm">L1 norm</h3><ul><li><p>Manhattan Distance</p><ul><li><p>L1-norm is also known as least absolute deviations (LAD), least absolute errors (LAE). It is basically minimizing the sum of the absolute differences (S) between the target value and the estimated values.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C8581E8F-5C6B-424E-839D-9A01C3BEAE53.png" /></li></ul></li><li><p>Intuitively, the L1 norm prefers a weight matrix which contains the larger number of zeros.</p></li></ul></li></ul><h3 id="l2-norm">L2 norm</h3><ul><li><p>Euclidean Distance</p><ul><li><p>L2-norm is also known as least squares. It is basically minimizing the sum of the square of the differences (S) between the target value and the estimated values:</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/7839524D-1CE8-4634-A5D3-8B493ADBF0CE.png" /></li></ul></li><li><p>Intuitively, the L2 norm prefers a weight matrix where the norm is distributed across all weight matrix entries.</p></li></ul></li></ul><h3 id="early-stopping">Early Stopping</h3><ul><li>Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit, and stop the algorithm then.</li></ul><h3 id="dropout">Dropout</h3><ul><li>Is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term "dropout" refers to dropping out units (both hidden and visible) in a neural network</li></ul><h3 id="sparse-regularizer-on-columns">Sparse regularizer on columns</h3><ul><li><p>This regularizer defines an L2 norm on each column and an L1 norm over all columns. It can be solved by proximal methods.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/DF7A3A69-3945-4222-86DA-EF91B8FF6740.png" /></li></ul></li></ul><h3 id="nuclear-norm-regularization">Nuclear norm regularization</h3><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5D348DC1-B376-4FCA-9FA5-F2C2E4697D84.png" /></li></ul><h3 id="mean-constrained-regularization">Mean-constrained regularization</h3><ul><li><p>This regularizer constrains the functions learned for each task to be similar to the overall average of the functions across all tasks. This is useful for expressing prior information that each task is expected to share similarities with each other task. An example is predicting blood iron levels measured at different times of the day, where each task represents a different person.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/AED450B9-7864-4B51-8554-0B9A8CBB92BE.png" /></li></ul></li></ul><h3 id="clustered-mean-constrained-regularization">Clustered mean-constrained regularization</h3><ul><li><p>This regularizer is similar to the mean-constrained regularizer, but instead enforces similarity between tasks within the same cluster. This can capture more complex prior information. This technique has been used to predict Netflix recommendations.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/4427EABD-9457-44F5-AE53-47D654CA898C.png" /></li></ul></li></ul><h3 id="graph-based-similarity">Graph-based similarity</h3><ul><li><p>More general than above, similarity between tasks can be defined by a function. The regularizer encourages the model to learn similar functions for similar tasks.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B26E5AA5-905F-49BB-A492-F0D34378ACC2.png" /></li></ul></li></ul><h2 id="optimization-1">Optimization</h2><h3 id="gradient-descent">Gradient Descent</h3><ul><li>Is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.</li></ul><h3 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h3><ul><li><p>Gradient descent uses total gradient over all examples per update, SGD updates after only 1 or few examples:</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/187B983B-A9BE-4B1C-8ACF-ACB40B0E5725.png" /></li></ul></li></ul><h3 id="mini-batch-stochastic-gradient-descent-sgd">Mini-batch Stochastic Gradient Descent (SGD)</h3><ul><li>Gradient descent uses total gradient over all examples per update, SGD updates after only 1 example</li></ul><h3 id="momentum">Momentum</h3><ul><li>Idea: Add a fraction v of previous update to current one. When the gradient keeps pointing in the same direction, this will<br />increase the size of the steps taken towards the minimum.</li></ul><h3 id="adagrad">Adagrad</h3><ul><li>Adaptive learning rates for each parameter</li></ul><h2 id="statistics">Statistics</h2><h3 id="measures-of-central-tendency">Measures of Central Tendency</h3><ul><li><p>Mean</p></li><li><p>Median</p><ul><li>Value in the middle or an ordered list, or average of two in middle.</li></ul></li><li><p>Mode</p><ul><li>Most Frequent Value</li></ul></li><li><p>Quantile</p><ul><li>Division of probability distributions based on contiguous intervals with equal probabilities. In short: Dividing observations numbers in a sample list equally.</li></ul></li></ul><h3 id="dispersion">Dispersion</h3><ul><li><p>Range</p></li><li><p>Medium Absolute Deviation (MAD)</p><ul><li>The average of the absolute value of the deviation of each value from the mean</li></ul></li><li><p>Inter-quartile Range (IQR)</p><ul><li>Three quartiles divide the data in approximately four equally divided parts</li></ul></li><li><p>Variance</p><ul><li><p>Definition</p><ul><li>The average of the squared differences from the Mean. Formally, is the expectation of the squared deviation of a random variable from its mean, and it informally measures how far a set of (random) numbers are spread out from their mean.</li></ul></li><li><p>Types</p><ul><li><p>Continuous<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/19DB13EA-7024-46CD-9404-64E8EA2850A4.png" /></p><ul><li>Discrete<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/C416473F-2CEC-4F12-8E9D-B0D7D1C686A6.png" /></li></ul></li></ul></li></ul></li><li><p>Standard Deviation</p><ul><li><p>sqrt(variance)<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/15CD2CF7-B982-4C9C-B69F-33D20657E7A2.png" /></p><ul><li><p>z-score/value/factor</p><ul><li>The signed number of <a href="https://en.wikipedia.org/wiki/Standard_deviation" target="_blank" rel="noopener">standard deviations</a> an observation or <a href="https://en.wikipedia.org/wiki/Data" target="_blank" rel="noopener">datum</a> is above the <a href="https://en.wikipedia.org/wiki/Mean" target="_blank" rel="noopener">mean</a>.</li></ul></li></ul></li></ul></li></ul><h3 id="relationship">Relationship</h3><ul><li><p>Covariance</p><ul><li><p>dot(de_mean(x), de_mean(y)) / (n - 1)<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/0C8FD4CE-AC05-4975-ABDD-B54BD11F8312.png" /></p><ul><li>A measure of how much two random variables change together. http://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean</li></ul></li></ul></li><li><p>Correlation</p><ul><li><p>Pearson</p><ul><li>Benchmarks linear relationship, most appropriate for measurements taken from an interval scale, is a measure of the linear dependence between two variables<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/65EE6640-3536-4A5D-8935-020FDEA6FCDB.png" /></li></ul></li><li><p>Spearman</p><ul><li>Benchmarks monotonic relationship (whether linear or not), Spearman's coefficient is appropriate for both continuous and discrete variables, including ordinal variables.</li></ul></li><li><p>Kendall</p><ul><li><p>Is a <a href="https://en.wikipedia.org/wiki/Statistic" target="_blank" rel="noopener">statistic</a> used to measure the <a href="https://en.wikipedia.org/wiki/Ordinal_association" target="_blank" rel="noopener">ordinal association</a> between two measured quantities.</p></li><li><p>Contrary to the <a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" target="_blank" rel="noopener">Spearman correlation</a>, the Kendall correlation is not affected by how far from each other ranks are but only by whether the ranks between observations are equal or not, and is thus only appropriate for <a href="https://en.wikipedia.org/wiki/Discrete_variable" target="_blank" rel="noopener">discrete variables</a> but not defined for <a href="https://en.wikipedia.org/wiki/Continuous_variable" target="_blank" rel="noopener">continuous variables</a>.</p></li></ul></li><li><p>Summary: Pearson’s r for two normally distributed variables // Spearman’s rho for ratio data, ordinal data, etc (rank-order correlation) // Kendall’s tau for ordinal variables</p></li></ul></li><li><p>Co-occurrence</p><ul><li>The results are presented in a matrix format, where the cross tabulation of two fields is a cell value. The cell value represents the percentage of times that the two fields exist in the same events.</li></ul></li></ul><h3 id="techniques">Techniques</h3><ul><li><p>Null Hypothesis</p><ul><li>Is a general statement or default position that there is no relationship between two measured phenomena, or no association among groups. The null hypothesis is generally assumed to be true until evidence indicates otherwise.</li></ul></li><li><p>p-value</p><ul><li><p>Five heads in a row Example</p><ul><li><p>This demonstrates that specifying a direction (on a symmetric test statistic) halves the p-value (increases the significance) and can mean the difference between data being considered significant or not.</p></li><li><p>Suppose a researcher flips a coin five times in a row and assumes a null hypothesis that the coin is fair. The test statistic of "total number of heads" can be one-tailed or two-tailed: a one-tailed test corresponds to seeing if the coin is biased towards heads, but a two-tailed test corresponds to seeing if the coin is biased either way. The researcher flips the coin five times and observes heads each time (HHHHH), yielding a test statistic of 5. In a one-tailed test, this is the upper extreme of all possible outcomes, and yields a p-value of (1/2)5 = 1/32 ≈ 0.03. If the researcher assumed a significance level of 0.05, this result would be deemed significant and the hypothesis that the coin is fair would be rejected. In a two-tailed test, a test statistic of zero heads (TTTTT) is just as extreme and thus the data of HHHHH would yield a p-value of 2×(1/2)5 = 1/16 ≈ 0.06, which is not significant at the 0.05 level.</p></li></ul></li><li><p>In this method, as part of experimental design, before performing the experiment, one first chooses a model (the null hypothesis) and a threshold value for p, called the significance level of the test, traditionally 5% or 1% and denoted as α. If the p-value is less than the chosen significance level (α), that suggests that the observed data is sufficiently inconsistent with the null hypothesis that the null hypothesis may be rejected. However, that does not prove that the tested hypothesis is true. For typical analysis, using the standard α = 0.05 cutoff, the null hypothesis is rejected when p &lt; .05 and not rejected when p &gt; .05. The p-value does not, in itself, support reasoning about the probabilities of hypotheses but is only a tool for deciding whether to reject the null hypothesis.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/031EBE1C-F064-4DB0-9DE7-CABBA8D17668.png" /></li></ul></li></ul></li><li><p>p-hacking</p><ul><li>The process of data mining involves automatically testing huge numbers of hypotheses about a single <a href="https://en.wikipedia.org/wiki/Data_set" target="_blank" rel="noopener">data set</a> by exhaustively searching for combinations of variables that might show a correlation. Conventional tests of <a href="https://en.wikipedia.org/wiki/Statistical_significance" target="_blank" rel="noopener">statistical significance</a> are based on the probability that an observation arose by chance, and necessarily accept some risk of mistaken test results, called the <a href="https://en.wikipedia.org/wiki/Statistical_significance" target="_blank" rel="noopener">significance</a>.</li></ul></li></ul><h3 id="central-limit-theorem">Central Limit Theorem</h3><ul><li><p>States that a random variable defined as the average of a large number of independent and identically distributed random variables is itself approximately normally distributed.</p><ul><li>http://blog.vctr.me/posts/central-limit-theorem.html</li></ul></li></ul><h3 id="experiments-and-tests">Experiments and Tests</h3><ul><li><p>Flow Chart of Commonly Used Stat Tests<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/0DDFCAD3-A0C9-4978-A0F1-B47872AC82B2.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/45D4572F-11EE-4280-A978-384D0E1D872A.png" /></li></ul></li><li><p>Research Question</p><ul><li><p>Research question (Q):</p><ul><li>Asks whether the independent variable has an effect: “If there is a change in the independent variable, will there also be a change in the dependent variable?”</li></ul></li><li><p>Null hypothesis (Ho):</p><ul><li>The assumption that there is no effect: “There is no change in the dependent variable when the independent variable changes.”</li></ul></li></ul></li><li><p>Types of variables</p><ul><li><p>Dependent variable is the measure of interest</p></li><li><p>Independent variable is manipulated to observe the effect on dependent variable</p></li><li><p>Controlled variables are materials, measurements and methods that don’t change</p></li></ul></li><li><p>Experiment design</p><ul><li><p>Between subjects: Each subject sees one and only one condition</p></li><li><p>Within subjects: Subjects see more than one or all conditions</p></li></ul></li><li><p>Testing reliability with p-values</p><ul><li><p>Most tests calculate a p-value measuring observation extremity</p></li><li><p>Compare to significance level threshold α</p></li><li><p>α is the probability of rejecting H0 given that it is true</p></li><li><p>Commonly use α of 5% or 1%</p></li></ul></li></ul><h2 id="linear-algebra">Linear Algebra</h2><h3 id="matrices">Matrices</h3><ul><li><p>Almost all Machine Learning algorithms use Matrix algebra in one way or another. This is a broad subject, too large to be included here in it’s full length. Here’s a start: https://en.wikipedia.org/wiki/Matrix_(mathematics)</p><ul><li><p>Basic Operations: Addition, Multiplication, Transposition</p></li><li><p>Transformations</p></li><li><p>Trace, Rank, Determinante, Inverse</p></li></ul></li></ul><h3 id="eigenvectors-and-eigenvalues">Eigenvectors and Eigenvalues</h3><ul><li><p>In <a href="https://en.wikipedia.org/wiki/Linear_algebra" target="_blank" rel="noopener">linear algebra</a>, an eigenvector or characteristic vector of a <a href="https://en.wikipedia.org/wiki/Linear_map" target="_blank" rel="noopener">linear transformation</a> T from a <a href="https://en.wikipedia.org/wiki/Vector_space" target="_blank" rel="noopener">vector space</a> V over a <a href="https://en.wikipedia.org/wiki/Field_(mathematics)" target="_blank" rel="noopener">field</a> F into itself is a non-zero <a href="https://en.wikipedia.org/wiki/Vector_space" target="_blank" rel="noopener">vector</a> that does not change its direction when that linear transformation is applied to it.</p><ul><li>http://setosa.io/ev/eigenvectors-and-eigenvalues/<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/877F41BA-7063-48C3-AA8C-26173DDA8DE4.png" /></li></ul></li></ul><h3 id="derivatives-chain-rule">Derivatives Chain Rule</h3><ul><li><p>Rule<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/A416F2D4-5106-41F7-8E5B-40B5E73EA434.png" /></p><ul><li>Leibniz Notation<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/D2FA6B8A-FA31-4015-87B8-5FDCC70A63F4.png" /></li></ul></li></ul><h3 id="jacobian-matrix">Jacobian Matrix</h3><ul><li><p>The <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)" target="_blank" rel="noopener">matrix</a> of all first-order <a href="https://en.wikipedia.org/wiki/Partial_derivative" target="_blank" rel="noopener">partial derivatives</a> of a <a href="https://en.wikipedia.org/wiki/Vector-valued_function" target="_blank" rel="noopener">vector-valued function</a>. When the matrix is a <a href="https://en.wikipedia.org/wiki/Square_matrix" target="_blank" rel="noopener">square matrix</a>, both the matrix and its <a href="https://en.wikipedia.org/wiki/Determinant" target="_blank" rel="noopener">determinant</a> are referred to as the Jacobian in literature</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/4E2AA87B-6A9A-42C7-8C3E-C2B25E198D30.png" /></li></ul></li></ul><h3 id="gradient">Gradient</h3><ul><li><p>The gradient is a multi-variable generalization of the derivative. The gradient is a vector-valued function, as opposed to a derivative, which is scalar-valued.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/695F6632-0CA9-4AE7-9058-6F7E6DC780B4.png" /></li></ul></li></ul><h3 id="tensors">Tensors</h3><ul><li><p>For Machine Learning purposes, a Tensor can be described as a Multidimentional Matrix Matrix. Depending on the dimensions, the Tensor can be a Scalar, a Vector, a Matrix, or a Multidimentional Matrix.</p><ul><li><p>When measuring the forces applied to an infinitesimal cube, one can store the force values in a multidimensional matrix.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/1F2AE40B-6E5E-4501-AEB1-FFCAF145B311.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/7E0FE9AC-605D-4FB4-8790-DE3B05336F1F.png" /></li></ul></li></ul></li></ul><h3 id="curse-of-dimensionality">Curse of Dimensionality</h3><ul><li>When the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality.</li></ul><h1 id="machine-learning-data-processing">Machine Learning Data Processing</h1><h2 id="feature-selection">Feature Selection</h2><h3 id="correlation">Correlation</h3><ul><li><p>Features should be uncorrelated with each other and highly correlated to the feature we’re trying to predict.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/87F7C9A2-8634-4E57-85E2-9B3C4B9D33F8.png" /></p><ul><li><p>Covariance</p><ul><li>A measure of how much two random variables change together. Math: dot(de_mean(x), de_mean(y)) / (n - 1)<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/0C8FD4CE-AC05-4975-ABDD-B54BD11F8312.png" /></li></ul></li></ul></li></ul><h3 id="dimensionality-reduction-1">Dimensionality Reduction</h3><ul><li><p>Principal Component Analysis (PCA)</p><ul><li><p>Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.</p><ul><li>Plot the variance per feature and select the features with the largest variance.</li></ul></li></ul></li><li><p>Singular Value Decomposition (SVD)</p><ul><li><p>SVD is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix (for example, a symmetric matrix with positive eigenvalues) to any m×n matrix via an extension of the polar decomposition. It has many useful applications in signal processing and statistics.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/53B97C2C-8826-477D-BC26-41CABEC4A42E.png" /></li></ul></li></ul></li></ul><h3 id="importance">Importance</h3><ul><li><p>Filter Methods</p><ul><li><p>Filter type methods select features based only on general metrics like the correlation with the variable to predict. Filter methods suppress the least interesting variables. The other variables will be part of a classification or a regression model used to classify or to predict data. These methods are particularly effective in computation time and robust to overfitting.</p><ul><li><p>Correlation</p></li><li><p>Linear Discriminant Analysis</p></li><li><p>ANOVA: Analysis of Variance</p></li><li><p>Chi-Square</p></li></ul></li></ul></li><li><p>Wrapper Methods</p><ul><li><p>Wrapper methods evaluate subsets of variables which allows, unlike filter approaches, to detect the possible interactions between variables. The two main disadvantages of these methods are : The increasing overfitting risk when the number of observations is insufficient. AND. The significant computation time when the number of variables is large.</p><ul><li><p>Forward Selection</p></li><li><p>Backward Elimination</p></li><li><p>Recursive Feature Ellimination</p></li><li><p>Genetic Algorithms</p></li></ul></li></ul></li><li><p>Embedded Methods</p><ul><li><p>Embedded methods try to combine the advantages of both previous methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously.</p><ul><li><p>Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.</p></li><li><p>Ridge regression performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients.</p></li></ul></li></ul></li></ul><h2 id="feature-encoding">Feature Encoding</h2><h3 id="machine-learning-algorithms-perform-linear-algebra-on-matrices-which-means-all-features-must-be-numeric.-encoding-helps-us-do-this.">Machine Learning algorithms perform Linear Algebra on Matrices, which means all features must be numeric. Encoding helps us do this.</h3><h3 id="label-encoding">Label Encoding</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/397D4FCF-04A7-4D68-927F-D6414FA747FE.png" /></p><ul><li><p>One Hot Encoding<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/8F75FD12-E83A-4DE5-94ED-CDDD231E3E14.png" /></p><ul><li>In One Hot Encoding, make sure the encodings are done in a way that all features are linearly independent.</li></ul></li></ul><h2 id="feature-normalisation-or-scaling">Feature Normalisation or Scaling</h2><h3 id="section"></h3><ul><li><p>Since the range of values of raw data varies widely, in some <a href="https://en.wikipedia.org/wiki/Machine_learning" target="_blank" rel="noopener">machine learning</a> algorithms, objective functions will not work properly without <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)" target="_blank" rel="noopener">normalization</a>. Another reason why feature scaling is applied is that <a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener">gradient descent</a> converges much faster with feature scaling than without it.</p></li><li><p>Methods</p><ul><li><p>Rescaling</p><ul><li><p>The simplest method is rescaling the range of features to scale the range in [0, 1] or [−1, 1].</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/044BC9F5-9D15-4A64-9529-63403036B623.png" /></li></ul></li></ul></li><li><p>Standardization</p><ul><li><p>Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C228CF35-C057-4DF4-BEC2-6A7CB7D1C92F.png" /></li></ul></li></ul></li><li><p>Scaling to unit length</p><ul><li><p>To scale the components of a feature vector such that the complete vector has length one.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/1452F430-91D5-4E1A-8021-3BA21B90AD83.png" /></li></ul></li></ul></li></ul></li></ul><h2 id="dataset-construction">Dataset Construction</h2><h3 id="training-dataset">Training Dataset</h3><ul><li><p>A set of examples used for learning</p><ul><li><ul><li>To fit the parameters of the classifier in the Multilayer Perceptron, for instance, we would use the training set to find the “optimal” weights when using back-progapation.</li></ul></li></ul></li></ul><h3 id="test-dataset">Test Dataset</h3><ul><li><p>A set of examples used only to assess the performance of a fully-trained classifier</p><ul><li>In the Multilayer Perceptron case, we would use the test to estimate the error rate after we have chosen the final model (MLP size and actual weights) After assessing the final model on the test set, YOU MUST NOT tune the model any further.</li></ul></li></ul><h3 id="validation-dataset">Validation Dataset</h3><ul><li><p>A set of examples used to tune the parameters of a classifier</p><ul><li>In the Multilayer Perceptron case, we would use the validation set to find the “optimal” number of hidden units or determine a stopping point for the back-propagation algorithm</li></ul></li></ul><h3 id="cross-validation-1">Cross Validation</h3><ul><li>One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds.</li></ul><h2 id="feature-engineering">Feature Engineering</h2><h3 id="decompose">Decompose</h3><ul><li>Converting 2014-09-20T20:45:40Z into categorical attributes like hour_of_the_day, part_of_day, etc.</li></ul><h3 id="discretization">Discretization</h3><ul><li><p>Continuous Features</p><ul><li>Typically data is discretized into partitions of K equal lengths/width (equal intervals) or K% of the total data (equal frequencies).</li></ul></li><li><p>Categorical Features</p><ul><li>Values for categorical features may be combined, particularly when there’s few samples for some categories.</li></ul></li></ul><h3 id="reframe-numerical-quantities">Reframe Numerical Quantities</h3><ul><li>Changing from grams to kg, and losing detail might be both wanted and efficient for calculation</li></ul><h3 id="crossing">Crossing</h3><ul><li>Creating new features as a combination of existing features. Could be multiplying numerical features, or combining categorical variables. This is a great way to add domain expertise knowledge to the dataset.</li></ul><h2 id="feature-imputation">Feature Imputation</h2><h3 id="hot-deck">Hot-Deck</h3><ul><li>The technique then finds the first missing value and uses the cell value immediately prior to the data that are missing to impute the missing value.</li></ul><h3 id="cold-deck">Cold-Deck</h3><ul><li>Selects donors from another dataset to complete missing data.</li></ul><h3 id="mean-substitution">Mean-substitution</h3><ul><li>Another imputation technique involves replacing any missing value with the mean of that variable for all other cases, which has the benefit of not changing the sample mean for that variable.</li></ul><h3 id="regression-1">Regression</h3><ul><li>A regression model is estimated to predict observed values of a variable based on other variables, and that model is then used to impute values in cases where that variable is missing</li></ul><h3 id="some-libraries...">Some Libraries...</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/091E0B05-1B93-4959-B8C7-7E135A336EF7.png" /></p><h2 id="feature-cleaning">Feature Cleaning</h2><h3 id="missing-values">Missing values</h3><ul><li>One may choose to either omit elements from a dataset that contain missing values or to impute a value</li></ul><h3 id="special-values">Special values</h3><ul><li>Numeric variables are endowed with several formalized special values including ±Inf, NA and NaN. Calculations involving special values often result in special values, and need to be handled/cleaned</li></ul><h3 id="outliers">Outliers</h3><ul><li>They should be detected, but not necessarily removed. Their inclusion in the analysis is a statistical decision.</li></ul><h3 id="obvious-inconsistencies">Obvious inconsistencies</h3><ul><li>A person's age cannot be negative, a man cannot be pregnant and an under-aged person cannot possess a drivers license.</li></ul><h2 id="data-exploration">Data Exploration</h2><h3 id="variable-identification">Variable Identification</h3><ul><li>Identify Predictor (Input) and Target (output) variables. Next, identify the data type and category of the variables.</li></ul><h3 id="univariate-analysis">Univariate Analysis</h3><ul><li><p>Continuous Features</p><ul><li>Mean, Median, Mode, Min, Max, Range, Quartile, IQR, Variance, Standard Deviation, Skewness, Histogram, Box Plot</li></ul></li><li><p>Categorical Features</p><ul><li>Frequency, Histogram</li></ul></li></ul><h3 id="bi-variate-analysis">Bi-variate Analysis</h3><ul><li><p>Finds out the relationship between two variables.</p></li><li><p>Scatter Plot</p></li><li><p>Correlation Plot - Heatmap</p></li><li><ul><li><p>Two-way table</p><ul><li>We can start analyzing the relationship by creating a two-way table of count and count%.</li></ul></li><li><p>Stacked Column Chart</p></li><li><p>Chi-Square Test</p><ul><li>This test is used to derive the statistical significance of relationship between the variables.</li></ul></li><li><p>Z-Test/ T-Test</p></li><li><p>ANOVA</p></li></ul></li></ul><h2 id="data-types">Data Types</h2><h3 id="nominal---is-for-mutual-exclusive-but-not-ordered-categories.">Nominal - is for mutual exclusive, but not ordered, categories.</h3><h3 id="ordinal---is-one-where-the-order-matters-but-not-the-difference-between-values.">Ordinal - is one where the order matters but not the difference between values.</h3><h3 id="ratio---has-all-the-properties-of-an-interval-variable-and-also-has-a-clear-definition-of-0.0.">Ratio - has all the properties of an interval variable, and also has a clear definition of 0.0.</h3><h3 id="interval---is-a-measurement-where-the-difference-between-two-values-is-meaningful.">Interval - is a measurement where the difference between two values is meaningful.</h3><h3 id="section-1"></h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F45BC780-34A4-4478-8204-361A57CAC7A4.png" /></p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F3BE7BDA-CC8D-44E9-9586-3C4EA3B875B2.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/385CB279-89CD-4AD6-82E7-D80BAA9F2826.png" /></li></ul></li></ul><h1 id="machine-learning-models">Machine Learning Models</h1><h2 id="regression-2">Regression</h2><h3 id="linear-regression">Linear Regression</h3><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B3C1808C-9C79-4E45-9AC6-A5685A5D8407.png" /></li></ul><h3 id="generalised-linear-models-glms">Generalised Linear Models (GLMs)</h3><ul><li><p>Is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.</p></li><li><p>Link Function</p><ul><li><p>Identity</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C5A62BA8-B2BB-4325-A5CC-ADBF498E685A.png" /></li></ul></li><li><p>Inverse</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/72E0D7A0-9E26-49E9-A540-18A69873BE47.png" /></li></ul></li><li><p>Logit</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/3B5287CE-BF34-4688-844F-57875F627F8A.png" /></li></ul></li></ul></li><li><p>Cost Function is found via Maximum Likelihood Estimation</p></li></ul><h3 id="locally-estimated-scatterplot-smoothing-loess">Locally Estimated Scatterplot Smoothing (LOESS)</h3><h3 id="ridge-regression">Ridge Regression</h3><h3 id="least-absolute-shrinkage-and-selection-operator-lasso">Least Absolute Shrinkage and Selection Operator (LASSO)</h3><h3 id="logistic-regression">Logistic Regression</h3><ul><li><p>Logistic Function<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/249C3E38-0B9A-4493-982C-CC26B7614B12.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D75A0917-3A7D-441E-B18F-A51B087761D3.png" /></li></ul></li></ul><h2 id="bayesian">Bayesian</h2><h3 id="naive-bayes">Naive Bayes</h3><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/A1AD6FA2-19D3-4C06-A092-420D007E8E3E.png" /></p><ul><li>Naive Bayes Classifier. We neglect the denominator as we calculate for every class and pick the max of the numerator<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/1810007C-AE05-41BF-A70B-6FDA309289BC.png" /></li></ul><h3 id="multinomial-naive-bayes">Multinomial Naive Bayes</h3><h3 id="bayesian-belief-network-bbn">Bayesian Belief Network (BBN)</h3><h2 id="dimensionality-reduction-2">Dimensionality Reduction</h2><h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3><h3 id="partial-least-squares-regression-plsr">Partial Least Squares Regression (PLSR)</h3><h3 id="principal-component-regression-pcr">Principal Component Regression (PCR)</h3><h3 id="partial-least-squares-discriminant-analysis">Partial Least Squares Discriminant Analysis</h3><h3 id="quadratic-discriminant-analysis-qda">Quadratic Discriminant Analysis (QDA)</h3><h3 id="linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)</h3><h2 id="instance-based">Instance Based</h2><h3 id="k-nearest-neighbour-knn">k-nearest Neighbour (kNN)</h3><h3 id="learning-vector-quantization-lvq">Learning Vector Quantization (LVQ)</h3><h3 id="self-organising-map-som">Self-Organising Map (SOM)</h3><h3 id="locally-weighted-learning-lwl">Locally Weighted Learning (LWL)</h3><h2 id="decision-tree">Decision Tree</h2><h3 id="random-forest">Random Forest</h3><h3 id="classification-and-regression-tree-cart">Classification and Regression Tree (CART)</h3><h3 id="gradient-boosting-machines-gbm">Gradient Boosting Machines (GBM)</h3><h3 id="conditional-decision-trees">Conditional Decision Trees</h3><h3 id="gradient-boosted-regression-trees-gbrt">Gradient Boosted Regression Trees (GBRT)</h3><h2 id="clustering-2">Clustering</h2><h3 id="algorithms">Algorithms</h3><ul><li><p>Hierarchical Clustering</p><ul><li><p>Linkage</p><ul><li><p>complete</p></li><li><p>single</p></li><li><p>average</p></li><li><p>centroid</p></li></ul></li><li><p>Dissimilarity Measure</p><ul><li><p>Euclidean</p><ul><li>Euclidean distance or Euclidean metric is the "ordinary" straight-line distance between two points in Euclidean space.</li></ul></li><li><p>Manhattan</p><ul><li>The distance between two points measured along axes at right angles.</li></ul></li></ul></li></ul></li><li><p>k-Means</p><ul><li>How many clusters do we select?</li></ul></li><li><p>k-Medians</p></li><li><p>Fuzzy C-Means</p></li><li><p>Self-Organising Maps (SOM)</p></li><li><p>Expectation Maximization</p></li><li><p>DBSCAN</p></li></ul><h3 id="validation">Validation</h3><ul><li><p>Data Structure Metrics</p><ul><li><p>Dunn Index</p></li><li><p>Connectivity</p></li><li><p>Silhouette Width</p></li></ul></li><li><p>Stability Metrics</p><ul><li><p>Non-overlap APN</p></li><li><p>Average Distance AD</p></li><li><p>Figure of Merit FOM</p></li><li><p>Average Distance Between Means ADM</p></li></ul></li></ul><h2 id="neural-networks">Neural Networks</h2><h3 id="unit-neurons">Unit (Neurons)</h3><ul><li><p>A unit often refers to the activation function in a layer by which the inputs are transformed via a nonlinear activation function (for example by the logistic sigmoid function). Usually, a unit has several incoming connections and several outgoing connections.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5697CF3A-4832-4849-98B9-3132500C6576.png" /></li></ul></li></ul><h3 id="input-layer">Input Layer</h3><ul><li>Comprised of multiple Real-Valued inputs. Each input must be linearly independent from each other.</li></ul><h3 id="hidden-layers">Hidden Layers</h3><ul><li><p>Layers other than the input and output layers. A layer is the highest-level building block in deep learning. A layer is a container that usually receives weighted input, transforms it with a set of mostly non-linear functions and then passes these values as output to the next layer.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/EB87C752-40AC-40E4-89BF-E85F3AA6AE97.png" /></li></ul></li></ul><h3 id="batch-normalization">Batch Normalization</h3><ul><li><p>Using mini-batches of examples, as opposed to one example at a time, is helpful in several ways. First, the gradient of the loss over a mini-batch is an estimate of the gradient over the training set, whose quality improves as the batch size increases. Second, computation over a batch can be much more efficient than m computations for individual examples, due to the parallelism afforded by the modern computing platforms.</p><ul><li>With SGD, the training proceeds in steps, and at each step we consider a mini- batch x1...m of size m. The mini-batch is used to approx- imate the gradient of the loss function with respect to the parameters.</li></ul></li></ul><h3 id="learning-rate">Learning Rate</h3><ul><li><p>Neural networks are often trained by gradient descent on the weights. This means at each iteration we use backpropagation to calculate the derivative of the loss function with respect to each weight and subtract it from that weight.</p><ul><li>However, if you actually try that, the weights will change far too much each iteration, which will make them “overcorrect” and the loss will actually increase/diverge. So in practice, people usually multiply each derivative by a small value called the “learning rate” before they subtract it from its corresponding weight.</li></ul></li><li><p>Tricks</p><ul><li><p>Simplest recipe: keep it fixed and use the same for all parameters.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/0FCF647C-94C1-42AA-B26D-BB5F5DB14248.png" /></li></ul></li><li><p>Better results by allowing learning rates to decrease Options:</p><ul><li><p>Reduce by 0.5 when validation error stops improving</p></li><li><p>Reduction by O(1/t) because of theoretical convergence guarantees, with hyper-parameters ε0 and τ and t is iteration numbers.</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/5100966F-881F-45C0-9D48-807D86657D02.png" /></li></ul></li><li><p>Better yet: No hand-set learning of rates by using AdaGrad</p></li></ul></li></ul></li></ul><h3 id="weight-initialization">Weight Initialization</h3><ul><li><p>All Zero Initialization</p><ul><li><p>In the ideal situation, with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative. A reasonable-sounding idea then might be to set all the initial weights to zero, which you expect to be the “best guess” in expectation.</p><ul><li>But, this turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during back-propagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.</li></ul></li></ul></li><li><p>Initialization with Small Random Numbers</p><ul><li><p>Thus, you still want the weights to be very close to zero, but not identically zero. In this way, you can random these neurons to small numbers which are very close to zero, and it is treated as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network.</p><ul><li>The implementation for weights might simply drawing values from a normal distribution with zero mean, and unit standard deviation. It is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.</li></ul></li></ul></li><li><p>Calibrating the Variances</p><ul><li><p>One problem with the above suggestion is that the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs. It turns out that you can normalize the variance of each neuron's output to 1 by scaling its weight vector by the square root of its fan-in (i.e., its number of inputs)</p><ul><li>This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence. The detailed derivations can be found from Page. 18 to 23 of the slides. Please note that, in the derivations, it does not consider the influence of ReLU neurons.</li></ul></li></ul></li></ul><h3 id="backpropagation">Backpropagation</h3><ul><li><p>Is a method used in artificial neural networks to calculate the error contribution of each neuron after a batch of data. It calculates the gradient of the loss function. It is commonly used in the gradient descent optimization algorithm. It is also called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers.</p><ul><li><p>Neural Network taking 4 dimension vector representation of words.<br /><img src="https://2020.iosdevlog.com/2020/02/22/ds/76B7C54A-E48B-4D1B-A1CF-602F165D7C09.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/DB988698-5B05-4D61-8514-5DE7B085C286.png" /></li></ul></li></ul></li><li><p>In this method, we reuse partial derivatives computed for higher layers in lower layers, for efficiency.</p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/3DAD67C3-1FCB-475E-96B7-E2D2793A9FB6.png" /></p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/F6753EA8-E4DB-4409-978F-AB2BCC323032.png" /></p><ul><li><p><img src="https://2020.iosdevlog.com/2020/02/22/ds/4C67AB0A-2FB0-4B03-B9FA-E36203B3E37C.png" /></p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/F340BAA3-345D-4D1E-9A6A-761D6BCF4E1F.png" /></li></ul></li></ul></li></ul></li></ul></li></ul><h3 id="activation-functions">Activation Functions</h3><ul><li><p>Defines the output of that node given an input or set of inputs.</p></li><li><p>Types</p><ul><li><p>ReLU</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/B6CD5EBF-451C-4E1F-AA90-DFB9FD2165FE.png" /></li></ul></li><li><p>Sigmoid / Logistic</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D54BBB93-EE9A-433D-99ED-7D4C82B94AD2.png" /></li></ul></li><li><p>Binary</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C2E4A20B-CE21-414D-82F6-D179E30C7572.png" /></li></ul></li><li><p>Tanh</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/FCECCABF-A68B-421E-8498-EDF2D313371B.png" /></li></ul></li><li><p>Softplus</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/C4BDC911-1EDC-44D1-AEFA-FDC64360BA6D.png" /></li></ul></li><li><p>Softmax</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/D42822B8-CE14-4B16-A24D-8151614F601D.png" /></li></ul></li><li><p>Maxout</p><ul><li><img src="https://2020.iosdevlog.com/2020/02/22/ds/9E4B97B9-A6DC-4F0D-8D8A-7030819525B1.png" /></li></ul></li><li><p>Leaky ReLU, PReLU, RReLU, ELU, SELU, and others.</p></li></ul></li></ul><p>参考：<a href="https://github.com/dformoso/machine-learning-mindmap" target="_blank" rel="noopener" class="uri">https://github.com/dformoso/machine-learning-mindmap</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/22/ds/Process.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;Process&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="ML" scheme="https://2020.iosdevlog.com/tags/ML/"/>
    
      <category term="DS" scheme="https://2020.iosdevlog.com/tags/DS/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习算法的数学解析与Python实现》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/02/21/9787111642602/"/>
    <id>https://2020.iosdevlog.com/2020/02/21/9787111642602/</id>
    <published>2020-02-21T10:47:23.000Z</published>
    <updated>2020-02-21T12:52:56.826Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/1.jpg" alt="" /><figcaption>《机器学习算法的数学解析与Python实现》</figcaption></figure><p>书名：机器学习算法的数学解析与Python实现<br />作者：莫凡<br />出版社：机械工业出版社<br />出版时间：2020-01<br />ISBN：9787111642602</p><a id="more"></a><p>第1章首先介绍机器学习究竟是什么，特别是与“人工智能”“深度学习”这些经常在一起出现的术语究竟有什么关系，又有什么区别。本章也将对机器学习知识体系里的一些常用术语进行简要说明，如果读者此前并不了解机器学习，则可以通过本章了解相关背景知识。</p><p>第2章对当前机器学习算法常用的Python编程语言以及相关的Python库进行介绍，同时列举一些常用的功能。</p><p>第3章开始正式介绍机器学习算法，要介绍的第一款机器学习算法是线性回归，本章将对回归问题、线性模型和如何用线性模型解决回归问题，以及对机器学习解决问题的主要模式进行介绍。</p><p>从第4章开始，介绍当下机器学习应用最广的分类问题，第一款解决分类问题的算法是Logistic回归分类算法，即用线性模型结合Logistic函数解决分类问题。</p><p>第5章介绍KNN分类算法，这款算法不依赖太复杂的数学原理，因此一般被认为是最直观好懂的分类算法之一。</p><p>第6章介绍朴素贝叶斯分类算法，它基于贝叶斯公式设计，理论清晰、逻辑易懂，是一款典型的基于概率统计理论解决分类问题的机器学习算法。</p><p>第7章介绍决策树分类算法，这是一款很重要的算法，从思想到结构都对程序员非常友好，当前XGBoost等主流机器学习算法就是在决策树算法的基础上，结合集成学习方法设计而成的。</p><p>第8章介绍支持向量机分类算法，这是一款在学术界和工业界都有口皆碑的机器学习模型。在深度学习出现之前，支持向量机被视作最被看好的机器学习算法，能力强、理论美，也是本书中最为复杂的机器模型。</p><p>第9章介绍无监督学习的聚类问题，以及简单好懂的聚类算法——K-means聚类算法。</p><p>第10章介绍神经网络分类算法，当前大热的深度学习就是从神经网络算法这一支发展而来的，而且大量继承了神经网络的思想和结构，可以作为了解深度学习的预备。</p><p>第11章介绍集成学习方法，以及如何通过组合两个以上的机器学习模型来提升预测效果。</p><h2 id="第1章-机器学习概述">第1章 机器学习概述</h2><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/2.jpg" alt="" /><figcaption>人工智能、机器学习和深度学习三者是包含关系</figcaption></figure><h3 id="机器学习知识的三个需求层次">机器学习知识的三个需求层次</h3><ol type="1"><li>设计需求层次<ul><li>思想原理</li></ul></li><li>调用需求层次<ul><li>运行流程</li></ul></li><li>数学需求层次<ul><li>数学解析</li></ul></li></ol><p>“训练模型”</p><p>“训练” == “拟合”</p><p>算法</p><ul><li>数据结构算法<ul><li>算</li></ul></li><li>机器学习算法<ul><li>猜<ul><li>我猜是什么</li><li>我猜中没有</li></ul></li></ul></li></ul><h3 id="猜数字游戏">“猜数字”游戏</h3><p>裁判选定一个数字，接着参赛选手也报一个数字，裁判回答他猜大了或猜小了，不断重复这个过程，直到最后猜中。</p><table><thead><tr class="header"><th>猜数字</th><th>机器学习</th></tr></thead><tbody><tr class="odd"><td>参赛选手</td><td>算法模型</td></tr><tr class="even"><td>裁判回答</td><td>损失函数</td></tr></tbody></table><h3 id="拟合">拟合</h3><ul><li>欠拟合<ul><li>准确性不够</li></ul></li><li>过拟合<ul><li>泛化性不好</li></ul></li></ul><h3 id="机器学习的基本概念">机器学习的基本概念</h3><h4 id="术语">术语</h4><ul><li><p><strong>模型</strong><br />模型（Model）是机器学习的核心概念。如果认为编程有两大组成部分，即算法和数据结构，那么机器学习的两大组成部分就是模型和数据集。如果之前没有接触过相关概念，想必你现在很希望直观地理解什么是模型，但对模型给出一个简洁又严谨的定义并不容易，你可以认为它是某种机器学习算法在设定参数后的产物，它的作用和编程时用到的函数一样，可以根据某些输入得到某些输出。既然叫机器学习算法，不妨将它想象成一台机器，其上有很多旋钮，这些旋钮就是参数。机器本身是有输入和输出功能的，根据不同的旋钮组合，同一种输入可以产生不同的输出，而机器学习的过程就是找到合适的那组旋钮组合，通过输入得到你所希望的输出。</p></li><li><p><strong>数据集</strong><br />如果说机器学习的“机器”指的是模型，那么数据集就可以说是驱动着这台机器去“学习”的“燃料”。有些文献将数据集又分为训练集和测试集，其实它们的内容和形式并无差异，只是用在不同的地方：在训练模型阶段使用，就叫作训练集；在测试模型阶段使用，就叫作测试集。</p></li><li><p><strong>数据</strong><br />我们刚才提到了数据集，数据集就是数据的集合。在机器学习中，我们称一条数据为一个样本（Sample），形式类似一维数组。样本通常包含多个特征（Feature），如果是用于分类问题的数据集，还会包含类别（Class Label）信息，如果是回归问题的数据集，则会包含一个连续型的数值。</p></li><li><p><strong>特征</strong><br />这个术语又容易让你产生误解了。我们一般把可以作为人或事物特点的征象、标志等称作特征，譬如这个人鼻子很大，这就是特征，但在机器学习中，特征是某个对象的几个记录维度。我们都填写过个人信息表，特征就是这张表里的空格，如名字、性别、出生日期、籍贯等，一份个人信息表格可以看成一个样本，名字、籍贯这些信息就称作特征。前面说数据形式类似一维数组，那么特征就是数组的值。</p></li><li><p><strong>向量</strong><br />向量为线性代数术语，机器学习模型算法的运算均基于线性代数法则，不妨认为向量就是该类算法所对应的“数据结构”。一条样本数据就是以一个向量的形式输入模型的。一条监督学习数据的向量形式如下：</p><p>[特征X1值，特征X2值，…, Y1值]</p></li><li><p><strong>矩阵</strong><br />矩阵为线性代数术语，可以将矩阵看成由向量组成的数组，形式上也非常接近二维数组。前面所说的数据集，通常就是以矩阵的形式输入模型的，常见的矩阵形式如下：</p><p>[[特征X1值，特征X2值，…, Y1值]，<br />'[特征X1值，特征X2值，…, Y2值]，<br />…<br />[特征X1值，特征X2值，…, Yn值]]</p></li></ul><p>其实这个组织形式非常类似电子表格，不妨就以电子表格来对照理解。每一行就是一个样本，每一列就是一个特征维度，譬如某个数据集一共包括了7个样本，那就是有7行数据，每个样本又都有4个维度的特征，那就是每行数据有4列，用电子表格表示如图1-2所示，其中，A～D列为特征，E列为结果。</p><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/3.jpg" alt="" /><figcaption>用电子表格来表示机器学习的数据集矩阵</figcaption></figure><h4 id="常用函数">常用函数</h4><ul><li><strong>假设函数（Hypothesis Function）</strong></li></ul><p><span class="math display">\[H(x)\]</span></p><p>这里的 <span class="math inline">\(x\)</span> 可以简单理解成矩阵形式的数据，我们把数据“喂”给假设函数，假设函数就会返回一个结果，而这个结果正是机器学习所得到的预测结果。</p><ul><li><strong>损失函数（Loss Function）/ 目标函数</strong></li></ul><p><span class="math display">\[L(x)\]</span></p><p><span class="math inline">\(L\)</span> 代表 Loss，这里的 <span class="math inline">\(x\)</span> 是假设函数的预测结果。</p><p>函数返回值越大，表示结果偏差越大。</p><ul><li><strong>成本函数（Cost Function）</strong></li></ul><p><span class="math display">\[J(x)\]</span></p><p>这里的 <span class="math inline">\(x\)</span> 也是假设函数的预测结果。</p><p>函数返回值越大，表示偏差越大。</p><table><thead><tr class="header"><th>差别</th><th style="text-align: center;">损失函数</th><th style="text-align: center;">成本函数</th></tr></thead><tbody><tr class="odd"><td>对象</td><td style="text-align: center;">单个样本</td><td style="text-align: center;">整个数据集</td></tr><tr class="even"><td>角度</td><td style="text-align: center;">微观</td><td style="text-align: center;">宏观</td></tr></tbody></table><p>成本函数是由损失函数计算得到的。</p><p><span class="math inline">\(J(x) = Sum(L(x))\)</span></p><p>或者</p><p><span class="math inline">\(J(x) = Avg(L(x))\)</span></p><h4 id="机器学习的基本模式">机器学习的基本模式</h4><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/4.jpg" alt="" /><figcaption>假设函数产生的偏差驱动着机器学习模型不断优化</figcaption></figure><ul><li>数据</li><li>假设函数</li><li>损失函数</li></ul><h4 id="优化方法">优化方法</h4><p><span class="math display">\[ min(L(x)) \]</span></p><p><span class="math display">\[新参数值 = 旧参数值 - 损失值\]</span></p><p>牛顿法、拟牛顿法、共轭梯度法</p><p><strong>梯度下降（Gradient Descent）</strong>法是机器学习中常用的一种优化方法</p><p>梯度下降（Gradient Descent）法是机器学习中常用的一种优化方法，梯度是微积分学的术语，某个函数在某点的梯度指向该函数取得最大值的方向，那么它的反方向自然就是取得最小值的方向。所以只要对损失函数采用梯度下降法，让假设函数朝着梯度的负方向更新权值，就能达到令损失值最小化的效果。</p><p><strong>倒车</strong>:<br />1. 方向<br />1. 大小</p><ul><li>批量梯度下降（Batch Gradient Descent<ul><li>每次迭代都使用全部样本</li></ul></li><li>随机梯度下降（Stochastic Gradient Descent<ul><li>每次迭代只使用一个样本</li></ul></li></ul><p>因为需要计算的样本小，随机梯度下降的迭代速度更快，但更容易陷入局部最优，而不能达到全局最优点。</p><h3 id="机器学习问题分类">机器学习问题分类</h3><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/5.jpg" alt="" /><figcaption>机器学习问题具体类别的判断方法图</figcaption></figure><p>无监督学习（Unsupervised Learning）<br />有监督学习（Supervised Learning）</p><h3 id="常用的机器学习算法">常用的机器学习算法</h3><ol type="1"><li>线性回归算法<ul><li>这是最基本的机器学习算法，但麻雀虽小，五脏俱全，该算法称得上是机器学习算法界的“Hello World”程序，是用线性方法解决回归问题。</li></ul></li><li>Logistic回归分类算法<ul><li>这可谓是线性回归算法的“孪生兄弟”，其核心思想仍然是线性方法，但套了一件名为Logistic函数的“马甲”，使得其具有解决分类问题的能力。</li></ul></li><li>KNN分类算法<ul><li>该算法是本书介绍的分类算法中唯一一个不依赖数学或统计模型，纯粹依靠“生活经验”的算法，它通过“找最近邻”的思想解决分类问题，其核心思想和区块链技术中的共识机制有着深远的关系。</li></ul></li><li>朴素贝叶斯分类算法<ul><li>这是一套能够刷新你世界观的算法，它认为结果不是确定性的而是概率性的，你眼前所见的不过是概率最大的结果罢了。当然，算法是用来解决问题的，朴素贝叶斯分类算法解决的是分类问题。</li></ul></li><li>决策树分类算法<ul><li>如果程序员的思维逻辑能够用if-else来概括的话，决策树分类算法应该就是最接近程序员逻辑的机器学习算法。</li></ul></li><li>支持向量机分类算法<ul><li>如果说Logistic回归分类算法是最基本的线性分类算法，那么支持向量机则是线性分类算法的最高形式，同时也是最“数学”的一种机器学习算法。该算法使用一系列令人拍案叫绝的数学技巧，将线性不可分的数据点映射成线性可分，再用最简单的线性方法来解决问题。</li></ul></li><li>K-means聚类算法<ul><li>有监督学习是当前机器学习的一种主流方式，但样本标记需要耗费大量人工成本，容易出现样本累积规模庞大，但标记不足的问题。无监督学习则是一种无须依赖标记样本的机器学习算法，聚类算法就是其中具有代表性的一种，而K-means是聚类算法中的典型代表。</li></ul></li><li>神经网络分类算法<ul><li>神经网络就是由许多神经元连接所构成的网络，很多人认为该算法是一种仿生算法，模仿的对象正是我们的大脑。神经网络分类算法也是当下热门的深度学习算法的起点。</li></ul></li></ol><h4 id="机器学习算法的性能衡量指标">机器学习算法的性能衡量指标</h4><p>NFL定律（No Free Lunch Theorem，中文一般翻译为“没有免费午餐定律”）。</p><ul><li>TP:True Positive，预测结果为正类，且与事实相符，即事实为正类。</li><li>TN:True Negative，预测结果为负类，且与事实相符，即事实为负类。</li><li>FP:False Positive，预测结果为正类，但与事实不符，即事实为负类。</li><li>FN:False Negative，预测结果为负类，但与事实不符，即事实为正类。</li></ul><p>常用的指标</p><ul><li>准确率（Accuracy）</li></ul><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/6.jpg" alt="" /><figcaption>准确率</figcaption></figure><ul><li>精确率（Precision），又叫查准率</li></ul><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/7.jpg" alt="" /><figcaption>查准率</figcaption></figure><ul><li>召回率（Recall），又叫查全率</li></ul><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/8.jpg" alt="" /><figcaption>查全率</figcaption></figure><h3 id="数据对算法结果的影响">数据对算法结果的影响</h3><h4 id="数据决定了算法的能力上限">数据决定了算法的能力上限</h4><blockquote><p>数据决定了模型能够达到的上限，而算法只是逼近这个上限。</p></blockquote><h4 id="特征工程">特征工程</h4><p>机器学习模型正是从这些特征中进行学习，特征有多少价值，机器才能学多少价值。</p><h2 id="第2章-机器学习所需的环境">第2章 机器学习所需的环境</h2><h3 id="python-简介">Python 简介</h3><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/9.jpg" alt="" /><figcaption>Python官网首页</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/10.jpg" alt="" /><figcaption>Python下载页面</figcaption></figure><p>安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip</span></span><br><span class="line">pip install library</span><br><span class="line"><span class="comment"># conda</span></span><br><span class="line">conda install library</span><br></pre></td></tr></table></figure><p>使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># library.a_class</span></span><br><span class="line"><span class="keyword">import</span> library</span><br><span class="line"><span class="comment"># lib.a_class</span></span><br><span class="line"><span class="keyword">import</span> libray <span class="keyword">as</span> lib</span><br><span class="line"><span class="comment"># a_class</span></span><br><span class="line"><span class="keyword">from</span> library <span class="keyword">import</span> a_class</span><br></pre></td></tr></table></figure><h3 id="numpy-简介">Numpy 简介</h3><p>Numpy是Python语言的科学计算支持库，提供了线性代数、傅里叶变换等非常有用的数学工具。Numpy是Python圈子里非常知名的基础库，即使你并不直接进行科学计算，但如图像处理等相关功能库，其底层实现仍需要数学工具进行支持，则需要首先安装Numpy库。</p><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/11.jpg" alt="" /><figcaption>Numpy官网首页</figcaption></figure><p>安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U numpy</span><br></pre></td></tr></table></figure><p>使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/12.jpg" alt="" /><figcaption>Numpy常用函数功能表</figcaption></figure><h3 id="scikit-learn-简介">Scikit-Learn 简介</h3><p>正如机器学习中推荐使用Python语言，用Python语言使用机器学习算法时，推荐使用Scikit-Learn工具，或者应该反过来，现在机器学习推荐使用Python，正是因为Python拥有Scikit-Learn这样功能强大的支持包，它已经把底层的脏活、累活都默默完成了，让使用者能够将宝贵的注意力和精力集中在解决问题上，极大地提高了产出效率。</p><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/13.jpg" alt="" /><figcaption>Scikit-Learn官网首页</figcaption></figure><p>安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip</span></span><br><span class="line">pip install -U scikit-learn</span><br><span class="line"><span class="comment"># conda</span></span><br><span class="line">conda install scikit-learn</span><br></pre></td></tr></table></figure><p>使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn</span><br></pre></td></tr></table></figure><p>调用机器学习算法也非常简单，Scikit-Learn库已经将算法按模型分类，查找起来非常方便。如线性回归算法可以从线性模型中找到，用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line">model = linear_model.LinearRegression()</span><br></pre></td></tr></table></figure><p>Logistic回归算法也是依据线性模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">model =linear_model.LogisticRegression()</span><br></pre></td></tr></table></figure><p>类似的还有基于近邻模型的KNN算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line">model =NearestNeighbors()</span><br></pre></td></tr></table></figure><p>生成模型后，一般使用fit方法给模型“喂”数据及进行训练。完成训练的模型可以使用predict方法进行预测。<br />Scikit-Learn库对机器学习算法进行了高度封装，使用过程非常简单，只要根据格式填入数据即可，不涉及额外的数学运算操作，甚至可以说只要知道机器学习算法的名字和优劣，就能直接使用，非常便利。</p><h3 id="pandas简介">Pandas简介</h3><p>Pandas是Python语言中知名的数据处理库。数据是模型算法的燃料，也决定了算法能够达到的上限。一般在学习中接触的数据都十分规整，可以直接供模型使用。但实际上，从生产环境中采集得到的“野生”数据则需要首先进行数据清洗工作，最常见的如填充丢失字段值。数据清洗工作一般使用Pandas来完成，前文所提到的特征工程也可通过Pandas完成。</p><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/14.jpg" alt="" /><figcaption>Pandas官网首页</figcaption></figure><p>安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip</span></span><br><span class="line">pip install -U pandas</span><br><span class="line"><span class="comment"># conda</span></span><br><span class="line">conda install pandas</span><br></pre></td></tr></table></figure><h4 id="pandas的基本用法">Pandas的基本用法</h4><p>Pandas针对数据处理的常用功能而设计，具有从不同格式的文件中读写数据的功能，使用Pandas进行一些统计操作特别便利。与Numpy类似，Pandas也有两个核心的数据类型，即Series和DataFrame。</p><ul><li>Series：一维数据，可以认为是一个统计功能增强版的List类型。</li><li>DataFrame：多维数据，由多个Series组成，不妨认为是电子表格里的Sheet。使用Pandas包很简单，只要import导入即可。业界习惯在导入时使用“pd”作为它的别名：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/15.jpg" alt="" /><figcaption>Pandas常用函数功能表</figcaption></figure><h2 id="第3章-线性回归算法">第3章 线性回归算法</h2><p>机器学习涉及的知识面很广，但总的来说有两条主线，</p><ul><li><strong>问题</strong></li><li><strong>模型</strong></li></ul><p><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/16.jpg" alt="16" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/17.jpg" alt="17" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/18.jpg" alt="18" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/19.jpg" alt="19" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/20.jpg" alt="20" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/21.jpg" alt="21" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/22.jpg" alt="22" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/23.jpg" alt="23" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/24.jpg" alt="24" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/25.jpg" alt="25" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/26.jpg" alt="26" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/27.jpg" alt="27" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/28.jpg" alt="28" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/29.jpg" alt="29" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/30.jpg" alt="30" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/31.jpg" alt="31" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/32.jpg" alt="32" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/33.jpg" alt="33" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/34.jpg" alt="34" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/35.jpg" alt="35" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/36.jpg" alt="36" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/37.jpg" alt="37" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/38.jpg" alt="38" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/39.jpg" alt="39" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/40.jpg" alt="40" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/41.jpg" alt="41" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/42.jpg" alt="42" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/43.jpg" alt="43" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/44.jpg" alt="44" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/45.jpg" alt="45" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/46.jpg" alt="46" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/47.jpg" alt="47" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/48.jpg" alt="48" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/49.jpg" alt="49" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/50.jpg" alt="50" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/51.jpg" alt="51" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/52.jpg" alt="52" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/53.jpg" alt="53" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/54.jpg" alt="54" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/55.jpg" alt="55" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/56.jpg" alt="56" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/57.jpg" alt="57" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/58.jpg" alt="58" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/59.jpg" alt="59" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/60.jpg" alt="60" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/61.jpg" alt="61" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/62.jpg" alt="62" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/63.jpg" alt="63" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/64.jpg" alt="64" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/65.jpg" alt="65" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/66.jpg" alt="66" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/67.jpg" alt="67" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/68.jpg" alt="68" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/69.jpg" alt="69" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/70.jpg" alt="70" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/71.jpg" alt="71" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/72.jpg" alt="72" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/73.jpg" alt="73" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/74.jpg" alt="74" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/75.jpg" alt="75" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/76.jpg" alt="76" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/77.jpg" alt="77" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/78.jpg" alt="78" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/79.jpg" alt="79" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/80.jpg" alt="80" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/81.jpg" alt="81" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/82.jpg" alt="82" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/83.jpg" alt="83" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/84.jpg" alt="84" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/85.jpg" alt="85" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/86.jpg" alt="86" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/87.jpg" alt="87" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/88.jpg" alt="88" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/89.jpg" alt="89" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/90.jpg" alt="90" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/91.jpg" alt="91" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/92.jpg" alt="92" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/93.jpg" alt="93" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/94.jpg" alt="94" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/95.jpg" alt="95" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/96.jpg" alt="96" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/97.jpg" alt="97" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/98.jpg" alt="98" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/99.jpg" alt="99" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/100.jpg" alt="100" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/101.jpg" alt="101" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/102.jpg" alt="102" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/103.jpg" alt="103" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/104.jpg" alt="104" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/105.jpg" alt="105" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/106.jpg" alt="106" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/107.jpg" alt="107" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/108.jpg" alt="108" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/109.jpg" alt="109" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/110.jpg" alt="110" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/111.jpg" alt="111" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/112.jpg" alt="112" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/113.jpg" alt="113" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/114.jpg" alt="114" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/115.jpg" alt="115" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/116.jpg" alt="116" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/117.jpg" alt="117" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/118.jpg" alt="118" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/119.jpg" alt="119" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/120.jpg" alt="120" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/121.jpg" alt="121" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/122.jpg" alt="122" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/123.jpg" alt="123" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/124.jpg" alt="124" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/125.jpg" alt="125" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/126.jpg" alt="126" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/127.jpg" alt="127" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/128.jpg" alt="128" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/129.jpg" alt="129" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/130.jpg" alt="130" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/131.jpg" alt="131" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/132.jpg" alt="132" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/133.jpg" alt="133" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/134.jpg" alt="134" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/135.jpg" alt="135" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/136.jpg" alt="136" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/137.jpg" alt="137" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/138.jpg" alt="138" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/139.jpg" alt="139" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/140.jpg" alt="140" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/141.jpg" alt="141" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/142.jpg" alt="142" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/143.jpg" alt="143" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/144.jpg" alt="144" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/145.jpg" alt="145" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/146.jpg" alt="146" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/147.jpg" alt="147" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/148.jpg" alt="148" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/149.jpg" alt="149" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/150.jpg" alt="150" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/151.jpg" alt="151" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/152.jpg" alt="152" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/153.jpg" alt="153" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/154.jpg" alt="154" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/155.jpg" alt="155" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/156.jpg" alt="156" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/157.jpg" alt="157" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/158.jpg" alt="158" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/159.jpg" alt="159" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/160.jpg" alt="160" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/161.jpg" alt="161" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/162.jpg" alt="162" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/163.jpg" alt="163" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/164.jpg" alt="164" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/165.jpg" alt="165" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/166.jpg" alt="166" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/167.jpg" alt="167" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/168.jpg" alt="168" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/169.jpg" alt="169" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/170.jpg" alt="170" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/171.jpg" alt="171" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/172.jpg" alt="172" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/173.jpg" alt="173" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/174.jpg" alt="174" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/175.jpg" alt="175" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/176.jpg" alt="176" /><br /><img src="https://2020.iosdevlog.com/2020/02/21/9787111642602/177.jpg" alt="177" /></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/21/9787111642602/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《机器学习算法的数学解析与Python实现》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：机器学习算法的数学解析与Python实现&lt;br /&gt;
作者：莫凡&lt;br /&gt;
出版社：机械工业出版社&lt;br /&gt;
出版时间：2020-01&lt;br /&gt;
ISBN：9787111642602&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="ML" scheme="https://2020.iosdevlog.com/tags/ML/"/>
    
      <category term="Math" scheme="https://2020.iosdevlog.com/tags/Math/"/>
    
      <category term="Python" scheme="https://2020.iosdevlog.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>hexo 数学公式 mathjax</title>
    <link href="https://2020.iosdevlog.com/2020/02/20/math/"/>
    <id>https://2020.iosdevlog.com/2020/02/20/math/</id>
    <published>2020-02-20T10:39:08.000Z</published>
    <updated>2020-02-20T14:30:52.289Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/20/math/1.png" alt="" /><figcaption>math</figcaption></figure><a id="more"></a><h2 id="更换渲染工具为-hexo-renderer-pandoc">更换渲染工具为 hexo-renderer-pandoc</h2><p>首先需要安装 pandoc</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install pandoc</span><br></pre></td></tr></table></figure><p>更换渲染工具为 hexo-renderer-pandoc</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure><h2 id="hexo-renderer-mathjax"><code>hexo-renderer-mathjax</code></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure><p>在hexo 博客中的 <code>_config.yml</code> 中添加 hexo-math 插件</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">math:</span></span><br><span class="line">  <span class="symbol">engine:</span> <span class="string">'mathjax'</span> <span class="comment"># or 'katex'</span></span><br><span class="line">  <span class="symbol">mathjax:</span></span><br><span class="line">    <span class="symbol">src:</span> custom_mathjax_source</span><br><span class="line">    <span class="symbol">config:</span></span><br><span class="line">      <span class="comment"># MathJax config</span></span><br><span class="line">  <span class="symbol">katex:</span></span><br><span class="line">    <span class="symbol">css:</span> custom_css_source</span><br><span class="line">    <span class="symbol">js:</span> custom_js_source <span class="comment"># not used</span></span><br><span class="line">    <span class="symbol">config:</span></span><br><span class="line">      <span class="comment"># KaTeX config</span></span><br></pre></td></tr></table></figure><h2 id="硬换行">硬换行</h2><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">pandoc:</span></span><br><span class="line">  <span class="symbol">extensions:</span></span><br><span class="line">    - <span class="string">"+hard_line_breaks"</span></span><br></pre></td></tr></table></figure><h2 id="打开-主题-的-mathjax-开关">打开 主题 的 mathjax 开关</h2><p><code>vim themes/landscape/_config.yml</code> 文件，找到 <code>mathjax</code> 位置, 设置为以下</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MathJax Support</span></span><br><span class="line"><span class="symbol">mathjax:</span></span><br><span class="line">  <span class="symbol">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="symbol">per_page:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="在每一个博客中都打开-mathjax-开关">在每一个博客中都打开 mathjax 开关</h2><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"><span class="symbol">title:</span> <span class="number">2020</span></span><br><span class="line"><span class="symbol">date:</span> <span class="number">2020</span>-<span class="number">02</span>-<span class="number">20</span> <span class="number">18</span><span class="symbol">:</span><span class="number">39</span><span class="symbol">:</span>08</span><br><span class="line"><span class="symbol">tags:</span></span><br><span class="line">    - tag1</span><br><span class="line">    - tag2</span><br><span class="line"><span class="symbol">categories:</span></span><br><span class="line">    - parent</span><br><span class="line">    - child</span><br><span class="line"><span class="symbol">mathjax:</span> <span class="literal">true</span></span><br><span class="line">---</span><br></pre></td></tr></table></figure><h2 id="重新生成">重新生成</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo generate</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><h2 id="测试">测试</h2><p><span class="math inline">\(h(\theta) = \alpha\)</span></p><p><span class="math display">\[\begin{array}{l}{u_{i} \leftarrow\left[u_{i}-\gamma_{t}\left(\lambda-\left(y_{t}-w^{\top} \Phi\left(x_{t}\right)\right) \Phi_{i}\left(x_{t}\right)\right)\right]_{+}} \\{v_{i} \leftarrow\left[v_{i}-\gamma_{t}\left(\lambda+\left(y_{t}-w_{t}^{\top} \Phi\left(x_{t}\right)\right) \Phi_{i}\left(x_{t}\right)\right)\right]_{+}}\end{array}\]</span></p><p><span class="math display">\[f^{\prime}\left(x_{0}\right)=\lim _{\Delta x \rightarrow 0} \frac{f\left(x_{0}+\Delta x\right)-f\left(x_{0}\right)}{\Delta x}\]</span></p><p><span class="math display">\[\begin{equation}w \leftarrow w-\gamma_{t}\left\{\begin{array}{ll}{\lambda w} &amp; {\text { if } y_{t} w^{\top} \Phi\left(x_{t}\right)&gt;1} \\{\lambda w-y_{t} \Phi\left(x_{t}\right)} &amp; {\text { otherwise }}\end{array}\right.\end{equation}\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/20/math/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;math&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="math" scheme="https://2020.iosdevlog.com/categories/math/"/>
    
    
      <category term="hexo" scheme="https://2020.iosdevlog.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>《大国崛起的新政治经济学》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/02/20/9787220098970/"/>
    <id>https://2020.iosdevlog.com/2020/02/20/9787220098970/</id>
    <published>2020-02-20T09:30:50.000Z</published>
    <updated>2020-02-20T14:07:00.017Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/20/9787220098970/1.jpg" alt="" /><figcaption>《大国崛起的新政治经济学》</figcaption></figure><p>作者：聂永有，殷凤等<br />出版社：四川人民出版社<br />出版时间：2016-09<br />ISBN：9787220098970</p><a id="more"></a><h2 id="中共十八届五中全会提出了五大发展理念">中共十八届五中全会，提出了五大发展理念</h2><ul><li>创新</li><li>协调</li><li>绿色</li><li>开放</li><li>共享</li></ul><p>500年来，在这个充满希望而又遍布危机的发展舞台上，相继出现了9个世界性大国</p><p>葡萄牙、西班牙、荷兰、</p><p>英国（工业革命）、法国</p><p>后来居上的</p><p>德国、日本、俄罗斯与美国</p><h2 id="拉动经济增长的三驾马车">拉动经济增长的三驾马车</h2><ol type="1"><li>消费</li><li>投资</li><li>净出口</li></ol><h2 id="为什么必须由市场而不是政府来决定资源的配置">为什么必须由市场而不是政府来决定资源的配置？</h2><p>这是由市场本身所具有的特定功能决定的。</p><ol type="1"><li>市场价格能够有效协调商品供求关系。</li><li>市场价格的自由涨跌可以有效引导资源配置。</li><li>市场价格机制能够有效提高经济效率。</li></ol><h2 id="政府的经济职能可概括为三个方面">政府的经济职能可概括为三个方面</h2><ol type="1"><li>提升经济效率，如管制行业垄断和不正当竞争行为，解决外部性问题，保障公共物品的供给和改善信息不对称问题等，也就是解决市场失灵问题。</li><li>促进社会公平，一般认为，市场能够有效促进效率，甚至产生马太效应，恶化社会公平问题，这时需要政府适当的干预，采用诸如税收和政府转移支付等收入再分配政策缩小贫富差异，改善这个社会的收入分配状况。</li><li>维持宏观经济稳定。</li></ol><h2 id="经济学家认为能影响经济增长率长期变动的因素可分为七个">经济学家认为，能影响经济增长率长期变动的因素可分为七个：</h2><ol type="1"><li>就业人数和年龄、性别构成；</li><li>包括非全日制工人在内的工时数；</li><li>就业人员的受教育程度；</li><li>资本存量；</li><li>资源配置改善；</li><li>规模经济的程度；</li><li>知识进步。</li></ol><p>其中，前四项可归结为生产要素的供给增长（前三项为劳动要素的增长，第四项为资本要素的增长）；<br />后三项是生产要素的生产率增长，也就是技术进步的贡献。</p><h2 id="大国创新之路有哪些共同规律">大国创新之路有哪些共同规律？</h2><ol type="1"><li>创新是大国崛起的重要驱动力。</li><li>创新成果转化和创新同等重要。</li><li>结合自身国情和发展阶段，采取适合本国的创新模式非常重要。</li><li>世界科技革命和产业大变革通常是新兴大国崛起的重要历史机遇。</li></ol><h2 id="创新需要怎样的生态环境">创新需要怎样的生态环境？</h2><ol type="1"><li>激励创新的制度，是创新的根本保证。</li><li>激励创新的金融支撑手段，是创新的重要推进剂。</li></ol><h2 id="中国产业发展和结构中存在哪些问题">中国产业发展和结构中存在哪些问题？</h2><ol type="1"><li>产能过剩。</li><li>现有产业规模较大但是实力不强。</li><li>第三产业增长非常快，但行业结构并不合理，发展水平滞后。</li><li>外向型产业大多处于全球产业链低端。</li></ol><h2 id="产业转型与升级的路在何方">产业转型与升级的路在何方？</h2><ol type="1"><li>化解产能过剩危机。</li><li>要加强企业自主创新，促进技术升级。</li><li>要结合地区比较优势，加快产业转移与承接，实现产业雁行模式的梯度发展。</li><li>在产业选择方面，要重点发展包括先进制造业在内的新兴产业，并促成新兴科技与传统产业的有机融合，实现新技术、新产品和新业态的发展。</li><li>完善和提升产业价值链。</li></ol><h2 id="如何发挥产业政策在中国产业转型升级中的作用">如何发挥产业政策在中国产业转型升级中的作用？</h2><ol type="1"><li>促进产业结构调整，</li><li>对产能过剩问题的关注和解决。</li></ol><h2 id="中国式的市场失灵是如何产生的">中国式的市场失灵是如何产生的？</h2><ol type="1"><li>从体制上来看，现行的行政管理体制、财税体制并不利于资源和环境的管理，也不利于资源税的改革。</li><li>从法律角度来看，中国并不缺少资源与环境保护的相关法律，但执行不力。</li><li>从经济角度来看，“中国式的市场失灵”更多是市场本身的问题。</li><li>资源与环境不能完全依靠市场，市场失灵在某种程度上是客观存在的，</li></ol><h2 id="如何跨越增长的极限实现绿色经济的转型">如何跨越增长的极限，实现绿色经济的转型？</h2><ol type="1"><li>中国需要建立与资源环境管理相适应的财税体系和法律制度。</li><li>要从全球的、多维度的、战略的眼光来看待资源治理与经济增长的关系。</li><li>中国必须通过绿色科技创新来跨越增长的极限。</li></ol><h2 id="开放历程大体经历了以下三个时期">开放历程大体经历了以下三个时期：</h2><ul><li>被动式对外开放<ul><li>从鸦片战争到新中国建立的109年间</li></ul></li><li>一边倒式的对外开放<ul><li>孤立封锁政策</li><li>“另起炉灶”和“一边倒”</li></ul></li><li>主动的对外开放。<ul><li>第一阶段：1978—1991年<ul><li>这是以沿海地区开放为重点的探索开放阶段，以重点开放沿海地区，建立经济特区并实行特殊优惠政策为主要特征。</li></ul></li><li>第二阶段：1992－2000年<ul><li>对外开放加速向纵深推进，全方位开放格局基本形成，是建立有中国特色社会主义市场经济体制的阶段。</li></ul></li><li>第三阶段：2001年至今<ul><li>经历了15年曲折与漫长的谈判后，2001年12月11日，我国正式成为世界贸易组织（WTO）成员。</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/20/9787220098970/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《大国崛起的新政治经济学》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;作者：聂永有，殷凤等&lt;br /&gt;
出版社：四川人民出版社&lt;br /&gt;
出版时间：2016-09&lt;br /&gt;
ISBN：9787220098970&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="中国" scheme="https://2020.iosdevlog.com/tags/%E4%B8%AD%E5%9B%BD/"/>
    
  </entry>
  
  <entry>
    <title>《线性代数的几何意义》读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/02/20/algebra/"/>
    <id>https://2020.iosdevlog.com/2020/02/20/algebra/</id>
    <published>2020-02-20T07:12:11.000Z</published>
    <updated>2020-02-20T14:00:48.509Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/20/algebra/1.jpg" alt="" /><figcaption>algebra</figcaption></figure><p>作者: 任广千 / 谢聪 / 胡翠芳<br />出版社: 西安电子科技大学出版社<br />副标题: 图解线性代数<br />出版年: 2015-7-15<br />页数: 280<br />定价: 46.00元<br />装帧: 平装、四色印刷<br />ISBN: 9787560634548</p><a id="more"></a><p>代数英文是 <code>Algebra</code>，源于阿拉伯语，其本意是“结合在一起”的意思。</p><p>也就是说代数的功能是把 许多看似不相关的事物“结合在一起”，也就是进行抽象。</p><p>线性函数表现为直线，这只是几何意义。那么所谓“线性”的代数意义是什么呢?实际上，最基本 的意义只有两条:可加性和比例性。</p><ul><li>可加性: 即如果函数 <span class="math inline">\(f(x)\)</span> 是线性的，那么有:</li></ul><p><span class="math display">\[f(x1 +x2)= f(x1)+ f(x2)\]</span></p><p>一句话:和的函数等于函数的和。</p><ul><li>比例性: 也叫做齐次性、数乘性或均匀性，即如果函数 f (x)是线性的，那么有</li></ul><p><span class="math display">\[f(kx)=kf(x) \]</span></p><p>其中k是常数。</p><p>一句话:比例的函数等于函数的比例;或者说自变量缩放，函数也同等比例地缩放。</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/20/algebra/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;algebra&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;作者: 任广千 / 谢聪 / 胡翠芳&lt;br /&gt;
出版社: 西安电子科技大学出版社&lt;br /&gt;
副标题: 图解线性代数&lt;br /&gt;
出版年: 2015-7-15&lt;br /&gt;
页数: 280&lt;br /&gt;
定价: 46.00元&lt;br /&gt;
装帧: 平装、四色印刷&lt;br /&gt;
ISBN: 9787560634548&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="Math" scheme="https://2020.iosdevlog.com/tags/Math/"/>
    
      <category term="Algebra" scheme="https://2020.iosdevlog.com/tags/Algebra/"/>
    
  </entry>
  
  <entry>
    <title>《东野圭吾作品：11字谜案》人物关系图</title>
    <link href="https://2020.iosdevlog.com/2020/02/19/9787020156047/"/>
    <id>https://2020.iosdevlog.com/2020/02/19/9787020156047/</id>
    <published>2020-02-19T03:34:10.000Z</published>
    <updated>2020-02-19T09:59:27.736Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/19/9787020156047/1.svg" alt="" /><figcaption>《东野圭吾作品：11字谜案》人物关系图</figcaption></figure><a id="more"></a><figure><img src="https://2020.iosdevlog.com/2020/02/19/9787020156047/2.jpg" alt="" /><figcaption>《东野圭吾作品：11字谜案》</figcaption></figure><p>东野圭吾作品：11字谜案<br />作者：[日]东野圭吾<br />译者：羊恩媺<br />出版社：人民文学出版社<br />出版时间：2020-01<br />ISBN：9787020156047</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/19/9787020156047/1.svg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《东野圭吾作品：11字谜案》人物关系图&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="东野圭吾" scheme="https://2020.iosdevlog.com/tags/%E4%B8%9C%E9%87%8E%E5%9C%AD%E5%90%BE/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch iOS</title>
    <link href="https://2020.iosdevlog.com/2020/02/18/PyTorch-iOS/"/>
    <id>https://2020.iosdevlog.com/2020/02/18/PyTorch-iOS/</id>
    <published>2020-02-18T14:30:04.000Z</published>
    <updated>2020-02-20T14:05:37.315Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/18/PyTorch-iOS/15.png" alt="" /><figcaption>小武</figcaption></figure><a id="more"></a><h2 id="xcode-reset">Xcode Reset</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">default delete com.apple.Xcode</span><br></pre></td></tr></table></figure><h2 id="ui">UI</h2><h2 id="cocoapods">CocoaPods</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pod init</span><br><span class="line">cat Podfile</span><br></pre></td></tr></table></figure><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Uncomment the next line to define a global platform for your project</span></span><br><span class="line">platform <span class="symbol">:ios</span>, <span class="string">'12.0'</span></span><br><span class="line"></span><br><span class="line">target <span class="string">'PyTorch_iOS'</span> <span class="keyword">do</span></span><br><span class="line">  <span class="comment"># Comment the next line if you don't want to use dynamic frameworks</span></span><br><span class="line">  use_frameworks!</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Pods for PyTorch_iOS</span></span><br><span class="line">  pod <span class="string">'LibTorch'</span>, <span class="string">'~&gt; 1.4.0'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="model">model</h2><p><code>trace_model.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">model = torchvision.models.mobilenet_v2(pretrained=<span class="literal">True</span>)</span><br><span class="line">model.eval()</span><br><span class="line">example = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">traced_script_module = torch.jit.trace(model, example)</span><br><span class="line">traced_script_module.save(<span class="string">"model.pt"</span>)</span><br></pre></td></tr></table></figure><p>模型</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget -c https://download.pytorch.org/models/mobilenet_v2-b0353104.pth</span><br><span class="line">cp mobilenet_v2-b0353104.pth /Users/iosdevlog/.cache/torch/checkpoints/mobilenet_v2-b0353104.pth</span><br><span class="line">python trace_model.py</span><br></pre></td></tr></table></figure><p>拖动生成的 <code>model.pt</code> 到 <code>iOS</code> 项目。</p><h2 id="拍照相册">拍照/相册</h2><p><code>Info.plist</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">key</span>&gt;</span>NSCameraUsageDescription<span class="tag">&lt;/<span class="name">key</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">string</span>&gt;</span>Camera Usage Description<span class="tag">&lt;/<span class="name">string</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">key</span>&gt;</span>NSPhotoLibraryUsageDescription<span class="tag">&lt;/<span class="name">key</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">string</span>&gt;</span>Photo Library Usage Description<span class="tag">&lt;/<span class="name">string</span>&gt;</span></span><br></pre></td></tr></table></figure><p>拍照/相册选择图片</p><h2 id="官文教程"><img src="https://pytorch.org/mobile/ios/" alt="官文教程" /></h2><p>要在 <code>iOS</code> 上开始使用 <code>PyTorch</code>，我们建议您浏览以下<a href="https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld" target="_blank" rel="noopener">HelloWorld</a>。</p><h2 id="hello-world示例快速入门">HELLO WORLD示例快速入门<a href="https://pytorch.org/mobile/ios/#quickstart-with-a-hello-world-example" target="_blank" rel="noopener"></a></h2><p>HelloWorld是一个简单的图像分类应用程序，演示了如何在iOS上使用PyTorch C ++库。该代码用Swift编写，并使用Objective-C作为桥梁。</p><h3 id="模型准备">模型准备<a href="https://pytorch.org/mobile/ios/#model-preparation" target="_blank" rel="noopener"></a></h3><p>让我们从模型准备开始。如果您熟悉PyTorch，您可能应该已经知道如何训练和保存模型。如果您没有，我们将使用预先训练的图像分类模型<a href="https://pytorch.org/hub/pytorch_vision_mobilenet_v2/" target="_blank" rel="noopener">-MobileNet v2</a>，该模型已经包装在<a href="https://pytorch.org/docs/stable/torchvision/index.html" target="_blank" rel="noopener">TorchVision中</a>。要安装它，请运行以下命令。</p><blockquote><p>我们强烈建议您遵循<a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener">Pytorch Github页面</a>在本地计算机上设置Python开发环境。</p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchvision</span><br></pre></td></tr></table></figure><p>成功安装TorchVision后，让我们导航到HelloWorld文件夹并运行<code>trace_model.py</code>。该脚本包含跟踪和保存可在移动设备上运行的<a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html" target="_blank" rel="noopener">Torchscript模型</a>的代码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python trace_model.py</span><br></pre></td></tr></table></figure><p>如果一切正常，我们应该<code>model.pt</code>在<code>HelloWorld</code>文件夹中生成模型。现在将模型文件复制到我们的应用程序文件夹中<code>HelloWorld/model</code>。</p><blockquote><p>要了解有关TorchScript的更多详细信息，请访问<a href="https://pytorch.org/tutorials/advanced/cpp_export.html" target="_blank" rel="noopener">pytorch.org上的教程。</a></p></blockquote><h3 id="通过cocoapods安装libtorch">通过Cocoapods安装LibTorch<a href="https://pytorch.org/mobile/ios/#install-libtorch-via-cocoapods" target="_blank" rel="noopener"></a></h3><p>PyTorch C++库在<a href="https://cocoapods.org/" target="_blank" rel="noopener">Cocoapods中</a>可用，可以将其集成到我们的项目中，只需运行即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pod install</span><br></pre></td></tr></table></figure><p>现在是时候<code>HelloWorld.xcworkspace</code>在XCode中打开，选择一个iOS模拟器并启动它（cmd + R）。如果一切正常，我们应该在模拟器屏幕上看到狼的图片以及预测结果。</p><p>我已经加了一张新图片。</p><figure><img src="https://2020.iosdevlog.com/2020/02/18/PyTorch-iOS/Simulator.png" alt="" /><figcaption>iOS 模拟器</figcaption></figure><h3 id="代码演练">代码演练<a href="https://pytorch.org/mobile/ios/#code-walkthrough" target="_blank" rel="noopener"></a></h3><p>在这一部分中，我们将逐步介绍代码。</p><h4 id="图片载入">图片载入<a href="https://pytorch.org/mobile/ios/#image-loading" target="_blank" rel="noopener"></a></h4><p>让我们从图像加载开始。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> image = <span class="type">UIImage</span>(named: <span class="string">"image.jpg"</span>)!</span><br><span class="line">imageView.image = image</span><br><span class="line"><span class="keyword">let</span> resizedImage = image.resized(to: <span class="type">CGSize</span>(width: <span class="number">224</span>, height: <span class="number">224</span>))</span><br><span class="line"><span class="keyword">guard</span> <span class="keyword">var</span> pixelBuffer = resizedImage.normalized() <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们首先从包中加载图像，然后将其调整为224x224。然后，我们将此<code>normalized()</code>类别方法称为归一化像素缓冲区。让我们仔细看看下面的代码。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> normalizedBuffer: [<span class="type">Float32</span>] = [<span class="type">Float32</span>](repeating: <span class="number">0</span>, <span class="built_in">count</span>: w * h * <span class="number">3</span>)</span><br><span class="line"><span class="comment">// normalize the pixel buffer</span></span><br><span class="line"><span class="comment">// see https://pytorch.org/hub/pytorch_vision_resnet/ for more detail</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0</span> ..&lt; w * h &#123;</span><br><span class="line">    normalizedBuffer[i]             = (<span class="type">Float32</span>(rawBytes[i * <span class="number">4</span> + <span class="number">0</span>]) / <span class="number">255.0</span> - <span class="number">0.485</span>) / <span class="number">0.229</span> <span class="comment">// R</span></span><br><span class="line">    normalizedBuffer[w * h + i]     = (<span class="type">Float32</span>(rawBytes[i * <span class="number">4</span> + <span class="number">1</span>]) / <span class="number">255.0</span> - <span class="number">0.456</span>) / <span class="number">0.224</span> <span class="comment">// G</span></span><br><span class="line">    normalizedBuffer[w * h * <span class="number">2</span> + i] = (<span class="type">Float32</span>(rawBytes[i * <span class="number">4</span> + <span class="number">2</span>]) / <span class="number">255.0</span> - <span class="number">0.406</span>) / <span class="number">0.225</span> <span class="comment">// B</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>乍一看，这些代码可能看起来很奇怪，但是一旦我们理解了模型，它就会变得有意义。输入数据是形状为（3 x H x W）的3通道RGB图像，其中H和W至少应为224。图像必须加载到的范围内<code>[0, 1]</code>，然后使用<code>mean = [0.485, 0.456, 0.406]</code>和进行归一化<code>std = [0.229, 0.224, 0.225]</code>。</p><h4 id="torchscript模块">TorchScript模块<a href="https://pytorch.org/mobile/ios/#torchscript-module" target="_blank" rel="noopener"></a></h4><p>现在我们已经对输入数据进行了预处理，并且有了预先训练的TorchScript模型，下一步就是使用它们来运行谓词。为此，我们首先将模型加载到应用程序中。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="built_in">lazy</span> <span class="keyword">var</span> module: <span class="type">TorchModule</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">let</span> filePath = <span class="type">Bundle</span>.main.path(forResource: <span class="string">"model"</span>, ofType: <span class="string">"pt"</span>),</span><br><span class="line">        <span class="keyword">let</span> module = <span class="type">TorchModule</span>(fileAtPath: filePath) &#123;</span><br><span class="line">        <span class="keyword">return</span> module</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">fatalError</span>(<span class="string">"Can't find the model file!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;()</span><br></pre></td></tr></table></figure><p>请注意，<code>TorchModule</code>该类是的Objective-C包装器<code>torch::jit::script::Module</code>。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch::jit::script::<span class="type">Module</span> module = torch::jit::load(filePath.<span class="type">UTF8String</span>);</span><br></pre></td></tr></table></figure><p>由于Swift无法直接与C ++对话，因此我们必须使用Objective-C类作为桥梁，或者为C ++库创建C包装器。出于演示目的，我们将把所有内容包装在这个Objective-C类中。但是，我们正在努力为PyTorch提供Swift / Objective-C API包装器。敬请关注！</p><h4 id="运行推断">运行推断<a href="https://pytorch.org/mobile/ios/#run-inference" target="_blank" rel="noopener"></a></h4><p>现在该进行推断并获取结果了。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">guard</span> <span class="keyword">let</span> outputs = module.predict(image: <span class="type">UnsafeMutableRawPointer</span>(&amp;pixelBuffer)) <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同样，该<code>predict</code>方法只是一个Objective-C包装器。在后台，它调用C ++ <code>forward</code>函数。让我们看一下它是如何实现的。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">at::<span class="type">Tensor</span> tensor = torch::from_blob(imageBuffer, &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;, at::kFloat);</span><br><span class="line">torch::autograd::<span class="type">AutoGradMode</span> <span class="keyword">guard</span>(<span class="literal">false</span>);</span><br><span class="line">auto outputTensor = _impl.forward(&#123;tensor&#125;).toTensor();</span><br><span class="line">float* floatBuffer = outputTensor.data_ptr&lt;float&gt;();</span><br></pre></td></tr></table></figure><p>C ++函数<code>torch::from_blob</code>将从像素缓冲区创建输入张量。请注意，张量的形状<code>{1,3,224,224}</code>代表<code>NxCxWxH</code>我们在上一节中讨论的形状。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch::autograd::<span class="type">AutoGradMode</span> <span class="keyword">guard</span>(<span class="literal">false</span>);</span><br><span class="line">at::<span class="type">AutoNonVariableTypeMode</span> non_var_type_mode(<span class="literal">true</span>);</span><br></pre></td></tr></table></figure><p>以上两行告诉PyTorch引擎仅进行推断。这是因为默认情况下，PyTorch内置了对进行自动分化的支持，这也称为<a href="https://pytorch.org/docs/stable/notes/autograd.html" target="_blank" rel="noopener">autograd</a>。由于我们不进行手机培训，因此我们可以禁用自动毕业模式。</p><p>最后，我们可以调用此<code>forward</code>函数以获取输出张量并将其转换为<code>float</code>缓冲区。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">auto outputTensor = _impl.forward(&#123;tensor&#125;).toTensor();</span><br><span class="line">float* floatBuffer = outputTensor.data_ptr&lt;float&gt;();</span><br></pre></td></tr></table></figure><h3 id="收集结果">收集结果<a href="https://pytorch.org/mobile/ios/#collect-results" target="_blank" rel="noopener"></a></h3><p>输出张量是形状为1x1000的一维浮点数组，其中每个值表示从图像预测标签的置信度。下面的代码对数组进行排序，并检索前三个结果。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> zippedResults = <span class="built_in">zip</span>(labels.<span class="built_in">indices</span>, outputs)</span><br><span class="line"><span class="keyword">let</span> sortedResults = zippedResults.sorted &#123; $<span class="number">0.1</span>.floatValue &gt; $<span class="number">1.1</span>.floatValue &#125;.<span class="keyword">prefix</span>(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h3 id="pytorch演示应用">PyTorch演示应用<a href="https://pytorch.org/mobile/ios/#pytorch-demo-app" target="_blank" rel="noopener"></a></h3><p>对于更复杂的用例，我们建议您检查<a href="https://github.com/pytorch/ios-demo-app" target="_blank" rel="noopener">PyTorch演示应用程序</a>。该演示应用程序包含两个展示柜。一个运行量化模型的相机应用程序，可以实时预测来自设备后置相机的图像。还有一个基于文本的应用程序，它使用文本分类模型来根据输入字符串预测主题。</p><h2 id="从源代码构建pytorch-ios库">从源代码构建PYTORCH IOS库<a href="https://pytorch.org/mobile/ios/#build-pytorch-ios-libraries-from-source" target="_blank" rel="noopener"></a></h2><p>要跟踪iOS的最新更新，您可以从源代码构建PyTorch iOS库。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https:<span class="comment">//github.com/pytorch/pytorch</span></span><br><span class="line">cd pytorch</span><br><span class="line"># <span class="keyword">if</span> you are updating an existing checkout</span><br><span class="line">git submodule sync</span><br><span class="line">git submodule update --<span class="keyword">init</span> --recursive</span><br></pre></td></tr></table></figure><blockquote><p>确保已<code>cmake</code>在本地计算机上正确安装了Python。我们建议您遵循<a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener">Pytorch Github页面</a>来设置Python开发环境</p></blockquote><h3 id="为ios模拟器构建libtorch">为iOS模拟器构建LibTorch<a href="https://pytorch.org/mobile/ios/#build-libtorch-for-ios-simulators" target="_blank" rel="noopener"></a></h3><p>打开终端并导航到PyTorch根目录。运行以下命令</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BUILD_PYTORCH_MOBILE=1 IOS_PLATFORM=SIMULATOR ./scripts/build_ios.sh</span><br></pre></td></tr></table></figure><p>构建成功后，所有静态库和头文件将在 <code>build_ios/install</code></p><h3 id="为arm64设备构建libtorch">为arm64设备构建LibTorch<a href="https://pytorch.org/mobile/ios/#build-libtorch-for-arm64-devices" target="_blank" rel="noopener"></a></h3><p>打开终端并导航到PyTorch根目录。运行以下命令</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 ./scripts/build_ios.sh</span><br></pre></td></tr></table></figure><p>构建成功后，所有静态库和头文件将在 <code>build_ios/install</code></p><h3 id="xcode设置">XCode设置<a href="https://pytorch.org/mobile/ios/#xcode-setup" target="_blank" rel="noopener"></a></h3><p>在XCode中打开您的项目，将所有静态库以及头文件复制到您的项目中。导航到项目设置，将“ <strong>Header Search Paths</strong> ”值设置为刚复制的头文件的路径。</p><p>在构建设置中，搜索<strong>其他链接器标志</strong>。在下面添加自定义链接器标志</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-force_load $(PROJECT_DIR)/<span class="variable">$&#123;path-to-libtorch.a&#125;</span></span><br></pre></td></tr></table></figure><p>最后，通过选择“构建设置”，搜索“ <strong>启用位码”</strong>，然后将值设置为<strong>No</strong>，为目标禁用位码。</p><h3 id="api文件">API文件<a href="https://pytorch.org/mobile/ios/#api-docs" target="_blank" rel="noopener"></a></h3><p>当前，iOS框架直接使用Pytorch C ++前端API。可以在<a href="https://pytorch.org/cppdocs/" target="_blank" rel="noopener">这里</a>找到C ++文档。要了解更多信息，我们建议在PyTorch网页上浏览<a href="https://pytorch.org/tutorials/advanced/cpp_frontend.html" target="_blank" rel="noopener">C ++前端教程</a>。同时，我们正在努力为PyTorch提供Swift / Objective-C API包装器。</p><h3 id="定制版">定制版<a href="https://pytorch.org/mobile/ios/#custom-build" target="_blank" rel="noopener"></a></h3><p>从1.4.0开始，PyTorch支持自定义构建。现在，您可以构建PyTorch库，其中仅包含模型所需的运算符。为此，请按照以下步骤操作</p><p>1.确认您的PyTorch版本为1.4.0或更高版本。您可以通过检查的值来实现<code>torch.__version__</code>。</p><p>2.要转储模型中的运算符，请说<code>MobileNetV2</code>运行以下几行Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch, yaml</span><br><span class="line">model = torch.jit.load(<span class="string">'MobileNetV2.pt'</span>)</span><br><span class="line">ops = torch.jit.export_opnames(model)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'MobileNetV2.yaml'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> output:</span><br><span class="line">    yaml.dump(ops, output)</span><br></pre></td></tr></table></figure><p>在上面的代码段中，您首先需要加载ScriptModule。然后，使用<code>export_opnames</code>来返回ScriptModule及其子模块的运算符名称的列表。最后，将结果保存在yaml文件中。</p><p>3.要使用准备好的yaml运算符列表在本地运行iOS构建脚本，请将从最后一步生成的yaml文件传递到环境变量中<code>SELECTED_OP_LIST</code>。同样在自变量中，指定<code>BUILD_PYTORCH_MOBILE=1</code>以及平台/架构类型。以arm64构建为例，命令应为：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECTED_OP_LIST=MobileNetV2.yaml BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 ./scripts/build_ios.sh</span><br></pre></td></tr></table></figure><p>4.构建成功后，您可以按照上面的<a href="https://pytorch.org/mobile/ios/#xcode-setup" target="_blank" rel="noopener">XCode Setup</a>部分将结果库集成到项目中。</p><p>5.最后一步是在运行之前添加一行C ++代码<code>forward</code>。这是因为默认情况下，JIT将对运算符进行一些优化（例如，融合），这可能会破坏我们从模型中转储的操作的一致性。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch::jit::<span class="function">GraphOptimizerEnabledGuard <span class="title">guard</span><span class="params">(<span class="literal">false</span>)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="问题与贡献">问题与贡献<a href="https://pytorch.org/mobile/ios/#issues-and-contribution" target="_blank" rel="noopener"></a></h2><p>如果您有任何疑问或想为PyTorch做出贡献，请随时提出问题或打开请求请求以取得联系。</p><p>PyTorch iOS 官方：</p><p><a href="https://github.com/pytorch/ios-demo-app" target="_blank" rel="noopener" class="uri">https://github.com/pytorch/ios-demo-app</a></p><p>带拍照和相册的源码：</p><p><a href="https://github.com/Game2020/PyTorch_iOS" target="_blank" rel="noopener" class="uri">https://github.com/Game2020/PyTorch_iOS</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/18/PyTorch-iOS/15.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;小武&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="DL" scheme="https://2020.iosdevlog.com/categories/DL/"/>
    
    
      <category term="PyTorch" scheme="https://2020.iosdevlog.com/tags/PyTorch/"/>
    
      <category term="iOS" scheme="https://2020.iosdevlog.com/tags/iOS/"/>
    
  </entry>
  
  <entry>
    <title>《放学后》人物关系图</title>
    <link href="https://2020.iosdevlog.com/2020/02/18/9787544291224/"/>
    <id>https://2020.iosdevlog.com/2020/02/18/9787544291224/</id>
    <published>2020-02-17T16:22:29.000Z</published>
    <updated>2020-02-19T09:59:44.886Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/18/9787544291224/1.svg" alt="" /><figcaption>《放学后》人物关系图</figcaption></figure><a id="more"></a><figure><img src="https://2020.iosdevlog.com/2020/02/18/9787544291224/2.jpg" alt="" /><figcaption>《放学后》</figcaption></figure><p>书名：放学后<br />作者：[日]东野圭吾<br />译者：赵峻<br />出版社：南海出版公司出版<br />时间：2017-09<br />ISBN：9787544291224</p><h2 id="更衣室">更衣室</h2><figure><img src="https://2020.iosdevlog.com/2020/02/18/9787544291224/3.jpg" alt="" /><figcaption>更衣室简图</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/18/9787544291224/4.jpg" alt="" /><figcaption>门</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/18/9787544291224/5.jpg" alt="" /><figcaption>北条雅美 推理</figcaption></figure><h2 id="体育节">体育节</h2><table><thead><tr class="header"><th>时间</th><th>事件</th></tr></thead><tbody><tr class="odd"><td>14:15</td><td>　来宾、教职员趣味赛跑</td></tr><tr class="even"><td>14:30</td><td>　三人拉力赛（一年级）</td></tr><tr class="odd"><td>14:45</td><td>　师生对抗障碍赛</td></tr><tr class="even"><td>15:00</td><td>　创编舞（三年级）</td></tr><tr class="odd"><td>15:20</td><td>　化装游行（运动社团）</td></tr></tbody></table><h2 id="更衣室解迷">更衣室解迷</h2><figure><img src="https://2020.iosdevlog.com/2020/02/18/9787544291224/6.jpg" alt="" /><figcaption>更衣室解迷</figcaption></figure><figure><img src="https://2020.iosdevlog.com/2020/02/18/9787544291224/7.jpg" alt="" /><figcaption>更衣室解迷</figcaption></figure><h2 id="动机">动机</h2><blockquote><p>“对她们来说，最重要的应该是美丽、纯粹、真实的东西，比如友情、爱情，也可能是自己的身体或容貌。很多时候，更抽象的回忆或梦想对她们来说也很重要。反过来说，她们最憎恨企图破坏或者从她们手中夺</p></blockquote><ol type="1"><li>自慰被看到的惠美<ul><li>惠美&amp;惠子：杀害看到学生自慰的老师</li></ul></li><li>“他根本不知道头发被剪得乱七八糟对我来说有多痛苦。”-阳子<ul><li>阳子：试图诬陷教导主任性骚扰</li></ul></li><li>“我没有给过她任何东西,甚至一直都是从她身上予取予求,更夺走了她的自由、快乐, 以及孩子。” 冷漠的丈夫和婚姻给她的精神带来的创痛。-裕美子<ul><li>裕美子&amp;情夫：杀害前岛的</li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/18/9787544291224/1.svg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《放学后》人物关系图&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="东野圭吾" scheme="https://2020.iosdevlog.com/tags/%E4%B8%9C%E9%87%8E%E5%9C%AD%E5%90%BE/"/>
    
  </entry>
  
  <entry>
    <title>《Python深度学习：基于PyTorch》 读书笔记</title>
    <link href="https://2020.iosdevlog.com/2020/02/17/pytorch/"/>
    <id>https://2020.iosdevlog.com/2020/02/17/pytorch/</id>
    <published>2020-02-17T05:28:39.000Z</published>
    <updated>2020-02-17T15:42:20.233Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/1.jpg" alt="" /><figcaption>《Python深度学习：基于PyTorch》</figcaption></figure><p>书名：Python深度学习：基于PyTorch<br />作者：吴茂贵，郁明敏，杨本法，李涛，张粤磊<br />出版社：机械工业出版社<br />出版时间：2019-10<br />ISBN：9787111637172</p><a id="more"></a><p>建议初学者选择PyTorch的主要依据是：</p><ol type="1"><li>PyTorch是动态计算图，其用法更贴近Python，并且，PyTorch与Python共用了许多Numpy的命令，可以降低学习的门槛，比TensorFlow更容易上手。</li><li>PyTorch需要定义网络层、参数更新等关键步骤，这非常有助于理解深度学习的核心；而Keras虽然也非常简单，且容易上手，但封装粒度很粗，隐藏了很多关键步骤。</li><li>PyTorch的动态图机制在调试方面非常方便，如果计算图运行出错，马上可以跟踪问题。PyTorch的调试与Python的调试一样，通过断点检查就可以高效解决问题。</li><li>PyTorch的流行度仅次于TensorFlow。而最近一年，在GitHub关注度和贡献者的增长方面，PyTorch跟TensorFlow基本持平。PyTorch的搜索热度持续上涨，加上FastAI的支持，PyTorch将受到越来越多机器学习从业者的青睐。</li></ol><p>本书特点</p><ul><li>内容选择<ul><li>广泛涉猎</li><li>精讲</li><li>注重实战</li></ul></li><li>内容安排<ul><li>简单实例开始</li><li>循序渐进</li></ul></li><li>表达形式<ul><li>让图说话</li><li>一张好图胜过千言万语</li></ul></li></ul><p>本书内容</p><ol type="1"><li>PyTorch基础</li><li>深度学习基本原理</li><li>实战部分</li></ol><h2 id="基础篇">基础篇</h2><h3 id="pytorch基础">PyTorch基础</h3><h3 id="第1章-numpynumerical-python基础">第1章 Numpy（Numerical Python）基础</h3><p>基本的对像</p><ol type="1"><li>ndarray（N-dimensional Array Object）<ul><li>单一数据类型的多维数组</li></ul></li><li>ufunc（UniversalFunction Object）<ul><li>对数组进行处理的函数</li></ul></li></ol><p>Numpy的主要特点：</p><ol type="1"><li>ndarray，快速节省空间的多维数组，提供数组化的算术运算和高级的广播功能。</li><li>使用标准数学函数对整个数组的数据进行快速运算，且不需要编写循环。</li><li>读取/写入磁盘上的阵列数据和操作存储器映像文件的工具。</li><li>线性代数、随机数生成和傅里叶变换的能力。</li><li>集成C、C++、Fortran代码的工具。</li></ol><h4 id="生成numpy数组">1.1 生成Numpy数组</h4><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/3.jpg" alt="" /><figcaption>Tab</figcaption></figure><h5 id="从已有数据中创建数组">从已有数据中创建数组</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.array(l)</span><br></pre></td></tr></table></figure><h5 id="利用random模块生成数组">利用random模块生成数组</h5><table><thead><tr class="header"><th>函数</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>np.random.random</td><td>生成0到1之间的随机数</td></tr><tr class="even"><td>np.random.uniform</td><td>生成均匀分布的随机数</td></tr><tr class="odd"><td>np.random.randn</td><td>生成标准正态的随机数</td></tr><tr class="even"><td>np.random.randint</td><td>生成随机的整数</td></tr><tr class="odd"><td>np.random.normal</td><td>生成正态分布</td></tr><tr class="even"><td>np.random.shuffle</td><td>随机打乱顺序</td></tr><tr class="odd"><td>np.random.seed</td><td>设置随机数种子</td></tr><tr class="even"><td>random._sample</td><td>生成随机的浮点数</td></tr></tbody></table><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/4.jpg" alt="" /><figcaption>np.random模块常用函数</figcaption></figure><h5 id="创建特定形状的多维数组">创建特定形状的多维数组</h5><table><thead><tr class="header"><th>函数</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>np.zeros((3, 4))</td><td>创建3×4的元素全为0的数组</td></tr><tr class="even"><td>np.ones((3, 4))</td><td>创建3×4的元素全为1的数组</td></tr><tr class="odd"><td>np.empty( (2, 3))</td><td>创建2×3的空数组,空数据中的值并不为0,而是未初始化的垃圾值</td></tr><tr class="even"><td>np.zeros.like(darr)</td><td>以 darr相同维度创建元素全为0数组</td></tr><tr class="odd"><td>np.ones.like(darr)</td><td>以 narr相同维度创建元素全为1数组</td></tr><tr class="even"><td>np.empty.like(ndarr)</td><td>以 darr相同维度创建空数组</td></tr><tr class="odd"><td>np.eye(5)</td><td>该函数用于创建一个5×5的矩阵,对角线为1,其余为0</td></tr><tr class="even"><td>np.full((3, 5), 666)</td><td>创建3×5的元素全为666的数组,666为指定值</td></tr></tbody></table><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/5.jpg" alt="" /><figcaption>Numpy数组创建函数</figcaption></figure><h5 id="利用arangelinspace函数生成数组">利用arange、linspace函数生成数组`````</h5><p>arange是numpy模块中的函数，其格式为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">help(np.arange)</span><br><span class="line">arange([start,] stop[, step,], dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>其中start与stop用来指定范围，step用来设定步长。在生成一个ndarray时，start默认为0，步长step可为小数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linspace(start, stop, num=<span class="number">50</span>, endpoint=<span class="literal">True</span>, retstep=<span class="literal">False</span>, dtype=<span class="literal">None</span>, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h4 id="获取元素">1.2 获取元素</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">2020</span>)</span><br><span class="line">nd11 = np.random.random([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取指定位置的数据,获取第4个元素</span></span><br><span class="line">nd11[<span class="number">3</span>]</span><br><span class="line"><span class="comment"># 截取一段数据</span></span><br><span class="line">nd11[<span class="number">3</span>:<span class="number">6</span>]</span><br><span class="line"><span class="comment"># 截取固定间隔数据</span></span><br><span class="line">nd11[<span class="number">1</span>:<span class="number">6</span>:<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 倒序取数</span></span><br><span class="line">nd11[::<span class="number">-2</span>]</span><br><span class="line"><span class="comment"># 截取一个多维数组的一个区域内数据</span></span><br><span class="line">nd12 = np.arange(<span class="number">25</span>).reshape([<span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">nd12[<span class="number">1</span>:<span class="number">31</span>:<span class="number">3</span>]</span><br><span class="line"><span class="comment"># 截取一个多维数组中,数值在一个值域之內的数据</span></span><br><span class="line">nd12[(nd12 &gt; <span class="number">3</span>) &amp; (nd12 &lt; <span class="number">10</span>)]</span><br><span class="line"><span class="comment"># 截取多维数组中,指定的行,如读取第2,3行</span></span><br><span class="line">nd12[[<span class="number">1</span>, <span class="number">2</span>]]  <span class="comment"># 或nd12[1:3,:]</span></span><br><span class="line"><span class="comment"># 并截取多维数组中,指定的列,如读取第2,3列</span></span><br><span class="line">nd12[:, <span class="number">1</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/6.jpg" alt="" /><figcaption>获取多维数组中的元素</figcaption></figure><p>随机抽取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> random <span class="keyword">as</span> nr</span><br><span class="line"></span><br><span class="line">a = np.arange(<span class="number">1</span>, <span class="number">25</span>, dtype=float)</span><br><span class="line">c1 = nr.choice(a, size=(<span class="number">3</span>, <span class="number">4</span>))  <span class="comment"># size指定输出数组形状</span></span><br><span class="line">c2 = nr.choice(a, size=(<span class="number">3</span>, <span class="number">4</span>), replace=<span class="literal">False</span>)  <span class="comment"># replace缺省为True，即可重复抽取。</span></span><br><span class="line"><span class="comment"># 下式中参数p指定每个元素对应的抽取概率，缺省为每个元素被抽取的概率相同。</span></span><br><span class="line">c3 = nr.choice(a, size=(<span class="number">3</span>, <span class="number">4</span>), p=a / np.sum(a))</span><br><span class="line">print(<span class="string">"随机可重复抽取"</span>)</span><br><span class="line">print(c1)</span><br><span class="line">print(<span class="string">"随机但不重复抽取"</span>)</span><br><span class="line">print(c2)</span><br><span class="line">print(<span class="string">"随机但按制度概率抽取"</span>)</span><br><span class="line">print(c3)</span><br></pre></td></tr></table></figure><h4 id="numpy的算术运算">1.3 Numpy的算术运算</h4><h5 id="对应元素相乘">1.3.1 对应元素相乘</h5><p><code>np.info(np.multiply)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">multiply(x1, x2, /, out=<span class="literal">None</span>, *, where=<span class="literal">True</span>, casting=<span class="string">'same_kind'</span>, order=<span class="string">'K'</span>, dtype=<span class="literal">None</span>, subok=<span class="literal">True</span>[, signature, extobj])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">-1</span>, <span class="number">4</span>]])</span><br><span class="line">B = np.array([[<span class="number">2</span>, <span class="number">0</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">A*B</span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">array([[<span class="number">2</span>,  <span class="number">0</span>],</span><br><span class="line">       [<span class="number">-3</span>, <span class="number">16</span>]])</span><br><span class="line"><span class="comment"># 或另一种表示方法</span></span><br><span class="line">np.multiply(A, B)</span><br><span class="line"><span class="comment"># 运算结果也是</span></span><br><span class="line">array([[<span class="number">2</span>,  <span class="number">0</span>],</span><br><span class="line">       [<span class="number">-3</span>, <span class="number">16</span>]])</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/7.jpg" alt="" /><figcaption>对应元素相乘示意图</figcaption></figure><h5 id="点积运算">1.3.2 点积运算</h5><p>点积运算（Dot Product）又称为内积</p><p><code>np.info(np.dot)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dot(a, b, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X1=np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">X2=np.array([[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>],[<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]])</span><br><span class="line">X3=np.dot(X1,X2)</span><br><span class="line">print(X3)</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/8.jpg" alt="" /><figcaption>矩阵的点积示意图，对应维度的元素个数需要保持一致</figcaption></figure><h4 id="数组变形">1.4 数组变形</h4><h5 id="更改数组的形状">1.4.1 更改数组的形状</h5><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/9.jpg" alt="" /><figcaption>Numpy中改变向量形状的一些函数</figcaption></figure><h5 id="合并数组">1.4.2 合并数组</h5><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/10.jpg" alt="" /><figcaption>Numpy数组合并方法</figcaption></figure><ol type="1"><li>append、concatenate以及stack都有一个axis参数，用于控制数组的合并方式是按行还是按列。</li><li>对于append和concatenate，待合并的数组必须有相同的行数或列数（满足一个即可）。</li><li>stack、hstack、dstack，要求待合并的数组必须具有相同的形状（shape）。</li></ol><h4 id="批量处理">1.5 批量处理</h4><ol type="1"><li>得到数据集</li><li>随机打乱数据</li><li>定义批大小</li><li>批处理数据集</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 生成10000个形状为2X3的矩阵</span></span><br><span class="line">data_train = np.random.randn(<span class="number">10000</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 这是一个3维矩阵，第1个维度为样本数，后两个是数据形状</span></span><br><span class="line">print(data_train.shape)</span><br><span class="line"><span class="comment"># (10000,2,3)</span></span><br><span class="line"><span class="comment"># 打乱这10000条数据</span></span><br><span class="line">np.random.shuffle(data_train)</span><br><span class="line"><span class="comment"># 定义批量大小</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line"><span class="comment"># 进行批处理</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(data_train), batch_size):</span><br><span class="line">    x_batch_sum = np.sum(data_train[i:i+batch_size])</span><br><span class="line">    print(<span class="string">"第&#123;&#125;批次,该批次的数据之和:&#123;&#125;"</span>.format(i, x_batch_sum))</span><br></pre></td></tr></table></figure><p>【说明】批次从0开始，所以最后一个批次是9900。</p><h4 id="通用函数">1.6 通用函数</h4><p>ufunc是universalfunction的缩写，它是一种能对数组的每个元素进行操作的函数。</p><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/11.jpg" alt="" /><figcaption>Numpy中的几个常用通用函数</figcaption></figure><h4 id="广播">1.7 广播</h4><p>Numpy的Universal functions中要求输入的数组shape是一致的，当数组的shape不相等时，则会使用广播机制。不过，调整数组使得shape一样，需要满足一定的规则，否则将出错。</p><p>这些规则可归纳为以下4条。</p><ol type="1"><li>让所有输入数组都向其中shape最长的数组看齐，不足的部分则通过在前面加1补齐，如：<ul><li>a：2×3×2</li><li>b：3×2</li><li>则b向a看齐，在b的前面加1，变为：1×3×2</li></ul></li><li>输出数组的shape是输入数组shape的各个轴上的最大值</li><li>如果输入数组的某个轴和输出数组的对应轴的长度相同或者某个轴的长度为1时，这个数组能被用来计算，否则出错</li><li>当输入数组的某个轴的长度为1时，沿着此轴运算时都用（或复制）此轴上的第一组值。</li></ol><p>广播在整个Numpy中用于决定如何处理形状迥异的数组，涉及的算术运算包括（+，-，*，/…）。</p><p>这些规则说得很严谨，但不直观，下面我们结合图形与代码来进一步说明。</p><p>目的：A+B，其中A为4×1矩阵，B为一维向量（3,）。</p><p>要相加，需要做如下处理：</p><ol type="1"><li>根据规则1，B需要向看齐，把B变为（1,3）</li><li>根据规则2，输出的结果为各个轴上的最大值，即输出结果应该为（4,3）矩阵，那么A如何由（4,1）变为（4,3）矩阵？B又如何由（1,3）变为（4,3）矩阵？</li><li>根据规则4，用此轴上的第一组值（要主要区分是哪个轴），进行复制（但在实际处理中不是真正复制，否则太耗内存，而是采用其他对象如ogrid对象，进行网格处理）即可，详细处理过程如图所示。</li></ol><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/12.jpg" alt="" /><figcaption>Numpy广播规则示意图</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.arange(<span class="number">0</span>, <span class="number">40</span>, <span class="number">10</span>).reshape(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">B = np.arange(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">print(<span class="string">"A矩阵的形状:&#123;&#125;,B矩阵的形状:&#123;&#125;"</span>.format(A.shape, B.shape))</span><br><span class="line">C = A+B</span><br><span class="line">print(<span class="string">"C矩阵的形状:&#123;&#125;"</span>.format(C.shape))</span><br><span class="line">print(C)</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A矩阵的形状:(4, 1),B矩阵的形状:(3,)</span><br><span class="line">C矩阵的形状:(4, 3)</span><br><span class="line">[[ 0  1  2]</span><br><span class="line"> [10 11 12]</span><br><span class="line"> [20 21 22]</span><br><span class="line"> [30 31 32]]</span><br></pre></td></tr></table></figure><h3 id="第2章-pytorch基础">第2章 Pytorch基础</h3><h4 id="为何选择pytorch">2.1 为何选择Pytorch？</h4><p>PyTorch由4个主要的包组成：</p><ol type="1"><li><code>torch</code>：类似于Numpy的通用数组库，可将张量类型转换为torch.cuda.TensorFloat，并在GPU上进行计算。</li><li><code>torch.autograd</code>：用于构建计算图形并自动获取梯度的包。</li><li><code>torch.nn</code>：具有共享层和损失函数的神经网络库。</li><li><code>torch.optim</code>：具有通用优化算法（如SGD、Adam等）的优化包。</li></ol><h4 id="安装配置">2.2 安装配置</h4><p>参考 <a href="https://pytorch.org" target="_blank" rel="noopener" class="uri">https://pytorch.org</a> 就可以了。</p><h5 id="cpu版pytorch">2.2.1 CPU版Pytorch</h5><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/13.jpg" alt="" /><figcaption>下载Anaconda界面</figcaption></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh</span><br><span class="line">sh Miniconda3-latest-MacOSX-x86_64.sh</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/14.jpg" alt="" /><figcaption>PyTorch安装界面</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure><p>当前最新版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.4.0</span><br></pre></td></tr></table></figure><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/15.jpg" alt="" /><figcaption>验证安装是否成功</figcaption></figure><h5 id="gpu版pytorch">2.2.2 GPU版Pytorch</h5><h6 id="安装nvidia驱动">安装NVIDIA驱动</h6><p><a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener" class="uri">https://www.nvidia.cn/Download/index.aspx?lang=cn</a></p><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/16.jpg" alt="" /><figcaption>NVIDIA的下载界面</figcaption></figure><p>安装完成后，在命令行输入 <code>nvidia-smi</code>，用来显示GPU卡的基本信息</p><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/17.jpg" alt="" /><figcaption>显示GPU卡的基本信息</figcaption></figure><h6 id="安装cuda">安装CUDA</h6><p>CUDA（Compute Unified Device Architecture），是英伟达公司推出的一种基于新的并行编程模型和指令集架构的通用计算架构，它能利用英伟达GPU的并行计算引擎，比CPU更高效地解决许多复杂计算任务。安装CUDA Driver时，其版本需与NVIDIA GPU Driver的版本一致，这样CUDA才能找到显卡。</p><h6 id="安装cudnn">安装cuDNN</h6><p>NVIDIA cuDNN是用于深度神经网络的GPU加速库。注册NVIDIA并下载cuDNN包，获取地址为<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener" class="uri">https://developer.nvidia.com/rdp/cudnn-archive</a>。</p><h6 id="安装python及pytorch">安装Python及PyTorch</h6><p>安装GPU版PyTorch相同，只是选择CUDA时，不是None，而是对应CUDA的版本号。</p><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/18.jpg" alt="" /><figcaption>安装GPU版PyTorch</figcaption></figure><h6 id="验证">验证</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat test_gpu.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 测试 CUDA</span></span><br><span class="line">    print(<span class="string">"Support CUDA ?: "</span>, torch.cuda.is_available())</span><br><span class="line">    x = torch.tensor([<span class="number">10.0</span>])</span><br><span class="line">    x = x.cuda()</span><br><span class="line">    print(x)</span><br><span class="line">    y = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    y = y.cuda()</span><br><span class="line">    print(y)</span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">   <span class="comment"># 测试 CUDNN</span></span><br><span class="line">    <span class="keyword">from</span> torch.backends <span class="keyword">import</span> cudnn</span><br><span class="line">    print(<span class="string">"Support cudnn ?: "</span>, cudnn.is_acceptable(x))</span><br></pre></td></tr></table></figure><p><code>python torch</code></p><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/19.jpg" alt="" /><figcaption>运行test_gpu.py的结果</figcaption></figure><p>在命令行运行：<code>nvidia-smi</code></p><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/20.jpg" alt="" /><figcaption>含GPU进程的显卡信息</figcaption></figure><h4 id="jupyter-notebook环境配置">2.3 Jupyter Notebook环境配置</h4><ul><li>编程时具有语法高亮、缩进、Tab补全的功能</li><li>可直接通过浏览器运行代码，同时在代码块下方展示运行结果</li><li>以富媒体格式展示计算结果。富媒体格式包括：HTML、LaTeX、PNG、SVG等</li><li>对代码编写说明文档或语句时，支持Markdown语法</li><li>支持使用LaTeX编写数学性说明。</li></ul><ol type="1"><li>生成配置文件。</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure><p>执行上述代码，将在当前用户目录下生成文件：<code>.jupyter/jupyter_notebook_config.py</code></p><ol type="1"><li>生成当前用户登录Jupyter密码。打开Ipython，创建一个密文密码。</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [1]: from notebook.auth import passwd</span><br><span class="line">In [2]: passwd()</span><br><span class="line">Enter password: </span><br><span class="line">Verify password:</span><br></pre></td></tr></table></figure><ol type="1"><li>修改配置文件。</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure><p>进行如下修改：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip = <span class="string">'*'</span>  <span class="comment"># 就是设置所有ip皆可访问</span></span><br><span class="line">c.NotebookApp.password = u<span class="string">'sha:ce...刚才复制的那个密文'</span></span><br><span class="line">c.NotebookApp.open_browser = False  <span class="comment"># 禁止自动打开浏览器</span></span><br><span class="line">c.NotebookApp.port = 8888  <span class="comment"># 这是缺省端口，也可指定其他端口</span></span><br></pre></td></tr></table></figure><ol type="1"><li>启动Jupyter Notebook。</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 后台启动jupyter：不记日志：</span></span><br><span class="line">nohup jupyter notebook &gt;/dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>在浏览器上，输入IP:port，即可看到与下图类似的界面。</p><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/21.jpg" alt="" /><figcaption>Jupyter Notebook网页界面</figcaption></figure><p>接下来就可以在浏览器进行开发调试PyTorch、Python等任务了。</p><h4 id="numpy与tensor">2.4 Numpy与Tensor</h4><p>Numpy存取数据非常方便，而且还拥有大量的函数，所以深得数据处理、机器学习者喜爱。</p><p>Tensor，它可以是零维（又称为标量或一个数）、一维、二维及多维的数组。</p><p>Tensor自称为神经网络界的Numpy，它与Numpy相似，二者可以共享内存，且之间的转换非常方便和高效。</p><p>不过它们也有不同之处，最大的区别就是Numpy会把ndarray放在CPU中进行加速运算，而由Torch产生的Tensor会放在GPU中进行加速运算（假设当前环境有GPU）。</p><h5 id="tensor概述">2.4.1 Tensor概述</h5><p>对Tensor的操作很多，从接口的角度来划分，可以分为两类：</p><ol type="1"><li>torch.function，如torch.sum、torch.add等</li><li>tensor.function，如tensor.view、tensor.add等</li></ol><p>如果从修改方式的角度来划分，可以分为以下两类：</p><ol type="1"><li>不修改自身数据，如x.add(y)，x的数据不变，返回一个新的Tensor。</li><li>修改自身数据，如x.add_(y)（运行符带下划线后缀），运算结果存在x中，x被修改。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">z = x.add(y)</span><br><span class="line">print(z)</span><br><span class="line">print(x)</span><br><span class="line">x.add_(y)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">4</span>, <span class="number">6</span>])</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure><h5 id="创建tensor">2.4.2 创建Tensor</h5><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/22.jpg" alt="" /><figcaption>常见的创建Tensor的方法</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 根据list数据生成Tensor</span></span><br><span class="line">torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="comment"># 根据指定形状生成Tensor</span></span><br><span class="line">torch.Tensor(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 根据给定的Tensor的形状</span></span><br><span class="line">t = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="comment"># 查看Tensor的形状</span></span><br><span class="line">t.size()</span><br><span class="line"><span class="comment"># shape与size()等价方式</span></span><br><span class="line">t.shape</span><br><span class="line"><span class="comment"># 根据已有形状创建Tensor</span></span><br><span class="line">torch.Tensor(t.size())</span><br></pre></td></tr></table></figure><h5 id="修改tensor形状">2.4.3 修改Tensor形状</h5><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/23.jpg" alt="" /><figcaption>为tensor常用修改形状的函数</figcaption></figure><h5 id="索引操作">2.4.4 索引操作</h5><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/24.jpg" alt="" /><figcaption>常用选择操作函数</figcaption></figure><h5 id="广播机制">2.4.5 广播机制</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.arange(<span class="number">0</span>, <span class="number">40</span>, <span class="number">10</span>).reshape(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">B = np.arange(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 把ndarray转换为Tensor</span></span><br><span class="line">A1 = torch.from_numpy(A)  <span class="comment"># 形状为4x1</span></span><br><span class="line">B1 = torch.from_numpy(B)  <span class="comment"># 形状为3</span></span><br><span class="line"><span class="comment"># Tensor自动实现广播</span></span><br><span class="line">C = A1+B1</span><br><span class="line"><span class="comment"># 我们可以根据广播机制，手工进行配置</span></span><br><span class="line"><span class="comment"># 根据规则1，B1需要向A1看齐，把B变为（1,3）</span></span><br><span class="line">B2 = B1.unsqueeze(<span class="number">0</span>)  <span class="comment"># B2的形状为1x3</span></span><br><span class="line"><span class="comment"># 使用expand函数重复数组，分别的4x3的矩阵</span></span><br><span class="line">A2 = A1.expand(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">B3 = B2.expand(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 然后进行相加,C1与C结果一致</span></span><br><span class="line">C1 = A2+B3</span><br></pre></td></tr></table></figure><h5 id="逐元素操作">2.4.6 逐元素操作</h5><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/25.jpg" alt="" /><figcaption>常见逐元素操作</figcaption></figure><h5 id="归并操作">2.4.7 归并操作</h5><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/26.jpg" alt="" /><figcaption>常见的归并操作</figcaption></figure><h5 id="比较操作">2.4.8 比较操作</h5><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/27.jpg" alt="" /><figcaption>常用的比较函数</figcaption></figure><h5 id="矩阵操作">2.4.9 矩阵操作</h5><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/28.jpg" alt="" /><figcaption>常用矩阵函数</figcaption></figure><h5 id="pytorch与numpy比较">2.4.10 Pytorch与Numpy比较</h5><figure><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/29.jpg" alt="" /><figcaption>PyTorch与Numpy函数对照表</figcaption></figure><h4 id="tensor与autograd">2.5 Tensor与Autograd</h4><h5 id="自动求导要点">2.5.1 自动求导要点</h5><h5 id="计算图">2.5.2计算图</h5><h5 id="标量反向传播">2.5.3 标量反向传播</h5><h5 id="非标量反向传播">2.5.4 非标量反向传播</h5><h4 id="使用numpy实现机器学习">2.6 使用Numpy实现机器学习</h4><h4 id="使用tensor及antograd实现机器学习">2.7 使用Tensor及antograd实现机器学习</h4><h4 id="使用tensorflow架构">2.8 使用TensorFlow架构</h4><h3 id="第3章-pytorch实现神经网络工具箱">第3章 Pytorch实现神经网络工具箱</h3><h4 id="神经网络核心组件">3.1 神经网络核心组件</h4><h4 id="实现神经网络实例">3.2实现神经网络实例</h4><h5 id="背景说明">3.2.1背景说明</h5><h5 id="准备数据">3.2.2准备数据</h5><h5 id="可视化源数据">3.2.3可视化源数据</h5><h5 id="构建模型">3.2.4 构建模型</h5><h5 id="训练模型">3.2.5 训练模型</h5><h4 id="如何构建神经网络">3.3 如何构建神经网络？</h4><h5 id="构建网络层">3.3.1 构建网络层</h5><h5 id="前向传播">3.3.2 前向传播</h5><h5 id="反向传播">3.3.3 反向传播</h5><h5 id="训练模型-1">3.3.4 训练模型</h5><h4 id="nn.module">3.4 nn.Module</h4><h4 id="nn.functional">3.5 nn.functional</h4><h4 id="优化器">3.6 优化器</h4><h4 id="动态修改学习率参数">3.7 动态修改学习率参数</h4><h4 id="优化器比较">3.8 优化器比较</h4><h3 id="第4章-pytorch数据处理工具箱">第4章 Pytorch数据处理工具箱</h3><h4 id="数据处理工具箱概述">4.1 数据处理工具箱概述</h4><h4 id="utils.data简介">4.2 utils.data简介</h4><h4 id="torchvision简介">4.3 torchvision简介</h4><h5 id="transforms">4.3.1 transforms</h5><h5 id="imagefolder">4.3.2 ImageFolder</h5><h4 id="可视化工具">4.4 可视化工具</h4><h5 id="tensorboardx简介">4.4.1 tensorboardX简介</h5><h5 id="用tensorboardx可视化神经网络">4.4.2用tensorboardX可视化神经网络</h5><h5 id="用tensorboardx可视化损失值">4.4.3用tensorboardX可视化损失值</h5><h5 id="用tensorboardx可视化特征图">4.4.4用tensorboardX可视化特征图</h5><h2 id="深度学习基础">深度学习基础</h2><h3 id="第5章-机器学习基础">第5章 机器学习基础</h3><h4 id="机器学习的基本任务">5.1 机器学习的基本任务</h4><h5 id="监督学习">5.1.1监督学习</h5><h5 id="无监督学习">5.1.2 无监督学习</h5><h5 id="半监督学习">5.1.3 半监督学习</h5><h5 id="强化学习">5.1.4 强化学习</h5><h4 id="机器学习一般流程">5.2 机器学习一般流程</h4><h5 id="明确目标">5.2.1 明确目标</h5><h5 id="收集数据">5.2.2收集数据</h5><h5 id="数据探索与预处理">5.2.3 数据探索与预处理</h5><h5 id="选择模型">5.2.4 选择模型</h5><h5 id="评估及优化模型">5.2.5 评估及优化模型</h5><h4 id="过拟合与欠拟合">5.3 过拟合与欠拟合</h4><h5 id="权重正则化">5.3.1 权重正则化</h5><h5 id="dropout正则化">5.3.2 dropout正则化</h5><h5 id="批量正则化">5.3.3 批量正则化</h5><h5 id="权重初始化">5.3.4权重初始化</h5><h4 id="选择合适激活函数">5.4 选择合适激活函数</h4><h4 id="选择合适的损失函数">5.5 选择合适的损失函数</h4><h4 id="选择合适优化器">5.6 选择合适优化器</h4><h5 id="传统梯度优化的不足">5.6.1传统梯度优化的不足</h5><h5 id="动量算法">5.6.2动量算法</h5><h5 id="adagrad算法">5.6.3 AdaGrad算法</h5><h5 id="rmsprop算法">5.6.4 RMSProp算法</h5><h5 id="adam算法">5.6.5 Adam算法</h5><h4 id="gpu加速">5.7GPU加速</h4><h5 id="单gpu加速">5.7.1 单GPU加速</h5><h5 id="多gpu加速">5.7.2 多GPU加速</h5><h5 id="使用gpu注意事项">5.7.3使用GPU注意事项</h5><h3 id="第6章-视觉处理基础">第6章 视觉处理基础</h3><h4 id="卷积神经网络简介">6.1卷积神经网络简介</h4><h4 id="卷积层">6.2卷积层</h4><h5 id="卷积核">6.2.1 卷积核</h5><h5 id="步幅">6.2.2步幅</h5><h5 id="填充">6.2.3 填充</h5><h5 id="多通道上的卷积">6.2.4 多通道上的卷积</h5><h5 id="激活函数">6.2.5激活函数</h5><h5 id="卷积函数">6.2.6卷积函数</h5><h5 id="转置卷积">6.2.7转置卷积</h5><h4 id="池化层">6.3池化层</h4><h5 id="局部池化">6.3.1局部池化</h5><h5 id="全局池化">6.3.2全局池化</h5><h4 id="现代经典网络">6.4现代经典网络</h4><h5 id="lenet-5模型">6.4.1 LeNet-5模型</h5><h5 id="alexnet模型">6.4.2 AlexNet模型</h5><h5 id="vgg模型">6.4.3 VGG模型</h5><h5 id="googlenet模型">6.4.4 GoogleNet模型</h5><h5 id="resnet模型">6.4.5 ResNet模型</h5><h5 id="胶囊网络简介">6.4.6 胶囊网络简介</h5><h4 id="pytorch实现cifar10多分类">6.5 Pytorch实现cifar10多分类</h4><h5 id="数据集说明">6.5.1 数据集说明</h5><h5 id="加载数据">6.5.2 加载数据</h5><h5 id="构建网络">6.5.3 构建网络</h5><h5 id="训练模型-2">6.5.4 训练模型</h5><h5 id="测试模型">6.5.5 测试模型</h5><h5 id="采用全局平均池化">6.5.6 采用全局平均池化</h5><h5 id="像keras一样显示各层参数">6.5.7像keras一样显示各层参数</h5><h4 id="模型集成提升性能">6.6 模型集成提升性能</h4><h5 id="使用模型">6.6.1 使用模型</h5><h5 id="集成方法">6.6.2 集成方法</h5><h5 id="集成效果">6.6.3 集成效果</h5><h4 id="使用经典模型提升性能">6.7使用经典模型提升性能</h4><h3 id="第7章-自然语言处理基础">第7章 自然语言处理基础</h3><h4 id="循环神经网络基本结构">7.1 循环神经网络基本结构</h4><h4 id="前向传播与随时间反向传播">7.2前向传播与随时间反向传播</h4><h4 id="循环神经网络变种">7.3 循环神经网络变种</h4><h5 id="lstm">7.3.1 LSTM</h5><h5 id="gru">7.3.2 GRU</h5><h5 id="bi-rnn">7.3.3 Bi-RNN</h5><h4 id="循环神经网络的pytorch实现">7.4 循环神经网络的Pytorch实现</h4><h5 id="rnn实现">7.4.1 RNN实现</h5><h5 id="lstm实现">7.4.2LSTM实现</h5><h5 id="gru实现">7.4.3GRU实现</h5><h4 id="文本数据处理">7.5文本数据处理</h4><h4 id="词嵌入">7.6词嵌入</h4><h5 id="word2vec原理">7.6.1Word2Vec原理</h5><h5 id="cbow模型">7.6.2 CBOW模型</h5><h5 id="skim-gram模型">7.6.3 Skim-gram模型</h5><h4 id="pytorch实现词性判别">7.7 Pytorch实现词性判别</h4><h5 id="词性判别主要步骤">7.7.1 词性判别主要步骤</h5><h5 id="数据预处理">7.7.2 数据预处理</h5><h5 id="构建网络-1">7.7.3 构建网络</h5><h5 id="训练网络">7.7.4 训练网络</h5><h5 id="测试模型-1">7.7.5 测试模型</h5><h4 id="循环神经网络应用场景">7.8循环神经网络应用场景</h4><h3 id="第8章-生成式深度学习">第8章 生成式深度学习</h3><h4 id="用变分自编码器生成图像">8.1 用变分自编码器生成图像</h4><h5 id="自编码器">8.1.1 自编码器</h5><h5 id="变分自编码器">8.1.2变分自编码器</h5><h5 id="用变分自编码器生成图像-1">8.1.3用变分自编码器生成图像</h5><h4 id="gan简介">8.2 GAN简介</h4><h5 id="gan架构">8.2.1 GAN架构</h5><h5 id="gan的损失函数">8.2.2 GAN的损失函数</h5><h4 id="用gan生成图像">8.3用GAN生成图像</h4><h5 id="判别器">8.3.1判别器</h5><h5 id="生成器">8.3.2 生成器</h5><h5 id="训练模型-3">8.3.3 训练模型</h5><h5 id="可视化结果">8.3.4 可视化结果</h5><h4 id="vae与gan的异同">8.4 VAE与GAN的异同</h4><h4 id="condition-gan">8.5 Condition GAN</h4><h5 id="cgan的架构">8.5.1 CGAN的架构</h5><h5 id="cgan-生成器">8.5.2 CGAN 生成器</h5><h5 id="cgan-判别器">8.5.3 CGAN 判别器</h5><h5 id="cgan-损失函数">8.5.4 CGAN 损失函数</h5><h5 id="cgan-可视化">8.5.5 CGAN 可视化</h5><h5 id="查看指定标签的数据">8.5.6 查看指定标签的数据</h5><h5 id="可视化损失值">8.5.7 可视化损失值</h5><h4 id="dcgan">8.6 DCGAN</h4><h4 id="提升gan训练效果的一些技巧">8.7 提升GAN训练效果的一些技巧</h4><h2 id="深度学习实战">深度学习实战</h2><h3 id="第9章-人脸检测与识别">第9章 人脸检测与识别</h3><h4 id="人脸识别一般流程">9.1 人脸识别一般流程</h4><h5 id="图像采集">9.1.1图像采集</h5><h5 id="人脸检测">9.1.2 人脸检测</h5><h4 id="特征提取">9.3特征提取</h4><h4 id="人脸识别">9.4人脸识别</h4><h5 id="人脸识别主要原理">9.4.1 人脸识别主要原理</h5><h5 id="人脸识别发展">9.4.2人脸识别发展</h5><h4 id="人脸检测与识别实例">9.5 人脸检测与识别实例</h4><h5 id="验证检测代码">9.5.1.验证检测代码</h5><h5 id="检测图像">9.5.2.检测图像</h5><h5 id="检测后进行预处理">9.5.3.检测后进行预处理</h5><h5 id="查看经检测后的图片">9.5.4.查看经检测后的图片</h5><h5 id="人脸识别-1">9.5.5.人脸识别</h5><h3 id="第10章-迁移学习实例">第10章 迁移学习实例</h3><p>10.1 迁移学习简介<br />10.2 特征提取<br />10.2.1 Pytorch提供的预处理模块<br />10.2.2 特征提取实例<br />10.3 数据增强<br />10.3.1 按比例缩放<br />10.3.2 裁剪<br />10.3.3翻转<br />10.3.4改变颜色<br />10.3.5组合多种增强方法<br />10.4 微调实例<br />10.4.1 数据预处理<br />10.4.2 加载预训练模型<br />10.4.3 修改分类器<br />10.4.4 选择损失函数及优化器<br />10.4.5 训练及验证模型<br />10.5 用预训练模型清除图像中的雾霾<br />10.5.1 导入需要的模块<br />10.5.2 查看原来的图像<br />10.5.3 定义一个神经网络<br />10.5.4 训练模型<br />10.5.5 查看处理后的图像</p><h3 id="第11章-神经网络机器翻译实例">第11章 神经网络机器翻译实例</h3><p>11.1 Encode-Decoder模型原理<br />11.2 注意力框架<br />11.3 Pytorch实现注意力Decoder<br />11.3.1 构建Encoder<br />11.3.2 构建简单Decoder<br />11.3.3 构建注意力Decoder<br />11.4 用注意力机制实现中英文互译<br />11.4.1 导入需要的模块<br />11.4.2数据预处理<br />11.4.3构建模型<br />11.4.4训练模型<br />11.4.5随机采样，对模型进行测试<br />11.4.6可视化注意力</p><h3 id="第12章-实战生成式模型">第12章 实战生成式模型</h3><p>12.1 Deep Dream模型<br />12.1.1 Deep Dream原理<br />12.1.2 DeepDream算法流程<br />12.1.3 用Pytorch实现Deep Dream<br />12.2 风格迁移<br />12.2.1 内容损失<br />12.2.2 风格损失<br />12.2.3 用Pytorch实现神经网络风格迁移<br />12.3 Pytorch实现图像修复<br />12.3.1 网络结构<br />12.3.2 损失函数<br />12.3.3 图像修复实例<br />12.4 Pytorch实现DiscoGAN<br />12.4.1 DiscoGAN架构<br />12.4.2 损失函数<br />12.4.3 DiscoGAN实现<br />12.4.4 用Pytorch实现从边框生成鞋子<br />### 第13章 Caffe2模型迁移实例<br />13.1 Caffe2简介<br />13.2 Caffe如何迁移到Caffe2<br />13.3 Pytorch如何迁移到caffe2</p><h3 id="第14章-ai新方向对抗攻击">第14章 AI新方向：对抗攻击</h3><p>14.1对抗攻击简介<br />14.1.1白盒攻击与黑盒攻击<br />14.1.2无目标攻击与有目标攻击<br />14.2常见对抗样本生成方式<br />14.2.1快速梯度符号法<br />14.2.2快速梯度算法<br />14.3 Pytorch实现对抗攻击<br />14.3.1 实现无目标攻击<br />14.3.2 实现有目标攻击<br />14.4 对抗攻击和防御措施<br />14.4.1 对抗攻击<br />14.4.2 常见防御方法分类<br />14.5 总结<br />### 第15章 强化学习<br />15.1 强化学习简介<br />15.2Q Learning 原理<br />15.2.1 Q Learning主要流程<br />15.2.2 Q函数<br />15.2.3 贪婪策略<br />15.3 用Pytorch实现Q Learning<br />15.3.1 定义Q-Learing主函数<br />15.3.2执行Q-Learing<br />15.4 SARSA 算法<br />15.4.1 SARSA算法主要步骤<br />15.4.2 用Pytorch实现SARSA算法</p><p>第16章 深度强化学习<br />16.1 DSN算法原理<br />16.1.1 Q-Learning方法的局限性<br />16.1.2 用DL处理RL需要解决的问题<br />16.1.3 用DQN解决方法<br />16.1.4 定义损失函数<br />16.1.5 DQN的经验回放机制<br />16.1.6 目标网络<br />16.1.7 网络模型<br />16.1.8 DQN算法<br />16.2 用Pytorch实现 DQN算法</p><h2 id="ai在各行业的最新应用">AI在各行业的最新应用</h2><h3 id="ai电商">AI+电商</h3><h3 id="ai金融">AI+金融</h3><h3 id="ai医疗">AI+医疗</h3><h3 id="ai零售">AI+零售</h3><h3 id="ai投行">AI+投行</h3><h3 id="ai制造">AI+制造</h3><h3 id="aiit服务">AI+IT服务</h3><h3 id="ai汽车">AI+汽车</h3><h3 id="ai公共安全">AI+公共安全</h3><p><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/30.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/31.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/32.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/33.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/34.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/35.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/36.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/37.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/38.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/39.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/40.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/41.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/42.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/43.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/44.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/45.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/46.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/47.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/48.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/49.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/50.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/51.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/52.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/53.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/54.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/55.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/56.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/57.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/58.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/59.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/60.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/61.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/62.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/63.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/64.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/65.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/66.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/67.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/68.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/69.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/70.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/71.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/72.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/73.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/74.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/75.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/76.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/77.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/78.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/79.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/80.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/81.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/82.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/83.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/84.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/85.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/86.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/87.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/88.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/89.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/90.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/91.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/92.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/93.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/94.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/95.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/96.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/97.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/98.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/99.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/100.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/101.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/102.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/103.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/104.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/105.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/106.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/107.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/108.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/109.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/110.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/111.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/112.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/113.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/114.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/115.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/116.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/117.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/118.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/119.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/120.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/121.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/122.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/123.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/124.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/125.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/126.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/127.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/128.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/129.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/130.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/131.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/132.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/133.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/134.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/135.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/136.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/137.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/138.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/139.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/140.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/141.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/142.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/143.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/144.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/145.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/146.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/147.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/148.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/149.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/150.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/151.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/152.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/153.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/154.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/155.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/156.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/157.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/158.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/159.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/160.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/161.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/162.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/163.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/164.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/165.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/166.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/167.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/168.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/169.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/170.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/171.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/172.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/173.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/174.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/175.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/176.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/177.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/178.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/179.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/180.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/181.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/182.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/183.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/184.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/185.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/186.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/187.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/188.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/189.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/190.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/191.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/192.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/193.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/194.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/195.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/196.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/197.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/198.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/199.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/200.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/201.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/202.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/203.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/204.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/205.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/206.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/207.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/208.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/209.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/210.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/211.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/212.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/213.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/214.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/215.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/216.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/217.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/218.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/219.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/220.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/221.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/222.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/223.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/224.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/225.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/226.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/227.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/228.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/229.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/230.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/231.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/232.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/233.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/234.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/235.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/236.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/237.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/238.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/239.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/240.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/241.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/242.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/243.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/244.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/245.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/246.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/247.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/248.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/249.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/250.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/251.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/252.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/253.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/254.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/255.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/256.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/257.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/258.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/259.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/260.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/261.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/262.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/263.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/264.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/265.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/266.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/267.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/268.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/269.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/270.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/271.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/272.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/273.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/274.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/275.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/276.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/277.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/278.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/279.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/280.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/281.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/282.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/283.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/284.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/285.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/286.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/287.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/288.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/289.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/290.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/291.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/292.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/293.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/294.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/295.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/296.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/297.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/298.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/299.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/300.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/301.jpg" /><br /><img src="https://2020.iosdevlog.com/2020/02/17/pytorch/302.jpg" /></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/17/pytorch/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《Python深度学习：基于PyTorch》&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;书名：Python深度学习：基于PyTorch&lt;br /&gt;
作者：吴茂贵，郁明敏，杨本法，李涛，张粤磊&lt;br /&gt;
出版社：机械工业出版社&lt;br /&gt;
出版时间：2019-10&lt;br /&gt;
ISBN：9787111637172&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="PyTorch" scheme="https://2020.iosdevlog.com/tags/PyTorch/"/>
    
      <category term="Python" scheme="https://2020.iosdevlog.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>智能安卓恶意软件检测系统</title>
    <link href="https://2020.iosdevlog.com/2020/02/16/android/"/>
    <id>https://2020.iosdevlog.com/2020/02/16/android/</id>
    <published>2020-02-16T10:50:36.000Z</published>
    <updated>2020-02-16T11:04:03.888Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/16/android/1.png" alt="" /><figcaption>HinDroid</figcaption></figure><p><a href="https://dl.acm.org/doi/10.1145/3097983.3098026" target="_blank" rel="noopener">HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network</a></p><a id="more"></a><h2 id="主要贡献">主要贡献</h2><p>这篇论文希望解决的问题是如何有效地监测安卓手机系统下的恶意软件。</p><p>之前很多恶意软件的分析和检测都是基于某种 <strong>指纹签字</strong> 技术，然而这种技术常常被恶意软件开发者的新手段绕过。</p><p>因此，寻找更加复杂有效的检测方式就成了各种信息安全公司所追逐的目标。</p><p>这篇论文的主要贡献是根据安卓的 API，提出了一种新的基于结构性异构信息网络的方法，来对安卓程序的 API 模式进行更加复杂的建模，从而能够理解整个安卓程序的语义。作者们还采用了多核学习（Multi-Kernel Learning）的方法，在结构性异构信息网络的基础上对程序语义模式进行分类。</p><h2 id="核心方法">核心方法</h2><p>首先，需要将安卓的程序代码转换为可以分析的形式。一般来说，安卓的软件被打包为后缀名为 Dex 的 Dalivik 执行文件，这个执行文件无法被直接分析。于是，需要把这个执行文件通过一个叫 Smali 的反汇编器解析成 Smali 代码。这个时候，软件的语义就能够通过 Smali 代码来解析了。作者们从 Smali 代码中提取所有的 API 调用，通过对 API 的分析来对程序行为建模。下一步，就是要从繁复的 API 调用中摸索出这里面的规律。</p><p>作者们这个时候构建了四类矩阵来表达 API 和某个 App 之间的基本特征：</p><ol type="1"><li>某一个 App 是否包含了某一个 API；</li><li>某两个 API 是否同时出现在某一段代码中；</li><li>某两个 API 是否出现在同一个 App 中；</li><li>某两个 API 是否使用了相同的调用方法。</li></ol><p>为了发现更加复杂的规律，作者们在这里引入了一个工具叫异构信息网络。异构信息网络的概念最早由伊利诺伊大学香槟分校的数据挖掘权威韩家炜（Jiawei Han）和他当时的学生孙怡舟（Yizhou Sun，目前在加州大学洛杉矶分校任教）提出。异构信息网络的核心思想就是希望能够表达一系列实体（Entity）之间的复杂规律。</p><p>把 App 和 API 的关系描述成为异构信息网络以后，下面的工作就是定义更高阶的规律关系。</p><p>利用异构信息网络和元路径构建了程序的语义表达后，下一步就是进行恶意软件的判别。</p><figure><img src="https://2020.iosdevlog.com/2020/02/16/android/2.png" alt="" /><figcaption>Multi-kernel Learning</figcaption></figure><p>参考: <a href="https://time.geekbang.org/column/article/394" target="_blank" rel="noopener" class="uri">https://time.geekbang.org/column/article/394</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/16/android/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;HinDroid&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3097983.3098026&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="paper" scheme="https://2020.iosdevlog.com/tags/paper/"/>
    
      <category term="Android" scheme="https://2020.iosdevlog.com/tags/Android/"/>
    
  </entry>
  
  <entry>
    <title>《白夜行》人物关系图</title>
    <link href="https://2020.iosdevlog.com/2020/02/16/9787544242516/"/>
    <id>https://2020.iosdevlog.com/2020/02/16/9787544242516/</id>
    <published>2020-02-15T16:00:48.000Z</published>
    <updated>2020-02-16T11:25:23.613Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/16/9787544242516/1.svg" alt="" /><figcaption>《白夜行》人物关系图</figcaption></figure><a id="more"></a><p>作者: [日] 东野圭吾<br />出版社: 南海出版公司<br />出品方: 新经典文化<br />原作名: 白夜行<br />译者: 刘姿君<br />出版年: 2008-9<br />页数: 467<br />定价: 29.80元<br />装帧: 平装<br />丛书: 新经典文库·东野圭吾作品<br />ISBN: 9787544242516</p><figure><img src="https://2020.iosdevlog.com/2020/02/16/9787544242516/2.png" alt="" /><figcaption>《白夜行》</figcaption></figure><h2 id="内容简介">内容简介</h2><p>“只希望能手牵手在太阳下散步”，这个象征故事内核的绝望念想，有如一个美丽的幌子，随着无数凌乱、压抑、悲凉的故事片段像纪录片一样一一还原：没有痴痴相思，没有海枯石烂，只剩下一个冰冷绝望的诡计，最后一丝温情也被完全抛弃，万千读者在一曲救赎罪恶的凄苦爱情中悲切动容……</p><h2 id="经典">经典</h2><blockquote><p>“捡别人丢的东西不还，跟偷别人随意放置的东西，并没有什么差别。有错的难道不是把装了钱的包随便放的人吗？这个社会上，让别人有机可乘的人注定要吃亏。”</p></blockquote><blockquote><p>“我的人生就像在 <strong>白夜里走路</strong>。”</p></blockquote><p>-- 书名来原</p><blockquote><p>“专家系统是人工智能的应用之一，就是以电脑取代专家的系统。”</p></blockquote><p>-- 2008 年的小说就已经有 AI 应用</p><blockquote><p>“喏，夏美，一天当中，有太阳升起的时候，也有下沉的时候。人生也一样，有白天和黑夜，只是不会像真正的太阳那样，有定时的日出和日落。看个人，有些人一辈子都活在太阳的照耀下，也有些人不得不一直活在漆黑的深夜里。人害怕的，就是本来一直存在的太阳落下不再升起，也就是非常害怕原本照在身上的光芒消失，现在的夏美就是这样。”</p></blockquote><blockquote><p>“我呢，”雪穗继续说，“从来就没有生活在太阳底下过。”</p></blockquote><blockquote><p>“我的天空里没有太阳，总是黑夜，但并不暗，因为有东西代替了太阳。虽然没有太阳那么明亮，但对我来说已经足够。凭借着这份光，我便能把黑夜当成白天。你明白吧？我从来就没有太阳，所以不怕失去。”</p></blockquote><h2 id="白夜行vs-幻夜">《白夜行》vs 《幻夜》</h2><figure><img src="https://2020.iosdevlog.com/2020/02/12/9787544291811/1.svg" alt="" /><figcaption>《幻夜》人物关系图</figcaption></figure><p>失去生命中最后的光芒，她完全堕入无边幻夜，开始极尽邪恶而妖艳的表演……</p><p>如果要说《白夜行》中有着任何一丝可以被称为是 <strong>希望</strong> 的东西，那么到了《幻夜》，东野圭吾连这一丝都从我们面前夺走了。</p><p>从这个意义上，《幻夜》是一本不折不扣的 <strong>绝望之书</strong>。”</p><h3 id="开头">开头</h3><p>《白夜行》</p><p>女主遭性侵，男主杀人。</p><p>《幻夜》</p><p>男主杀人，女主遭性侵。</p><h3 id="书名">书名</h3><p>《白夜行》</p><p>《飘》</p><p>《幻夜》</p><p>《飘》</p><h3 id="精品店名">精品店名</h3><p>《白夜行》</p><p>BLUE SNOW / 蓝<strong>雪</strong></p><p>《幻夜》</p><p>WHITE NIGHT / <strong>白夜</strong></p><h3 id="警察">警察</h3><blockquote><p>“<strong>枪虾</strong> 会挖洞，住在洞里。可有个家伙却要去住在它的洞里，那就是 <strong>虾虎鱼</strong>。不过虾虎鱼也不白住，它会在洞口巡视，要是有外敌靠近，就摆动尾鳍通知洞里的枪虾。它们合作无间，这好像叫互利共生。”</p></blockquote><p>-- 笹垣比喻为 <strong>枪虾 &amp; 虾虎鱼</strong></p><blockquote><p>“大学毕业后，我曾经尝试过各种道路，因为不清楚应该如何生存下去。就在这时，我遇到了一个女子，我发现那个人正是我的理想。我在她身边工作，经常和她一起行动。当她舍弃一切、想去国外生活的时候，在我的再三恳求下，她同意带我一起去。我想成为那样的人，所有的一切都模仿她。后来，连外形，也就是容貌，都想变得和她一样。“</p></blockquote><p>-- (假）新海美冬</p><blockquote><p>“她会不会就是你自己？是不是以前你就被真正的新海美冬这样仰慕过？而且，那个时候你见过曾我。所以到了今天，如果他再出现在作为新海美冬而活着的你面前，无疑是一种障碍，不对吗？“</p></blockquote><p>-- 加藤</p><h3 id="结尾">结尾</h3><p>《白夜行》</p><p>只见雪穗正沿扶梯上楼，背影犹如白色的幽灵。</p><p>她一次都没有回头。</p><p>亮司最后强奸了美佳，变成了他的父亲那样的人。</p><p>雪穗安排这一切，变成了她母亲那样的人。</p><p>男主死</p><p>女主无罪</p><p>《幻夜》</p><blockquote><p>“没有。这么美好的夜晚还是第一次看到，简直像幻夜一般。”</p></blockquote><p>说着，她露出娇媚的笑容。</p><p>女主无罪</p><p>男主死</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/16/9787544242516/1.svg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《白夜行》人物关系图&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="东野圭吾" scheme="https://2020.iosdevlog.com/tags/%E4%B8%9C%E9%87%8E%E5%9C%AD%E5%90%BE/"/>
    
  </entry>
  
  <entry>
    <title>Kindle 无 USB 传书</title>
    <link href="https://2020.iosdevlog.com/2020/02/15/kindle/"/>
    <id>https://2020.iosdevlog.com/2020/02/15/kindle/</id>
    <published>2020-02-15T15:01:35.000Z</published>
    <updated>2020-02-15T15:24:51.509Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/15/kindle/1.jpg" alt="" /><figcaption>不能连 USB</figcaption></figure><a id="more"></a><p>如何把电子书资源导入到Kindle？</p><ol type="1"><li>亚马逊官方商城直接购买</li><li>使用 Kindle 电子邮箱传书（包括第三方推送）</li><li>使用 USB 数据线导入</li><li>通过公众号【亚马逊 Kindle 服务号】将文章推送到 Kindle</li><li>通过 Cailbre 传书到 Kindle</li><li>通过手机发送传书到 Kindle app</li></ol><p>如果是已经在电脑里面的书籍需要传书，使用 USB 数据线导入。</p><p>Kindle 的 USB 接口连接到 macOS 上面，能充电，却不能显示 U 盘。<br />还可以选 使用 Kindle 电子邮箱传书（包括第三方推送）。<br />不过电子邮箱传书大小有限制。<br />Kindle 如果可以访问网页版浏览器，这时候就可以自己搭建 <code>http</code> 服务器传书。</p><p><code>Kindle</code> 的 <code>USB</code> 接口连接到 <code>macOS</code> 上面，能充电，却不能显示 <code>U</code> 盘。</p><p>除了使用邮箱推送，还可以让 <code>Kindle</code> 访问 <code>http</code>。</p><h2 id="查看本机-ip-地址">查看本机 <code>ip</code> 地址</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifconfig | grep 192.168</span><br><span class="line">192.168.x.x</span><br></pre></td></tr></table></figure><h2 id="http-服务">http 服务</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m SimpleHTTPServer</span><br></pre></td></tr></table></figure><ol type="1"><li>Kindle 主页</li><li>最左侧三个点</li><li>体验网页版浏览器</li><li><code>192.168.x.x:8000</code></li></ol><p>下载文件即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/15/kindle/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;不能连 USB&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="kindle" scheme="https://2020.iosdevlog.com/tags/kindle/"/>
    
  </entry>
  
  <entry>
    <title>通过挖掘类比关系加速创新</title>
    <link href="https://2020.iosdevlog.com/2020/02/15/mining/"/>
    <id>https://2020.iosdevlog.com/2020/02/15/mining/</id>
    <published>2020-02-15T13:00:11.000Z</published>
    <updated>2020-02-15T13:30:15.722Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/15/mining/1.png" alt="" /><figcaption>挖掘</figcaption></figure><p>Paper: <a href="http://www.hyadatalab.com/papers/analogy-kdd17.pdf" target="_blank" rel="noopener">Accelerating Innovation Through Analogy Mining</a></p><p>这篇文章主要阐述了帮助创新的一个重要步骤，那就是如何找到合适并且有效的类比案例。</p><p>如何找到合适的类比，并能从中获取灵感，可能就是创新的一个关键因素。</p><a id="more"></a><h2 id="论文的核心方法">论文的核心方法</h2><p>首先，作者们提出了一组叫“目的”（Purpose）和“机制”（Mechanism）的概念。什么叫“目的”呢？那就是当前的产品是要解决什么问题的。什么叫“机制”呢？那就是当前的产品是使用什么手段或者方法来解决这个问题的。对于一个产品，如果我们能够明确这个产品的目的和机制，找到类比就变得更加容易。比如，我们可以针对某一个问题，相同的目的，采用不同的机制或者对不同的问题采用相同的机制。</p><p>作者们认为，<strong>这种对产品信息的分类符合很多工程设计的过程，是创新过程中的一个必要环节</strong>。</p><p>有了这种想法以后，很自然的下一个步骤就是如何从数据中学习到目的和机制，如何自动挖掘出海量产品信息的目的和机制。要想学习到这样的信息，作者们提出了一种依靠标签数据的监督学习（Supervised Leanring）机制。具体说来，作者们把文本信息中的每句话、短语交给亚马逊土耳其机器人（Amazon Mechanical Turk）上的在线工人，来标注每个文本信息是目的信息还是机制信息。也就是说，作者们依靠有标注的数据来训练提出的算法。</p><p>首先，我们有一组文本，每组文本都有这些文本的原始文字。<strong>针对每个文档，我们都收集 K 个目的标注和 K 个机制标注</strong>。这时，我们定义一组“目的标注”（Purpose Annotation）向量，其实也就是一组 0 或者 1 的向量。当文本原始文字中的某个字被标识为目的的时候，这个向量的相应元素置 1，反之置 0。类似的，我们也可以定义“机制标注”（Mechanism Annotation）向量。因为我们有 K 个标注，因此我们也有相应的 K 个“目的标注”向量和“机制标注”向量。这两组向量可以说是原始标签信息的一种向量的表达。</p><p>下一步就是从每一个有标签信息的文档里产生 <strong>唯一的目的向量和机制向量</strong>。这篇文章采用的方法是，利用每个单词的 <strong>嵌入向量（Embedding）</strong> 来获得这个唯一的向量。</p><p>参考: <a href="https://time.geekbang.org/column/article/391" target="_blank" rel="noopener" class="uri">https://time.geekbang.org/column/article/391</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/15/mining/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;挖掘&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Paper: &lt;a href=&quot;http://www.hyadatalab.com/papers/analogy-kdd17.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Accelerating Innovation Through Analogy Mining&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这篇文章主要阐述了帮助创新的一个重要步骤，那就是如何找到合适并且有效的类比案例。&lt;/p&gt;
&lt;p&gt;如何找到合适的类比，并能从中获取灵感，可能就是创新的一个关键因素。&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="paper" scheme="https://2020.iosdevlog.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>一张图搞定《人民的名义》里错综复杂的人物关系图</title>
    <link href="https://2020.iosdevlog.com/2020/02/14/people/"/>
    <id>https://2020.iosdevlog.com/2020/02/14/people/</id>
    <published>2020-02-14T15:45:47.000Z</published>
    <updated>2020-02-14T18:01:07.356Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/14/people/1.svg" alt="" /><figcaption>《人民的名义》人物关系图</figcaption></figure><a id="more"></a><p>书名：人民的名义<br />作者：周梅森<br />出版社：北京十月文艺出版社<br />出版时间：2017-01<br />ISBN：9787530216194</p><p>人民的名义</p><p>一分未花赵德汉<br />人脉能手丁义珍</p><p>王之蔑视李达康<br />学习外语陈清泉</p><p>信仰坚定刘新建<br />宇宙区长孙连城</p><p>锄地狂魔高育良</p><p>还没退休季昌明</p><p>不许牛逼赵瑞龙<br />聪明绝顶杜老板</p><p>再判回来<br />专业点穴易学习</p><p>命运坎坷高小琴<br />胜天半子祁同伟</p><p>挖地能手祁同伟<br />--沙瑞金评</p><blockquote><p>他拦下了李达康的车，挽救了李达康的政治前途啊！这是他的心里话，他真心感谢这位不畏权势的年轻反贪局局长。这位反贪局局长把一个天大的麻烦帮他拦在省内了。</p></blockquote><blockquote><p>沙瑞金看着湖面，又和田国富说了起来：国富，你不觉得这个事有点意思吗？李达康是市长，又做过赵立春的秘书，可他没批湖畔花园和湖上美食城，倒是育良同志给赵家公子批了，岂不耐人寻味？</p></blockquote><blockquote><p>今天我历尽艰难找到了你，真心是想带你回家，我不希望你死！可你清楚，有人希望你死！你死了，他们就安全了，他们就可以继续以 <strong>人民的名义</strong> 夸夸其谈了！老同学，你说你在这里找到了人民，那就请你以 <strong>人民的名义</strong> 去想一想，以残存的良知想一想，是不是该收手了？</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/14/people/1.svg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《人民的名义》人物关系图&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="读书" scheme="https://2020.iosdevlog.com/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="人物" scheme="https://2020.iosdevlog.com/tags/%E4%BA%BA%E7%89%A9/"/>
    
  </entry>
  
  <entry>
    <title>线性大规模支持向量机</title>
    <link href="https://2020.iosdevlog.com/2020/02/14/svm/"/>
    <id>https://2020.iosdevlog.com/2020/02/14/svm/</id>
    <published>2020-02-14T13:41:28.000Z</published>
    <updated>2020-02-16T07:03:36.366Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/14/svm/1.png" alt="" /><figcaption>svm</figcaption></figure><p>Paper: <a href="https://www.cs.cornell.edu/people/tj/publications/joachims_06a.pdf" target="_blank" rel="noopener">Training Linear SVMs in Linear Time</a></p><a id="more"></a><p>回到这篇时间检验奖的论文，它解决的是大规模优化支持向量机的问题，特别是线性支持向量机。</p><p>这篇文章第一次提出了简单易行的线性支持向量机实现，包括对有序回归（Ordinal Regression）的支持。</p><p>算法对于分类问题达到了 O(sn)（其中 s 是非 0 的特征数目而 n 是数据点的个数），也就是实现了线性复杂度，而对有序回归的问题达到了 O(snlog(n)) 的复杂度。</p><p>算法本身简单、高效、易于实现，并且理论上可以扩展到核函数（Kernel）的情况。</p><p>在此之前，很多线性支持向量机的实现都无法达到线性复杂度 。</p><p>比如当时的 LibSVM（台湾国立大学的学者发明）、SVM-Torch、以及早期的 SVM-Light 中采用的分解算法（Decomposition Method）都只能比较有效地处理大规模的特征。</p><p>而对于大规模的数据 (n)，则是超线性（Super-Linear）的复杂度。</p><p>另外的一些方法，能够训练复杂度线性地随着训练数据的增长而增长，但是却对于特征数 N 呈现了二次方 (N^2) 的复杂度。</p><p>因此之前的这些方法无法应用到大规模的数据上。</p><p>这样的情况对于有序回归支持向量机更加麻烦。</p><p>从德国学者拉尔夫·赫布里希（Ralf Herbrich）提出有序回归支持向量机以来，一直需要通过转化为普通的支持向量机的分类问题而求解。</p><p>这个转换过程需要产生 O(n^2) 的训练数据，使得整个问题的求解也在这个量级的复杂度。</p><p>这篇文章里，Thorsten 首先做的是对普通的支持向量机算法的模型形式（Formalism）进行了变形。</p><p>他把传统的分类支持向量机（Classification SVM）写成了结构化分类支持向量机（Structural Classification SVM），并且提供了一个定理来证明两者之间的等价性。</p><p>粗一看，这个等价的结构化分类支持向量机并没有提供更多有价值的信息。</p><p>然而这个新的优化目标函数的对偶（Dual）形式，由于它特殊的稀疏性，使它能够被用来进行大规模训练。</p><p>紧接着，Thorsten 又把传统的有序回归支持向量机的优化函数，写成了结构化支持向量机的形式，并且证明了两者的等价性。</p><p>把两种模型表达成结构化向量机的特例之后，Thorsten 开始把解决结构化向量机的一种算法——切割平面算法（Cutting-Plane），以下称 CP 算法，运用到了这两种特例上。</p><p>首先，他展示了 CP 算法在分类问题上的应用。</p><p>简单说来，这个算法就是保持一个工作集合（Working Set），来存放当前循环时依然被违反的约束条件（Constraints），然后在下一轮中集中优化这部分工作集合的约束条件。</p><p>整个流程开始于一个空的工作集合，每一轮优化的是一个基于当前工作集合的支持向量机子问题，算法直到所有的约束条件的误差小于一个全局的参数误差为止。</p><p>Thorsten 在文章中详细证明了这个算法的有效性和时间复杂度。</p><p>相同的方法也使得有序回归支持向量机的算法能够转换成为更加计算有效的优化过程。</p><p>Thorsten 在文章中做了详尽的实验来展现新算法的有效性。</p><p>从数据的角度，他使用了 5 个不同的数据集，分别是路透社 RCV1 数据集的好几个子集。</p><p>数据的大小从 6 万多数据点到 80 多万数据点不等，特征数也从几十到四万多特征不等，这几种不同的数据集还是比较有代表性的。</p><p>从方法的比较上来说，Thorsten 主要比较了传统的分解方法。</p><p>有两个方面是重点比较的，第一就是训练时间。</p><p>在所有的数据集上，这篇文章提出的算法都比传统算法快几个数量级，提速达到近 100 倍。</p><p>而有序回归的例子中，传统算法在所有数据集上都无法得到最后结果。</p><p>Thorsten 进一步展示了训练时间和数据集大小的线性关系，从而验证了提出算法在真实数据上的表现。</p><p>第二个重要的比较指标是算法的准确度是否有所牺牲。</p><p>因为有时候算法的提速是在牺牲算法精度的基础上做到的，因此验证算法的准确度就很有意义。</p><p>在这篇文章里，Thorsten 展示，提出的算法精度，也就是分类准确度并没有统计意义上的区分度，也让这个算法的有效性有了保证。</p><p>Thorsten 在他的软件包 SVM-Perf 中实现了这个算法。</p><p>这个软件包一度成了支持向量机研究和开发的标准工具。</p><p>参考: <a href="https://time.geekbang.org/column/article/159" target="_blank" rel="noopener" class="uri">https://time.geekbang.org/column/article/159</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/14/svm/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;svm&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://www.cs.cornell.edu/people/tj/publications/joachims_06a.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Training Linear SVMs in Linear Time&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="svm" scheme="https://2020.iosdevlog.com/tags/svm/"/>
    
      <category term="paper" scheme="https://2020.iosdevlog.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>数据的重要性</title>
    <link href="https://2020.iosdevlog.com/2020/02/13/data/"/>
    <id>https://2020.iosdevlog.com/2020/02/13/data/</id>
    <published>2020-02-13T15:26:32.000Z</published>
    <updated>2020-02-13T15:52:29.666Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/13/data/1.png" alt="" /><figcaption>数据</figcaption></figure><a id="more"></a><p>机器学习三要素包括 <strong>数据</strong>、<strong>模型</strong>、<strong>算法</strong>。</p><p>总结成一句话:</p><blockquote><p>算法通过在数据上进行运算产生模型。</p></blockquote><p>2020年2月12日0时-24时，湖北省新增新冠状肺炎病例14840例(含临床诊断病例13332例)。</p><p>你有什么想说的？</p><p>​<strong>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限。</strong></p><p><strong>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限。</strong></p><p><strong>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限。</strong></p><p>重要的事情说 3 遍。</p><h2 id="特征工程">特征工程</h2><h3 id="概念">概念</h3><blockquote><p>特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。通俗的说，就是尽可能的从原始数据中获取更多信息，从而使得预测模型达到最佳。</p></blockquote><p>--维基百科</p><p>简而言之，特征工程是一个把原始数据变成特征的过程，这些特征可以很好的描述数据，并且利用它们建立的模型在未知数据上表现性能可以达到最优。</p><h3 id="重要性">重要性</h3><p>实验结果取决于获取的数据、使用的特征以及选择的模型，甚至问题的形式和评估精度的客观方法也扮演了一部分。我们需要的是能够很好地描述数据内部结构的好特征。</p><ul><li>特征越好，灵活性越强</li></ul><p>只要特征选得好，即使是一般的模型（或算法）也能获得很好的性能，因为大多数模型（或算法）在好的数据特征下表现的性能都还不错。好特征的灵活性在于它允许你选择不复杂的模型，同时运行速度也更快，也更容易理解和维护。</p><ul><li>特征越好，构建的模型越简单</li></ul><p>有了好的特征，即便你的参数不是最优的，你的模型性能也能仍然会表现的很nice，所以你就不需要花太多的时间去寻找最有参数，这大大的降低了模型的复杂度，使模型趋于简单。</p><ul><li>特征越好，模型的性能越出色</li></ul><p>特征工程的最终目的就是提升模型的性能。</p><p>总结：</p><p><strong>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/13/data/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;数据&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="ML" scheme="https://2020.iosdevlog.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>《AI技术内参》开篇</title>
    <link href="https://2020.iosdevlog.com/2020/02/13/ai/"/>
    <id>https://2020.iosdevlog.com/2020/02/13/ai/</id>
    <published>2020-02-13T11:44:32.000Z</published>
    <updated>2020-02-13T11:50:21.115Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://2020.iosdevlog.com/2020/02/13/ai/1.png" alt="" /><figcaption>《AI技术内参》</figcaption></figure><a id="more"></a><p>第一，一些经典的人工智能技术。这些技术涵盖搜索、推荐系统、广告系统、图像处理等领域。了解这些经典技术能够让你迅速入门并能为今后的学习打下基础。这部分内容帮助你分析核心的算法模型，并为你进行系统性学习提供纲要和指引。</p><p>第二，最新的顶级学术会议动态，帮助你了解和掌握这些学术会议最火热和最新的研究成果。每一年和人工智能相关的顶级学术会议有十余个，每个会议都会有上百篇甚至几百篇论文发表。从这些论文和成果中找到有价值的信息，对于初学者，甚至是有一定经验的从业人员来说都是非常困难、也非常耗时的一件事情。那么，在这个专栏里，我会为你精选内容，可以让你不错过任何有价值的最新成果。</p><p>第三，人工智能的从业人员提供指南，帮助数据科学家和工程师提升自我价值，帮助人工智能团队的管理者构建团队，为你在职场发展中的关键步骤出谋划策。</p><figure><img src="https://2020.iosdevlog.com/2020/02/13/ai/2.png" alt="" /><figcaption>《AI技术内参》</figcaption></figure><p>总结：<a href="https://time.geekbang.org/column/article/153" target="_blank" rel="noopener" class="uri">https://time.geekbang.org/column/article/153</a></p>]]></content>
    
    <summary type="html">
    
      &lt;figure&gt;
&lt;img src=&quot;https://2020.iosdevlog.com/2020/02/13/ai/1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;《AI技术内参》&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://2020.iosdevlog.com/categories/AI/"/>
    
    
      <category term="极客时间" scheme="https://2020.iosdevlog.com/tags/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"/>
    
  </entry>
  
</feed>
